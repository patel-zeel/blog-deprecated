<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://patel-zeel.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://patel-zeel.github.io/blog/" rel="alternate" type="text/html" /><updated>2022-01-24T08:04:53-06:00</updated><id>https://patel-zeel.github.io/blog/feed.xml</id><title type="html">Zeel B Patel’s Blog</title><subtitle>Tips and Tricks, Techniques and Technology.</subtitle><entry><title type="html">Query by Committee</title><link href="https://patel-zeel.github.io/blog/ml/2022/01/24/Query_by_Committee.html" rel="alternate" type="text/html" title="Query by Committee" /><published>2022-01-24T00:00:00-06:00</published><updated>2022-01-24T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/01/24/Query_by_Committee</id><author><name>Zeel B Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">KL divergence v/s cross-entropy</title><link href="https://patel-zeel.github.io/blog/markdown/2022/01/20/kl-divergence.html" rel="alternate" type="text/html" title="KL divergence v/s cross-entropy" /><published>2022-01-20T00:00:00-06:00</published><updated>2022-01-20T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/markdown/2022/01/20/kl-divergence</id><author><name></name></author><category term="markdown" /><summary type="html">Ground In a classification problem, for a data-point $\mathbf{x}_i$, we have the true label $y_i$ associated with it. Let us assume that we have three possible outcomes ${L1, L2, L3}$ and for current $\mathbf{x}_i$, corresponding $y_i$ is $L2$. Then Ground truth probability distribution is the following: pG(y=L1)=0pG(y=L2)=1pG(y=L3)=0p_G(y = L1) = 0\\ p_G(y = L2) = 1\\ p_G(y=L3) = 0pG​(y=L1)=0pG​(y=L2)=1pG​(y=L3)=0 Let us assume that our classifier model Predicted the following distribution: pP(y=L1)=0.1pP(y=L2)=0.8pP(y=L3)=0.1p_P(y = L1) = 0.1\\ p_P(y = L2) = 0.8\\ p_P(y=L3) = 0.1pP​(y=L1)=0.1pP​(y=L2)=0.8pP​(y=L3)=0.1 KL divergence We can use KL divergence to check how good is our model. The formula is: DKL(pG  ∥  pP)=∑yi∈{L1,L2,L3}pG(yi)log⁡pG(yi)pP(yi)D_{KL}(p_G\;\rVert\;p_P) = \sum_{y_i \in \{L1, L2, L3\}} p_G(y_i)\log\frac{p_G(y_i)}{p_P(y_i)}DKL​(pG​∥pP​)=yi​∈{L1,L2,L3}∑​pG​(yi​)logpP​(yi​)pG​(yi​)​ For our example, DKL(pG  ∥  pP)=log⁡10.8D_{KL}(p_G\;\rVert\;p_P) = \log\frac{1}{0.8}DKL​(pG​∥pP​)=log0.81​ It is evident that if $p_P(y = L2)$ decreses from $0.8$, $D_{KL}(p_G\;\rVert\;p_P)$ will increase and vice versa. Note that KL divergence is not symmetric which means $D_{KL}(p_G\;\rVert\;p_P) \ne D_{KL}(p_P\;\rVert\;p_G)$. Cross-entory Cross-entropy is another measure for distribution similarity. The formula is: H(pG,pP)=∑yi∈{L1,L2,L3}−pG(yi)log⁡pP(yi)H(p_G, p_P) = \sum_{y_i \in \{L1, L2, L3\}} - p_G(y_i)\log p_P(y_i)H(pG​,pP​)=yi​∈{L1,L2,L3}∑​−pG​(yi​)logpP​(yi​) For our example: H(pG,pP)=−log⁡0.8=log⁡10.8H(p_G, p_P) = -\log 0.8 = \log \frac{1}{0.8}H(pG​,pP​)=−log0.8=log0.81​ KL divergence v/s cross-entropy This shows that KL divergence and cross-entropy will return the same values for a simple classification problem. Then why do we use cross-entropy as a loss function and not KL divergence? That’s because KL divergence will compute additional constant terms (zero here) that are not adding any value in minimization.</summary></entry><entry><title type="html">Why .py files are better than .ipynb files for ML codebase</title><link href="https://patel-zeel.github.io/blog/markdown/2022/01/15/py_over_ipynb.html" rel="alternate" type="text/html" title="Why .py files are better than .ipynb files for ML codebase" /><published>2022-01-15T00:00:00-06:00</published><updated>2022-01-15T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/markdown/2022/01/15/py_over_ipynb</id><author><name></name></author><category term="markdown" /><summary type="html">I have shifted from .ipynb files to .py files (and Jupyter to VS code) in the last couple of months. Here are some reasons why I feel .py files are better than .ipynb files:</summary></entry><entry><title type="html">Anonymization tips for double-blind submission</title><link href="https://patel-zeel.github.io/blog/academic/2021/10/26/Anonymization-Tips.html" rel="alternate" type="text/html" title="Anonymization tips for double-blind submission" /><published>2021-10-26T00:00:00-05:00</published><updated>2021-10-26T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/academic/2021/10/26/Anonymization-Tips</id><author><name>Zeel B Patel</name></author><category term="Academic" /><summary type="html"></summary></entry><entry><title type="html">Input Warped GPs - A failed idea</title><link href="https://patel-zeel.github.io/blog/ml/2021/10/23/Warped-GP.html" rel="alternate" type="text/html" title="Input Warped GPs - A failed idea" /><published>2021-10-23T00:00:00-05:00</published><updated>2021-10-23T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/ml/2021/10/23/Warped-GP</id><author><name>Zeel B Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">SparseGPs in Stheno</title><link href="https://patel-zeel.github.io/blog/2021/10/12/SparseGPs.html" rel="alternate" type="text/html" title="SparseGPs in Stheno" /><published>2021-10-12T00:00:00-05:00</published><updated>2021-10-12T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/2021/10/12/SparseGPs</id><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Docker Cheatsheet</title><link href="https://patel-zeel.github.io/blog/markdown/2021/09/28/docker_cheatsheet.html" rel="alternate" type="text/html" title="Docker Cheatsheet" /><published>2021-09-28T00:00:00-05:00</published><updated>2021-09-28T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/markdown/2021/09/28/docker_cheatsheet</id><author><name></name></author><category term="markdown" /><summary type="html">Images</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://patel-zeel.github.io/blog/images/homepage-docker-logo.png" /><media:content medium="image" url="https://patel-zeel.github.io/blog/images/homepage-docker-logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How to apply constraint on parameters in various GP libraries</title><link href="https://patel-zeel.github.io/blog/2021/09/27/Constraints.html" rel="alternate" type="text/html" title="How to apply constraint on parameters in various GP libraries" /><published>2021-09-27T00:00:00-05:00</published><updated>2021-09-27T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/2021/09/27/Constraints</id><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Understanding Kernels in Gaussian Processes</title><link href="https://patel-zeel.github.io/blog/ml/2021/03/22/GP_Kernels.html" rel="alternate" type="text/html" title="Understanding Kernels in Gaussian Processes" /><published>2021-03-22T00:00:00-05:00</published><updated>2021-03-22T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/ml/2021/03/22/GP_Kernels</id><author><name>Zeel Patel</name></author><category term="ML" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://patel-zeel.github.io/blog/images/gaussiankernel.jpg" /><media:content medium="image" url="https://patel-zeel.github.io/blog/images/gaussiankernel.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Programatically download OpenAQ data</title><link href="https://patel-zeel.github.io/blog/data/openaq/2020/09/21/Programatically_download_OpenAQ_data.html" rel="alternate" type="text/html" title="Programatically download OpenAQ data" /><published>2020-09-21T00:00:00-05:00</published><updated>2020-09-21T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/data/openaq/2020/09/21/Programatically_download_OpenAQ_data</id><author><name>Zeel B Patel</name></author><category term="data" /><category term="openaq" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://patel-zeel.github.io/blog/images/openaq.png" /><media:content medium="image" url="https://patel-zeel.github.io/blog/images/openaq.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>