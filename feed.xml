<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://patel-zeel.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://patel-zeel.github.io/blog/" rel="alternate" type="text/html" /><updated>2022-04-09T00:12:50-05:00</updated><id>https://patel-zeel.github.io/blog/feed.xml</id><title type="html">Zeel B Patel’s Blog</title><subtitle>Tips and Tricks, Techniques and Technology.</subtitle><entry><title type="html">Gcloud cheatsheet</title><link href="https://patel-zeel.github.io/blog/markdown/2022/04/09/gcloud.html" rel="alternate" type="text/html" title="Gcloud cheatsheet" /><published>2022-04-09T00:00:00-05:00</published><updated>2022-04-09T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/markdown/2022/04/09/gcloud</id><author><name></name></author><category term="markdown" /><summary type="html">Initial setup Set default email, project-id &amp;amp; region gcloud config set account your-email-account gcloud config set project your-project-id gcloud config set compute/zone us-central1-f # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access) To get currently active project and region related info gcloud info</summary></entry><entry><title type="html">Uncertainty in Deep Learning</title><link href="https://patel-zeel.github.io/blog/ml/2022/04/08/uncertainty-in-deep-learning.html" rel="alternate" type="text/html" title="Uncertainty in Deep Learning" /><published>2022-04-08T00:00:00-05:00</published><updated>2022-04-08T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/04/08/uncertainty-in-deep-learning</id><author><name>Zeel Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">GitHub Contrubuting FAQs</title><link href="https://patel-zeel.github.io/blog/github/2022/04/08/_04_06_GitHub_FAQs.html" rel="alternate" type="text/html" title="GitHub Contrubuting FAQs" /><published>2022-04-08T00:00:00-05:00</published><updated>2022-04-08T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/github/2022/04/08/_04_06_GitHub_FAQs</id><author><name>Zeel B Patel</name></author><category term="GitHub" /><summary type="html"></summary></entry><entry><title type="html">Torch essentials</title><link href="https://patel-zeel.github.io/blog/ml/2022/03/08/torch-essentials.html" rel="alternate" type="text/html" title="Torch essentials" /><published>2022-03-08T00:00:00-06:00</published><updated>2022-03-08T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/03/08/torch-essentials</id><author><name>Zeel Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">Probabilistic Machine Learning</title><link href="https://patel-zeel.github.io/blog/ml/2022/03/06/probabilistic-machine-learning.html" rel="alternate" type="text/html" title="Probabilistic Machine Learning" /><published>2022-03-06T00:00:00-06:00</published><updated>2022-03-06T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/03/06/probabilistic-machine-learning</id><author><name>Zeel Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">PyTorch Tips</title><link href="https://patel-zeel.github.io/blog/ml/2022/02/25/torch-tips.html" rel="alternate" type="text/html" title="PyTorch Tips" /><published>2022-02-25T00:00:00-06:00</published><updated>2022-02-25T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/02/25/torch-tips</id><author><name></name></author><category term="ml" /><summary type="html">Several tips for building torch models from scratch from my experience. Some of the tips are like zen, they are not immediately intuitive but useful for efficient code. All the initializations or new tensor creation should only happen in __init__ method. During the forward() call, ideally no new tensors should be created from scratch such as torch.zeros(), torch.ones() etc. Reason: Violating this can sometimes brake your forward pass and end-to-end backprop may become buggy. .cuda() and .cpu() are discouraged, use .to(device) instead. Reason: .to(device) is more dynamic and scalable. Do not save models with torch.save(model), that may become incompaitable with different torch versions and may take more memory. Save torch.save(model.state_dict()) instead. Need to set parameter names dynamically? Use this example, zero=0;self.register_parameter(f&quot;name_{zero}&quot;). They can be accessed with model.name_0. Have something in model which is necessary for forward pass but does not require backprop? define those variables with self.register_buffer. Let .to(device) to be set outside the model defition. Reason: It is less confusing to the users this way and it is less messy with internal tools to set device such as: module.to(deivce) sends all parameters and buffers of model/submodules to the device. module.float() or module.double() will convert all model/submodule parameters and buffers into float32 and float64 respectively. Let .train() and .eval() to be set outside the model defition or set by user. Reason: It can be confusing to user if these things are used inside the model against torch conventions. torch.no_grad() should not be used within the model. Reason: Sometimes user may want to backprop through that chunk of code. Link the multiple modules togather. Reason: Ideally, it is useful if model is built like a assembled product (say a car). You should be able to replace the parts as per your requirement. Several benefits on these lines are: setting module.train() or module.eval() puts all submodules in train mode or eval mode respectively. All submodules parameters can be accesses directly from the parent module with module.parameters(). Creating a list of parameters in model __init__ definition? consider torch.nn.ModuleList(params) else individual parameters in the list will not be recognized as parameters.</summary></entry><entry><title type="html">Conference Presentation Tips</title><link href="https://patel-zeel.github.io/blog/academic/2022/01/29/presentation_tips.html" rel="alternate" type="text/html" title="Conference Presentation Tips" /><published>2022-01-29T00:00:00-06:00</published><updated>2022-01-29T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/academic/2022/01/29/presentation_tips</id><author><name></name></author><category term="Academic" /><summary type="html">General First page goes like this: Title Authors (Underline presenting author, no need to put * incase of equal contribution) Affiliations Conference name If importing figures from paper, avoid including the captions. Include lot of images and less maths Talk should end with summary and not the future work or thank you slide or something. Cite the references on the same slide in bottom. Refere to “Giving talks” section of this blog. Dos and Don’ts Never put too detailed information difficult to grasp: a table with many numbers, a complex derivation all in one go, very complicated diagram.</summary></entry><entry><title type="html">Comparing Gaussian Process Regression Frameworks</title><link href="https://patel-zeel.github.io/blog/ml/2022/01/25/GP_Frameworks_Comparison.html" rel="alternate" type="text/html" title="Comparing Gaussian Process Regression Frameworks" /><published>2022-01-25T00:00:00-06:00</published><updated>2022-01-25T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/01/25/GP_Frameworks_Comparison</id><author><name>Zeel B Patel, Harsh Patel, Shivam Sahni</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">Query by Committee</title><link href="https://patel-zeel.github.io/blog/ml/2022/01/24/Query_by_Committee.html" rel="alternate" type="text/html" title="Query by Committee" /><published>2022-01-24T00:00:00-06:00</published><updated>2022-01-24T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/01/24/Query_by_Committee</id><author><name>Zeel B Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">KL divergence v/s cross-entropy</title><link href="https://patel-zeel.github.io/blog/ml/2022/01/20/kl-divergence.html" rel="alternate" type="text/html" title="KL divergence v/s cross-entropy" /><published>2022-01-20T00:00:00-06:00</published><updated>2022-01-20T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/01/20/kl-divergence</id><author><name></name></author><category term="ML" /><summary type="html">Ground In a classification problem, for a data-point $\mathbf{x}_i$, we have the true label $y_i$ associated with it. Let us assume that we have three possible outcomes ${L1, L2, L3}$ and for current $\mathbf{x}_i$, corresponding $y_i$ is $L2$. Then Ground truth probability distribution is the following: pG(y=L1)=0pG(y=L2)=1pG(y=L3)=0p_G(y = L1) = 0\\ p_G(y = L2) = 1\\ p_G(y=L3) = 0pG​(y=L1)=0pG​(y=L2)=1pG​(y=L3)=0 Let us assume that our classifier model Predicted the following distribution: pP(y=L1)=0.1pP(y=L2)=0.8pP(y=L3)=0.1p_P(y = L1) = 0.1\\ p_P(y = L2) = 0.8\\ p_P(y=L3) = 0.1pP​(y=L1)=0.1pP​(y=L2)=0.8pP​(y=L3)=0.1 KL divergence We can use KL divergence to check how good is our model. The formula is: DKL(pG  ∥  pP)=∑yi∈{L1,L2,L3}pG(yi)log⁡pG(yi)pP(yi)D_{KL}(p_G\;\rVert\;p_P) = \sum_{y_i \in \{L1, L2, L3\}} p_G(y_i)\log\frac{p_G(y_i)}{p_P(y_i)}DKL​(pG​∥pP​)=yi​∈{L1,L2,L3}∑​pG​(yi​)logpP​(yi​)pG​(yi​)​ For our example, DKL(pG  ∥  pP)=log⁡10.8D_{KL}(p_G\;\rVert\;p_P) = \log\frac{1}{0.8}DKL​(pG​∥pP​)=log0.81​ It is evident that if $p_P(y = L2)$ decreses from $0.8$, $D_{KL}(p_G\;\rVert\;p_P)$ will increase and vice versa. Note that KL divergence is not symmetric which means $D_{KL}(p_G\;\rVert\;p_P) \ne D_{KL}(p_P\;\rVert\;p_G)$. Cross-entory Cross-entropy is another measure for distribution similarity. The formula is: H(pG,pP)=∑yi∈{L1,L2,L3}−pG(yi)log⁡pP(yi)H(p_G, p_P) = \sum_{y_i \in \{L1, L2, L3\}} - p_G(y_i)\log p_P(y_i)H(pG​,pP​)=yi​∈{L1,L2,L3}∑​−pG​(yi​)logpP​(yi​) For our example: H(pG,pP)=−log⁡0.8=log⁡10.8H(p_G, p_P) = -\log 0.8 = \log \frac{1}{0.8}H(pG​,pP​)=−log0.8=log0.81​ KL divergence v/s cross-entropy This shows that KL divergence and cross-entropy will return the same values for a simple classification problem. Then why do we use cross-entropy as a loss function and not KL divergence? That’s because KL divergence will compute additional constant terms (zero here) that are not adding any value in minimization.</summary></entry></feed>