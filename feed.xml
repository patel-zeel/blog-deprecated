<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://patel-zeel.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://patel-zeel.github.io/blog/" rel="alternate" type="text/html" /><updated>2022-07-31T13:50:57-05:00</updated><id>https://patel-zeel.github.io/blog/feed.xml</id><title type="html">Zeel B Patel’s Blog</title><subtitle>Tips and Tricks, Techniques and Technology.</subtitle><entry><title type="html">Get a list of contributors from a repo</title><link href="https://patel-zeel.github.io/blog/github/2022/07/30/_contributors_sorted_by_PRs.html" rel="alternate" type="text/html" title="Get a list of contributors from a repo" /><published>2022-07-30T00:00:00-05:00</published><updated>2022-07-30T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/github/2022/07/30/_contributors_sorted_by_PRs</id><author><name>Zeel B Patel</name></author><category term="GitHub" /><summary type="html"></summary></entry><entry><title type="html">JAX Optimizers</title><link href="https://patel-zeel.github.io/blog/ml/2022/06/10/JaxOptimizers.html" rel="alternate" type="text/html" title="JAX Optimizers" /><published>2022-06-10T00:00:00-05:00</published><updated>2022-06-10T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/06/10/JaxOptimizers</id><author><name>Zeel B Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">Iteratively reweighted least squares (IRLS) logistic regression</title><link href="https://patel-zeel.github.io/blog/ml/2022/05/14/Iteratively_reweighted_least_squares.html" rel="alternate" type="text/html" title="Iteratively reweighted least squares (IRLS) logistic regression" /><published>2022-05-14T00:00:00-05:00</published><updated>2022-05-14T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/05/14/Iteratively_reweighted_least_squares</id><author><name>Zeel B Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">Gcloud cheatsheet</title><link href="https://patel-zeel.github.io/blog/markdown/2022/04/09/gcloud.html" rel="alternate" type="text/html" title="Gcloud cheatsheet" /><published>2022-04-09T00:00:00-05:00</published><updated>2022-04-09T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/markdown/2022/04/09/gcloud</id><author><name></name></author><category term="markdown" /><summary type="html">Initial setup Following this guide.</summary></entry><entry><title type="html">GitHub Contrubuting FAQs</title><link href="https://patel-zeel.github.io/blog/github/2022/04/06/GitHub_FAQs.html" rel="alternate" type="text/html" title="GitHub Contrubuting FAQs" /><published>2022-04-06T00:00:00-05:00</published><updated>2022-04-06T00:00:00-05:00</updated><id>https://patel-zeel.github.io/blog/github/2022/04/06/GitHub_FAQs</id><author><name>Zeel B Patel</name></author><category term="GitHub" /><summary type="html"></summary></entry><entry><title type="html">Torch essentials</title><link href="https://patel-zeel.github.io/blog/ml/2022/03/08/torch-essentials.html" rel="alternate" type="text/html" title="Torch essentials" /><published>2022-03-08T00:00:00-06:00</published><updated>2022-03-08T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/03/08/torch-essentials</id><author><name>Zeel B Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">Probabilistic Machine Learning</title><link href="https://patel-zeel.github.io/blog/ml/2022/03/06/probabilistic-machine-learning.html" rel="alternate" type="text/html" title="Probabilistic Machine Learning" /><published>2022-03-06T00:00:00-06:00</published><updated>2022-03-06T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/03/06/probabilistic-machine-learning</id><author><name>Zeel B Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">Uncertainty in Deep Learning</title><link href="https://patel-zeel.github.io/blog/ml/2022/03/05/uncertainty-in-deep-learning.html" rel="alternate" type="text/html" title="Uncertainty in Deep Learning" /><published>2022-03-05T00:00:00-06:00</published><updated>2022-03-05T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/03/05/uncertainty-in-deep-learning</id><author><name>Zeel B Patel</name></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">PyTorch Tips</title><link href="https://patel-zeel.github.io/blog/ml/2022/02/25/torch-tips.html" rel="alternate" type="text/html" title="PyTorch Tips" /><published>2022-02-25T00:00:00-06:00</published><updated>2022-02-25T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/ml/2022/02/25/torch-tips</id><author><name></name></author><category term="ml" /><summary type="html">Several tips for building torch models from scratch from my experience. Some of the tips are like zen, they are not immediately intuitive but useful for efficient code. All the initializations or new tensor creation should only happen in __init__ method. During the forward() call, ideally no new tensors should be created from scratch such as torch.zeros(), torch.ones() etc. Reason: Violating this can sometimes brake your forward pass and end-to-end backprop may become buggy. .cuda() and .cpu() are discouraged, use .to(device) instead. Reason: .to(device) is more dynamic and scalable. Do not save models with torch.save(model), that may become incompaitable with different torch versions and may take more memory. Save torch.save(model.state_dict()) instead. Need to set parameter names dynamically? Use this example, zero=0;self.register_parameter(f&quot;name_{zero}&quot;). They can be accessed with model.name_0. Have something in model which is necessary for forward pass but does not require backprop? define those variables with self.register_buffer. Let .to(device) to be set outside the model defition. Reason: It is less confusing to the users this way and it is less messy with internal tools to set device such as: module.to(deivce) sends all parameters and buffers of model/submodules to the device. module.float() or module.double() will convert all model/submodule parameters and buffers into float32 and float64 respectively. Let .train() and .eval() to be set outside the model defition or set by user. Reason: It can be confusing to user if these things are used inside the model against torch conventions. torch.no_grad() should not be used within the model. Reason: Sometimes user may want to backprop through that chunk of code. Link the multiple modules togather. Reason: Ideally, it is useful if model is built like a assembled product (say a car). You should be able to replace the parts as per your requirement. Several benefits on these lines are: setting module.train() or module.eval() puts all submodules in train mode or eval mode respectively. All submodules parameters can be accesses directly from the parent module with module.parameters(). Creating a list of parameters in model __init__ definition? consider torch.nn.ModuleList(params) else individual parameters in the list will not be recognized as parameters.</summary></entry><entry><title type="html">Conference Presentation Tips</title><link href="https://patel-zeel.github.io/blog/academic/2022/01/29/presentation_tips.html" rel="alternate" type="text/html" title="Conference Presentation Tips" /><published>2022-01-29T00:00:00-06:00</published><updated>2022-01-29T00:00:00-06:00</updated><id>https://patel-zeel.github.io/blog/academic/2022/01/29/presentation_tips</id><author><name></name></author><category term="Academic" /><summary type="html">General First page goes like this: Title Authors (Underline presenting author, no need to put * incase of equal contribution) Affiliations Conference name If importing figures from paper, avoid including the captions. Include lot of images and less maths Talk should end with summary and not the future work or thank you slide or something. Cite the references on the same slide in bottom. Refere to “Giving talks” section of this blog. Dos and Don’ts Never put too detailed information difficult to grasp: a table with many numbers, a complex derivation all in one go, very complicated diagram.</summary></entry></feed>