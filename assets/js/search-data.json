{
  
    
        "post0": {
            "title": "Uncertainty in Deep Learning",
            "content": "import torch . 1 - Introduction . An online deep learning book from Ian Goodfellow, Yoshua Bengio, and Aaron Courville. | . 1.1 - Deep Learning . We define a single layer network as the following: . class SingleLayerNetwork(torch.nn.Module): def __init__(self, Q, D, K): &quot;&quot;&quot; Q: number of features D: number of outputs K: number of hidden features &quot;&quot;&quot; super().__init__() self.input = torch.nn.Linear(Q, K) # Transforms Q features into K hidden features self.output = torch.nn.Linear(K, D) # Transforms K hidden features to D output features self.non_lin_transform = torch.nn.ReLU() # A non-linear transformation def forward(self, X): &quot;&quot;&quot; X: input (N x Q) &quot;&quot;&quot; self.linear_transformed_X = self.input(X) # (N, Q) -&gt; (N, K) self.non_lin_transformed_X = self.non_lin_transform(linear_transformed_X) # (N, K) -&gt; (N, K) output = self.output(self.non_lin_transformed_X) # (N, K) -&gt; (N, D) return output . Q = 10 # Number of features N = 100 # Number of samples D = 15 # Number of outputs K = 32 # Number of hidden features X = torch.rand(N, Q) # Input Y = torch.rand(N, D) # Output . model = SingleLayerNetwork(Q=Q, D=D, K=K) model . SingleLayerNetwork( (input): Linear(in_features=10, out_features=32, bias=True) (output): Linear(in_features=32, out_features=15, bias=True) (non_lin_transform): ReLU() ) . for name, value in model.named_parameters(): print(name, value.shape) . input.weight torch.Size([32, 10]) input.bias torch.Size([32]) output.weight torch.Size([15, 32]) output.bias torch.Size([15]) . ReLU is does not contain any parameters here so it is merely a function. . 1.2 Model Uncertainty . In which cases we want our model to be uncertain? . When it encounters a out-of-the-distribution data | When training data is noisy (irreducible/aleatoric uncertainty) | When we have multiple predictors (model/epistemic uncertainty) | .",
            "url": "https://patel-zeel.github.io/blog/ml/2022/04/10/uncertainty-in-deep-learning.html",
            "relUrl": "/ml/2022/04/10/uncertainty-in-deep-learning.html",
            "date": " • Apr 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "GitHub Contrubuting FAQs",
            "content": "Q1: What is an efficient way to work on multiple issues at once? . Create separate branches for each issue. Do not work on the master branch. . Q1.1: Why not? . We will see that in Q5. . Q2: What to do if the main (or master) gets updated before I open a PR? . Pull the changes directly to your branch with: . git pull https://github.com/probml/pyprobml . Q3: What to do with the fork&#39;s main when the original main is updated? . Fetch upstream with GitHub GUI or use the same solution given in Q2. . Q4: Why and when keeping the fork&#39;s main up to date with the original main is important? . Whenever we need to create new branches (usually from the fork&#39;s main). . Q5: How to update a change in a PR that is open? . Push the change to the corresponding branch and PR will get updated automatically. .",
            "url": "https://patel-zeel.github.io/blog/github/2022/04/10/_04_06_GitHub_FAQs.html",
            "relUrl": "/github/2022/04/10/_04_06_GitHub_FAQs.html",
            "date": " • Apr 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Gcloud cheatsheet",
            "content": "Initial setup . Following this guide. . To set default email, project-id &amp; zone: gcloud config set account your-email-account gcloud config set project your-project-id gcloud config set compute/zone us-central1-f # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access) . | To get currently active project and zone related info: gcloud info . | To create an identity (I don’t know if this is required or not. This command should trigger installation of “gcloud Beta Commands” automatically in another shell and then you need to rerun the following command): gcloud beta services identity create --service tpu.googleapis.com . | . Working with TPU VMs . There are two different terms here: “TPU VMs” and “TPU nodes”. TPU nodes can be connected externally via another VM. TPU VMs are stand-alone systems with TPUs, RAM and CPU (96 core Intel 2 GHz processor and 335 GB RAM). We may be charged via GCP for the VM (CPUs and RAM). (I will update this info once I know for sure): . . To create a TPU VM in preferred zone via CLI (be careful about the --zone to avoid charges, check the first email received from TRC team to see what kind of TPUs are free in different zones. if --zone is not passed, VM will be created in the default zone that we set initially. This command triggered installation of “gcloud Alpha Commands”): gcloud alpha compute tpus tpu-vm create vm-1 --accelerator-type v2-8 --version tpu-vm-tf-2.8.0 --zone us-central1-f . | To get the list of TPU nodes/VMs: gcloud compute tpus list . | To delete a TPU node/VM: gcloud compute tpus delete vm-1 . | To connect with a vm via ssh (this automatically creates ssh key pair and places in default ssh config location): gcloud alpha compute tpus tpu-vm ssh vm-1 . | Follow this guide to create and attach a persistent disk with the TPU VM | . Working with TPU VMs via VS-code . Install the following extension on VS-code: . | Use the following button to connect to a remote machine (use “Connect to Host…” button): . | Manually update the default ssh config file (in my case, “C: Users zeelp .ssh”) to add a VM in VS-code (you can use VS-code command palette to figure out the config file for you and edit it. Please see the screeshot below). . | . . Note that ssh public-private key pair with name google_compute_engine is automatically generated when you connect with the VM for the first time with gcloud alpha compute tpus tpu-vm ssh command. The VM config for me looks like this: | . Host Cloud-TPU-Node-2 HostName &lt;External-IP-of-your-TPU-VM&gt; User zeelp Port 22 IdentityFile C: Users zeelp .ssh google_compute_engine .",
            "url": "https://patel-zeel.github.io/blog/markdown/2022/04/09/gcloud.html",
            "relUrl": "/markdown/2022/04/09/gcloud.html",
            "date": " • Apr 9, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Torch essentials",
            "content": "Let&#39;s go hands-on . import torch import numpy as np . tensor1 = torch.tensor([1,2,3.], dtype=torch.float32) tensor2 = torch.tensor([5,6,7.], dtype=torch.float64) display(tensor1, tensor2) . tensor([1., 2., 3.]) . tensor([5., 6., 7.], dtype=torch.float64) . display(type(tensor1), type(tensor2)) . torch.Tensor . torch.Tensor . display(tensor1.dtype, tensor2.dtype) . torch.float32 . torch.float64 . long_tensor = tensor1.to(torch.int32) # device, dtype, tensor display(long_tensor) . tensor([1, 2, 3], dtype=torch.int32) . long_tensor.device . device(type=&#39;cpu&#39;) . device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; long_tensor_gpu = long_tensor.to(device) long_tensor_gpu . tensor([1, 2, 3], device=&#39;cuda:0&#39;, dtype=torch.int32) . long_tensor_born_on_gpu = torch.zeros(2,10, device=device).to(torch.float64) long_tensor_born_on_gpu . tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device=&#39;cuda:0&#39;, dtype=torch.float64) . inspired_tensor = torch.tensor([1.,2.]).to(long_tensor_born_on_gpu) inspired_tensor . tensor([1., 2.], device=&#39;cuda:0&#39;, dtype=torch.float64) . np_array = np.array([1,2,3.]) np_array.log() . AttributeError Traceback (most recent call last) /tmp/ipykernel_7181/236375699.py in &lt;module&gt; 1 np_array = np.array([1,2,3.]) -&gt; 2 np_array.log() AttributeError: &#39;numpy.ndarray&#39; object has no attribute &#39;log&#39; . pt_array = torch.tensor([1,2,3.]) pt_array.log() # sin(), cos(), tan(), exp() . tensor([0.0000, 0.6931, 1.0986]) . Gradient is all you need . import matplotlib.pyplot as plt . x = torch.rand(5,1) y = 3 * x + 2 + torch.randn_like(x)*0.1 plt.scatter(x, y); . x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1) x_plus_ones.shape . torch.Size([5, 2]) . theta = torch.zeros(2,1, requires_grad=True) theta . tensor([[0.], [0.]], requires_grad=True) . theta.grad . theta.grad_fn . lr = 0.1 y_pred = x_plus_ones@theta loss = ((y_pred - y)**2).mean() loss.backward() # y_pred = torch.matmul(x_plus_ones, theta) # y_pred = torch.mm(x_plus_ones, theta) . theta.grad # dloss/dtheta . tensor([[-6.3681], [-2.8128]]) . theta.grad_fn . theta.data -= lr * theta.grad.data . theta . tensor([[0.6368], [0.2813]], requires_grad=True) . theta.grad_fn . with torch.no_grad(): plt.scatter(x, y) plt.plot(x, x_plus_ones@theta) . for i in range(10): theta.grad.data.zero_() y_pred = x_plus_ones@theta loss = ((y_pred - y)**2).mean() loss.backward() theta.data -= lr * theta.grad . with torch.no_grad(): plt.scatter(x, y) plt.plot(x, x_plus_ones@theta) . Advanced . class LinearRegression(torch.nn.Module): def __init__(self): super().__init__() self.theta = torch.nn.Parameter(torch.zeros(2,1)) # self.register_parameter(theta, torch.zeros(2,1)) def forward(self, x): # Don&#39;t call directly. it is called by __call__ method x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1) y_pred = x_plus_ones@self.theta return y_pred . model = LinearRegression() model . LinearRegression() . for name, value in model.named_parameters(): print(name, value) . theta Parameter containing: tensor([[0.], [0.]], requires_grad=True) . optimizer = torch.optim.Adam(model.parameters(), lr=0.1) loss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss() for i in range(10): optimizer.zero_grad() y_pred = model(x) loss = loss_fn(y_pred, y) loss.backward() optimizer.step() . model.state_dict() . OrderedDict([(&#39;theta&#39;, tensor([[0.9799], [0.9808]]))]) . Wanna run on GPU? . x_gpu = x.to(device) y_gpu = y.to(device) . print(model.theta) model.to(device) print(model.theta) . Parameter containing: tensor([[0.9799], [0.9808]], requires_grad=True) Parameter containing: tensor([[0.9799], [0.9808]], device=&#39;cuda:0&#39;, requires_grad=True) . optimizer = torch.optim.Adam(model.parameters(), lr=0.1) loss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss() for i in range(10): optimizer.zero_grad() y_pred = model(x_gpu) loss = loss_fn(y_pred, y_gpu) loss.backward() optimizer.step() . State dictionary . # model.load_state_dict(torch.load(path)) . NN way . class LinearRegression(torch.nn.Module): def __init__(self): super().__init__() self.layer = torch.nn.Linear(2, 1) # torch.nn.Linear(128, 64) # What else? # self.activation = torch.nn.ReLU() # torch.nn.LSTM() # torch.nn.Conv2d() def forward(self, x): # Don&#39;t call directly. it is called by __call__ method x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1) y_pred = self.layer(x_plus_ones) return y_pred .",
            "url": "https://patel-zeel.github.io/blog/ml/2022/03/08/torch-essentials.html",
            "relUrl": "/ml/2022/03/08/torch-essentials.html",
            "date": " • Mar 8, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Probabilistic Machine Learning",
            "content": "1 - Introduction . An inference problem requires statements about the value of an unobserved (latent) variable x based on observations y which are related to x, but may not be sufficient to fully determine x. This requires a notion of uncertainty. . We can define the following rules because $p(E) = 1$ for any event $E$. . Sum rule:$p(E) = p(E|A) + p(E| neg A)$ * Product rule: $p(E, A) = p(E|A)p(A) = p(A|E)p(E)$ | Bayes&#39; theorem: $p(E|A) = frac{p(A|E)p(E)}{p(A)}$ | .",
            "url": "https://patel-zeel.github.io/blog/ml/2022/03/06/probabilistic-machine-learning.html",
            "relUrl": "/ml/2022/03/06/probabilistic-machine-learning.html",
            "date": " • Mar 6, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "PyTorch Tips",
            "content": "Several tips for building torch models from scratch from my experience. Some of the tips are like zen, they are not immediately intuitive but useful for efficient code. . All the initializations or new tensor creation should only happen in __init__ method. During the forward() call, ideally no new tensors should be created from scratch such as torch.zeros(), torch.ones() etc. Reason: Violating this can sometimes brake your forward pass and end-to-end backprop may become buggy. . | .cuda() and .cpu() are discouraged, use .to(device) instead. Reason: .to(device) is more dynamic and scalable. . | Do not save models with torch.save(model), that may become incompaitable with different torch versions and may take more memory. Save torch.save(model.state_dict()) instead. . | Need to set parameter names dynamically? Use this example, zero=0;self.register_parameter(f&quot;name_{zero}&quot;). They can be accessed with model.name_0. . | Have something in model which is necessary for forward pass but does not require backprop? define those variables with self.register_buffer. . | Let .to(device) to be set outside the model defition. Reason: It is less confusing to the users this way and it is less messy with internal tools to set device such as: module.to(deivce) sends all parameters and buffers of model/submodules to the device. | . | module.float() or module.double() will convert all model/submodule parameters and buffers into float32 and float64 respectively. . | Let .train() and .eval() to be set outside the model defition or set by user. Reason: It can be confusing to user if these things are used inside the model against torch conventions. . | torch.no_grad() should not be used within the model. Reason: Sometimes user may want to backprop through that chunk of code. . | Link the multiple modules togather. Reason: Ideally, it is useful if model is built like a assembled product (say a car). You should be able to replace the parts as per your requirement. Several benefits on these lines are: setting module.train() or module.eval() puts all submodules in train mode or eval mode respectively. | All submodules parameters can be accesses directly from the parent module with module.parameters(). | . | Creating a list of parameters in model __init__ definition? consider torch.nn.ModuleList(params) else individual parameters in the list will not be recognized as parameters. | .",
            "url": "https://patel-zeel.github.io/blog/ml/2022/02/25/torch-tips.html",
            "relUrl": "/ml/2022/02/25/torch-tips.html",
            "date": " • Feb 25, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Conference Presentation Tips",
            "content": "General . First page goes like this: . Title . | . Authors (Underline presenting author, no need to put * incase of equal contribution) . Affiliations . Conference name . If importing figures from paper, avoid including the captions. | Include lot of images and less maths | Talk should end with summary and not the future work or thank you slide or something. | Cite the references on the same slide in bottom. | . Refere to “Giving talks” section of this blog. . Dos and Don’ts . Never put too detailed information difficult to grasp: a table with many numbers, a complex derivation all in one go, very complicated diagram. | .",
            "url": "https://patel-zeel.github.io/blog/academic/2022/01/29/presentation_tips.html",
            "relUrl": "/academic/2022/01/29/presentation_tips.html",
            "date": " • Jan 29, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Comparing Gaussian Process Regression Frameworks",
            "content": "import math import numpy as np import matplotlib.pyplot as plt import pandas as pd import torch import GPy import jax import gpytorch import botorch import tinygp import jax.numpy as jnp import optax from IPython.display import clear_output from sklearn.preprocessing import StandardScaler . WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) . Data . np.random.seed(0) # We don&#39;t want surprices in a presentation :) N = 10 train_x = torch.linspace(0, 1, N) train_y = torch.sin(train_x * (2 * math.pi)) + torch.normal(0, 0.1, size=(N,)) test_x = torch.linspace(0, 1, N*10) test_y = torch.sin(test_x * (2 * math.pi)) . plt.plot(train_x, train_y, &#39;ko&#39;, label=&#39;train&#39;); plt.plot(test_x, test_y, label=&#39;test&#39;); plt.legend(); . Defining kernel . begin{equation} sigma_f^2 = text{variance} ell = text{lengthscale} k_{RBF}(x_1, x_2) = sigma_f^2 exp left[- frac{ lVert x_1 - x_2 rVert^2}{2 ell^2} right] end{equation} GPy . gpy_kernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.) gpy_kernel . rbf. valueconstraintspriors . &lt;td class=tg-left&gt; variance &lt;/td&gt;&lt;td class=tg-right&gt; 1.0&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; lengthscale&lt;/td&gt;&lt;td class=tg-right&gt; 1.0&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . GPyTorch . gpytorch_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) gpytorch_kernel.outputscale = 1. # variance gpytorch_kernel.base_kernel.lengthscale = 1. # lengthscale gpytorch_kernel . ScaleKernel( (base_kernel): RBFKernel( (raw_lengthscale_constraint): Positive() ) (raw_outputscale_constraint): Positive() ) . TinyGP . def RBFKernel(variance, lengthscale): return jnp.exp(variance) * tinygp.kernels.ExpSquared(scale=jnp.exp(lengthscale)) tinygp_kernel = RBFKernel(variance=1., lengthscale=1.) tinygp_kernel . &lt;tinygp.kernels.Product at 0x7f544039d710&gt; . Define model . $$ sigma_n^2 = text{noise variance} $$ GPy . gpy_model = GPy.models.GPRegression(train_x.numpy()[:,None], train_y.numpy()[:,None], gpy_kernel) gpy_model.Gaussian_noise.variance = 0.1 gpy_model . Model: GP regression Objective: 16.757933772959404 Number of Parameters: 3 Number of Optimization Parameters: 3 Updates: True . GP_regression. valueconstraintspriors . &lt;td class=tg-left&gt; rbf.variance &lt;/td&gt;&lt;td class=tg-right&gt; 1.0&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; rbf.lengthscale &lt;/td&gt;&lt;td class=tg-right&gt; 1.0&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; Gaussian_noise.variance&lt;/td&gt;&lt;td class=tg-right&gt; 0.1&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . GPyTorch . class ExactGPModel(gpytorch.models.ExactGP): def __init__(self, train_x, train_y, likelihood, kernel): super().__init__(train_x, train_y, likelihood) self.mean_module = gpytorch.means.ConstantMean() self.covar_module = kernel def forward(self, x): mean_x = self.mean_module(x) covar_x = self.covar_module(x) return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) gpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood() gpytorch_model = ExactGPModel(train_x, train_y, gpytorch_likelihood, gpytorch_kernel) gpytorch_model.likelihood.noise = 0.1 gpytorch_model . ExactGPModel( (likelihood): GaussianLikelihood( (noise_covar): HomoskedasticNoise( (raw_noise_constraint): GreaterThan(1.000E-04) ) ) (mean_module): ConstantMean() (covar_module): ScaleKernel( (base_kernel): RBFKernel( (raw_lengthscale_constraint): Positive() ) (raw_outputscale_constraint): Positive() ) ) . TinyGP . def build_gp(theta, X): mean = theta[0] variance, lengthscale, noise_variance = jnp.exp(theta[1:]) kernel = variance * tinygp.kernels.ExpSquared(lengthscale) return tinygp.GaussianProcess(kernel, X, diag=noise_variance, mean=mean) tinygp_model = build_gp(theta=np.array([0., 1., 1., 0.1]), X=train_x.numpy()) tinygp_model # __repr__ . &lt;tinygp.gp.GaussianProcess at 0x7f5440401850&gt; . Train the model . GPy . gpy_model.optimize(max_iters=50) gpy_model . Model: GP regression Objective: 3.944394423452163 Number of Parameters: 3 Number of Optimization Parameters: 3 Updates: True . GP_regression. valueconstraintspriors . &lt;td class=tg-left&gt; rbf.variance &lt;/td&gt;&lt;td class=tg-right&gt; 0.9376905183253631&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; rbf.lengthscale &lt;/td&gt;&lt;td class=tg-right&gt; 0.2559000163858406&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; Gaussian_noise.variance&lt;/td&gt;&lt;td class=tg-right&gt;0.012506184441481319&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . GPyTorch . mll = gpytorch.mlls.ExactMarginalLogLikelihood(gpytorch_likelihood, gpytorch_model) botorch.fit_gpytorch_model(mll) display(gpytorch_model.mean_module.constant, # Mean gpytorch_model.covar_module.outputscale, # Variance gpytorch_model.covar_module.base_kernel.lengthscale, # Lengthscale gpytorch_model.likelihood.noise) # Noise variance . /opt/conda/lib/python3.7/site-packages/botorch/fit.py:143: UserWarning:CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1634272168290/work/c10/cuda/CUDAFunctions.cpp:112.) . Parameter containing: tensor([0.0923], requires_grad=True) . tensor(0.9394, grad_fn=&lt;SoftplusBackward0&gt;) . tensor([[0.2560]], grad_fn=&lt;SoftplusBackward0&gt;) . tensor([0.0124], grad_fn=&lt;AddBackward0&gt;) . TinyGP . from scipy.optimize import minimize def neg_log_likelihood(theta, X, y): gp = build_gp(theta, X) return -gp.condition(y) obj = jax.jit(jax.value_and_grad(neg_log_likelihood)) result = minimize(obj, [0., 1., 1., 0.1], jac=True, args=(train_x.numpy(), train_y.numpy())) result.x[0], np.exp(result.x[1:]) . (0.09213499552879165, array([0.9395271 , 0.25604163, 0.01243025])) . Inference . def plot_gp(pred_y, var_y): std_y = var_y ** 0.5 plt.figure() plt.scatter(train_x, train_y, label=&#39;train&#39;) plt.plot(test_x, pred_y, label=&#39;predictive mean&#39;) plt.fill_between(test_x.ravel(), pred_y.ravel() - 2*std_y.ravel(), pred_y.ravel() + 2*std_y.ravel(), alpha=0.2, label=&#39;95% confidence&#39;) plt.legend() . GPy . pred_y, var_y = gpy_model.predict(test_x.numpy()[:, None]) plot_gp(pred_y, var_y) . GPyTorch . gpytorch_model.eval() with torch.no_grad(), gpytorch.settings.fast_pred_var(): pred_dist = gpytorch_likelihood(gpytorch_model(test_x)) pred_y, var_y = pred_dist.mean, pred_dist.variance plot_gp(pred_y, var_y) . TinyGP . tinygp_model = build_gp(result.x, train_x.numpy()) pred_y, var_y = tinygp_model.predict(train_y.numpy(), test_x.numpy(), return_var=True) plot_gp(pred_y, var_y) . Tiny GP on CO2 dataset . data = pd.read_csv(&quot;data/co2.csv&quot;) # Train test split X = data[&quot;0&quot;].iloc[:290].values.reshape(-1, 1) X_test = data[&quot;0&quot;].iloc[290:].values.reshape(-1, 1) y = data[&quot;1&quot;].iloc[:290].values y_test = data[&quot;1&quot;].iloc[290:].values # Scaling the dataset Xscaler = StandardScaler() X = Xscaler.fit_transform(X) X_test = Xscaler.transform(X_test) yscaler = StandardScaler() y = yscaler.fit_transform(y.reshape(-1, 1)).ravel() y_test = yscaler.transform(y_test.reshape(-1, 1)).ravel() . plt.plot(X, y, label=&#39;train&#39;); plt.plot(X_test, y_test, label=&#39;test&#39;); plt.legend(); . class SpectralMixture(tinygp.kernels.Kernel): def __init__(self, weight, scale, freq): self.weight = jnp.atleast_1d(weight) self.scale = jnp.atleast_1d(scale) self.freq = jnp.atleast_1d(freq) def evaluate(self, X1, X2): tau = jnp.atleast_1d(jnp.abs(X1 - X2))[..., None] return jnp.sum( self.weight * jnp.prod( jnp.exp(-2 * jnp.pi ** 2 * tau ** 2 / self.scale ** 2) * jnp.cos(2 * jnp.pi * self.freq * tau), axis=-1, ) ) def build_spectral_gp(theta): kernel = SpectralMixture( jnp.exp(theta[&quot;log_weight&quot;]), jnp.exp(theta[&quot;log_scale&quot;]), jnp.exp(theta[&quot;log_freq&quot;]), ) return tinygp.GaussianProcess( kernel, X, diag=jnp.exp(theta[&quot;log_diag&quot;]), mean=theta[&quot;mean&quot;] ) . K = 4 # Number of mixtures div_factor = 0.4 np.random.seed(1) params = { &quot;log_weight&quot;: np.abs(np.random.rand(K))/div_factor, &quot;log_scale&quot;: np.abs(np.random.rand(K))/div_factor, &quot;log_freq&quot;: np.abs(np.random.rand(K))/div_factor, &quot;log_diag&quot;: np.abs(np.random.rand(1))/div_factor, &quot;mean&quot;: 0., } @jax.jit @jax.value_and_grad def loss(theta): return -build_spectral_gp(theta).condition(y) # opt = optax.sgd(learning_rate=0.001) opt = optax.adam(learning_rate=0.1) opt_state = opt.init(params) losses = [] for i in range(100): loss_val, grads = loss(params) updates, opt_state = opt.update(grads, opt_state) params = optax.apply_updates(params, updates) losses.append(loss_val) clear_output(wait=True) print(f&quot;iter {i}, loss {loss_val}&quot;) opt_gp = build_spectral_gp(params) params . iter 99, loss 27.987701416015625 . {&#39;log_diag&#39;: DeviceArray([-2.7388687], dtype=float32), &#39;log_freq&#39;: DeviceArray([-3.6072493, -3.1795945, -3.4490397, -2.373117 ], dtype=float32), &#39;log_scale&#39;: DeviceArray([3.9890492, 3.8530042, 4.0878096, 4.4860597], dtype=float32), &#39;log_weight&#39;: DeviceArray([-1.3715047, -0.6132469, -2.413771 , -1.6582283], dtype=float32), &#39;mean&#39;: DeviceArray(0.38844627, dtype=float32)} . plt.plot(losses); . mu, var = opt_gp.predict(y, X_test, return_var=True) plt.plot(X, y, c=&#39;k&#39;) plt.fill_between( X_test.ravel(), mu + np.sqrt(var), mu - np.sqrt(var), color=&quot;C0&quot;, alpha=0.5 ) plt.plot(X_test, mu, color=&quot;C0&quot;, lw=2) # plt.xlim(t.min(), 2025) plt.xlabel(&quot;year&quot;) _ = plt.ylabel(&quot;CO$_2$ in ppm&quot;) . Idea . $K_ mathbf{y} = text{cov_function}(X_{train}, X_{train}, sigma_f, ell, sigma_n)$ . GP Loss: $ log p( mathbf{y} mid mathbf{X}, theta)=- frac{1}{2} mathbf{y}^{T} K_{y}^{-1} mathbf{y}- frac{1}{2} log left|K_{y} right|- frac{n}{2} log 2 pi$ . Minimize inverse term fully | Now, Minimize both togather | Appendix . gp_model = ExactGPModel(train_x, train_y, gpytorch.likelihoods.GaussianLikelihood(), gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())) zmll = gpytorch.ExactMarginalLogLikelihood(gp_model.likelihood, gp_model) def loss1(model, x, y): l = torch.cholesky(model.likelihood(model(x)).covariance_matrix) alp = torch.cholesky_solve(y.reshape(-1,1), l) return (y.reshape(-1,1).T@alp).ravel() def loss_det(model, x, y): l = torch.cholesky(model.likelihood(model(x)).covariance_matrix) return torch.log(l.diagonal()).sum() def loss_full(model, x, y): dist = model(x) return -zmll(dist, y) zopt = torch.optim.Adam(gp_model.parameters(), lr=0.1) torch.manual_seed(0) for p in gp_model.parameters(): torch.nn.init.normal_(p) zlosses1 = [] zlosses2 = [] zlosses = [] for i in range(100): clear_output(True) print(i) zopt.zero_grad() zloss = 0 zloss = zloss + loss1(gp_model, train_x, train_y) zloss = zloss + loss_det(gp_model, train_x, train_y) zloss.backward() zopt.step() zlosses1.append(loss1(gp_model, train_x, train_y).item()) zlosses2.append(loss_det(gp_model, train_x, train_y).item()) zlosses.append(loss_full(gp_model, train_x, train_y).item()) # for i in range(100): # clear_output(True) # print(i) # zopt.zero_grad() # zloss = 0 # zloss = zloss + loss_det(gp_model, train_x, train_y) # zloss = zloss + loss1(gp_model, train_x, train_y) # zloss.backward() # zopt.step() # zlosses.append(loss2(gp_model, train_x, train_y).item()) plt.plot(zlosses1, label=&#39;inverse term&#39;); plt.plot(zlosses2, label=&#39;log det term&#39;); plt.plot(zlosses, label=&#39;full loss&#39;); plt.legend(); plt.figure() plt.plot(zlosses, c=&#39;g&#39;); . 99 . gp_model.eval() with torch.no_grad(), gpytorch.settings.fast_pred_var(): pdist = gp_model.likelihood(gp_model(test_x)) plot_gp(pdist.mean, pdist.variance); plt.ylim(-2,2) .",
            "url": "https://patel-zeel.github.io/blog/ml/2022/01/25/GP_Frameworks_Comparison.html",
            "relUrl": "/ml/2022/01/25/GP_Frameworks_Comparison.html",
            "date": " • Jan 25, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Query by Committee",
            "content": "import numpy as np import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation from matplotlib import rc plt.style.use(&#39;fivethirtyeight&#39;) rc(&#39;animation&#39;, html=&#39;jshtml&#39;) # Copy the models from copy import deepcopy # Sklearn imports from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, f1_score # Entropy function from scipy.stats import entropy # Progress helper from IPython.display import clear_output . QBC by posterior sampling . Interesting fact: For probabilistic models, QBC is similar to uncertainty sampling. How? . #### Draw $k$ parameter sets from the posterior distribution representing $k$ different models. | #### Query a point which shows maximum disagreement among the points. | . An example: Bayesian linear regression . np.random.seed(0) N = 10 X = np.linspace(-1,1,N).reshape(-1,1) t0 = 3 t1 = 2 y = X * t1 + t0 + np.random.rand(N,1) plt.scatter(X, y); . Assume a posterior . n_samples = 50 t0_dist_samples = np.random.normal(t0, 0.1, size=n_samples) t1_dist_samples = np.random.normal(t1, 1, size=n_samples) . Plot the models . plt.scatter(X, y) for i in range(len(t0_dist_samples)): sample_t0 = t0_dist_samples[i] sample_t1 = t1_dist_samples[i] plt.plot(X, X * sample_t1 + sample_t0,alpha=0.1) . QBC by bootstrapping . 2 class dataset . X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=3, shuffle=True) plt.figure() plt.scatter(X[:,0], X[:,1], c=y); . Full data fit with RF . model = RandomForestClassifier(random_state=0) model.fit(X, y); . RandomForestClassifier(random_state=0) . Visualize decision boundary . grid_X1, grid_X2 = np.meshgrid(np.linspace(X[:,0].min()-0.1, X[:,0].max()+0.1, 100), np.linspace(X[:,1].min()-0.1, X[:,1].max()+0.1, 100)) grid_X = [(x1, x2) for x1, x2 in zip(grid_X1.ravel(), grid_X2.ravel())] grid_pred = model.predict(grid_X) plt.figure(figsize=(6,5)) plt.scatter(X[:,0], X[:,1], c=y); plt.contourf(grid_X1, grid_X2, grid_pred.reshape(*grid_X1.shape), alpha=0.2); . Train, pool, test split . X_train_pool, X_test, y_train_pool, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y) X_train, X_pool, y_train, y_pool = train_test_split(X_train_pool, y_train_pool, train_size=20, random_state=0) X_list = [X_train, X_pool, X_test] y_list = [y_train, y_pool, y_test] t_list = [&#39;Train&#39;, &#39;Pool&#39;, &#39;Test&#39;] fig, ax = plt.subplots(1,3,figsize=(15,4), sharex=True, sharey=True) for i in range(3): ax[i].scatter(X_list[i][:,0], X_list[i][:,1], c=y_list[i]) ax[i].set_title(t_list[i]) . Fitting a model on initial train data . AL_model = RandomForestClassifier(n_jobs=28, random_state=0) AL_model.fit(X_train, y_train); . RandomForestClassifier(n_jobs=28, random_state=0) . Get the votes from trees on pool dataset . votes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_))) for learner_idx, learner in enumerate(AL_model.estimators_): votes[:, learner_idx] = learner.predict(X_pool) . votes.shape . (780, 100) . votes . array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [1., 1., 1., ..., 0., 1., 1.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) . Convert to probabilities . p_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1])) for vote_idx, vote in enumerate(votes): vote_counter = {0 : (1-vote).sum(), 1 : vote.sum()} for class_idx, class_label in enumerate(range(X.shape[1])): p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_) . p_vote . array([[1. , 0. ], [0.89, 0.11], [0.06, 0.94], ..., [0.93, 0.07], [1. , 0. ], [1. , 0. ]]) . Calculate dissimilarity (entropy) . example_id = 2 . ans = 0 for category in range(X_pool.shape[1]): ans += (-p_vote[example_id][category] * np.log(p_vote[example_id][category])) ans . 0.22696752250060448 . entr = entropy(p_vote, axis=1) . entr[example_id] . 0.22696752250060448 . Active Learning Flow . def get_query_idx(): # Gather the votes votes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_))) for learner_idx, learner in enumerate(AL_model.estimators_): votes[:, learner_idx] = learner.predict(X_pool) # Calcuate probability of votes p_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1])) for vote_idx, vote in enumerate(votes): vote_counter = {0 : (1-vote).sum(), 1 : vote.sum()} for class_idx, class_label in enumerate(range(X.shape[1])): p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_) # Calculate entropy for each example entr = entropy(p_vote, axis=1) # Choose example with highest entropy (disagreement) return entr.argmax() . Prepare data for random sampling . X_train_rand = X_train.copy() y_train_rand = y_train.copy() X_pool_rand = X_pool.copy() y_pool_rand = y_pool.copy() random_model = RandomForestClassifier(n_jobs=28, random_state=0) . Run active learning . AL_iters = 100 np.random.seed(0) AL_inds = [] AL_models = [] random_inds = [] random_models = [] for iteration in range(AL_iters): clear_output(wait=True) print(&quot;iteration&quot;, iteration) ######## Active Learning ############ # Fit the model AL_model.fit(X_train, y_train) AL_models.append(deepcopy(AL_model)) # Query a point query_idx = get_query_idx() AL_inds.append(query_idx) # Add it to the train data X_train = np.concatenate([X_train, X_pool[query_idx:query_idx+1, :]], axis=0) y_train = np.concatenate([y_train, y_pool[query_idx:query_idx+1]], axis=0) # Remove it from the pool data X_pool = np.delete(X_pool, query_idx, axis=0) y_pool = np.delete(y_pool, query_idx, axis=0) ######## Random Sampling ############ # Fit the model random_model.fit(X_train_rand, y_train_rand) random_models.append(deepcopy(random_model)) # Query a point query_idx = np.random.choice(len(X_pool)) random_inds.append(query_idx) # Add it to the train data X_train_rand = np.concatenate([X_train_rand, X_pool_rand[query_idx:query_idx+1, :]], axis=0) y_train_rand = np.concatenate([y_train_rand, y_pool_rand[query_idx:query_idx+1]], axis=0) # Remove it from the pool data X_pool_rand = np.delete(X_pool_rand, query_idx, axis=0) y_pool_rand = np.delete(y_pool_rand, query_idx, axis=0) . iteration 99 . Plot accuracy . random_scores = [] AL_scores = [] for iteration in range(AL_iters): clear_output(wait=True) print(&quot;iteration&quot;, iteration) AL_scores.append(accuracy_score(y_test, AL_models[iteration].predict(X_test))) random_scores.append(accuracy_score(y_test, random_models[iteration].predict(X_test))) plt.plot(AL_scores, label=&#39;Active Learning&#39;); plt.plot(random_scores, label=&#39;Random Sampling&#39;); plt.legend(); plt.xlabel(&#39;Iterations&#39;); plt.ylabel(&#39;Accuracy n(Higher is better)&#39;); . iteration 99 . Plot decision boundary . def update(i): for each in ax: each.cla() AL_grid_preds = AL_models[i].predict(grid_X) random_grid_preds = random_models[i].predict(grid_X) # Active learning ax[0].scatter(X_train[:n_train,0], X_train[:n_train,1], c=y_train[:n_train], label=&#39;initial_train&#39;, alpha=0.2) ax[0].scatter(X_train[n_train:n_train+i, 0], X_train[n_train:n_train+i, 1], c=y_train[n_train:n_train+i], label=&#39;new_points&#39;) ax[0].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2); ax[0].set_title(&#39;New points&#39;) ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label=&#39;test_set&#39;) ax[1].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2); ax[1].set_title(&#39;Test points&#39;); ax[0].text(locs[0],locs[1],&#39;Active Learning&#39;) # Random sampling ax[2].scatter(X_train_rand[:n_train,0], X_train_rand[:n_train,1], c=y_train_rand[:n_train], label=&#39;initial_train&#39;, alpha=0.2) ax[2].scatter(X_train_rand[n_train:n_train+i, 0], X_train_rand[n_train:n_train+i, 1], c=y_train_rand[n_train:n_train+i], label=&#39;new_points&#39;) ax[2].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2); ax[2].set_title(&#39;New points&#39;) ax[3].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label=&#39;test_set&#39;) ax[3].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2); ax[3].set_title(&#39;Test points&#39;); ax[2].text(locs[0],locs[1],&#39;Random Sampling&#39;); . locs = (2.7, 4) fig, ax = plt.subplots(2,2,figsize=(12,6), sharex=True, sharey=True) ax = ax.ravel() n_train = X_train.shape[0]-AL_iters anim = FuncAnimation(fig, func=update, frames=range(100)) plt.close() anim . &lt;/input&gt; Once Loop Reflect",
            "url": "https://patel-zeel.github.io/blog/ml/2022/01/24/Query_by_Committee.html",
            "relUrl": "/ml/2022/01/24/Query_by_Committee.html",
            "date": " • Jan 24, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "KL divergence v/s cross-entropy",
            "content": "Ground . In a classification problem, for a data-point $ mathbf{x}_i$, we have the true label $y_i$ associated with it. . Let us assume that we have three possible outcomes ${L1, L2, L3}$ and for current $ mathbf{x}_i$, corresponding $y_i$ is $L2$. Then Ground truth probability distribution is the following: . pG(y=L1)=0pG(y=L2)=1pG(y=L3)=0p_G(y = L1) = 0 p_G(y = L2) = 1 p_G(y=L3) = 0pG​(y=L1)=0pG​(y=L2)=1pG​(y=L3)=0 . Let us assume that our classifier model Predicted the following distribution: . pP(y=L1)=0.1pP(y=L2)=0.8pP(y=L3)=0.1p_P(y = L1) = 0.1 p_P(y = L2) = 0.8 p_P(y=L3) = 0.1pP​(y=L1)=0.1pP​(y=L2)=0.8pP​(y=L3)=0.1 . KL divergence . We can use KL divergence to check how good is our model. The formula is: . DKL(pG  ∥  pP)=∑yi∈{L1,L2,L3}pG(yi)log⁡pG(yi)pP(yi)D_{KL}(p_G ; rVert ;p_P) = sum_{y_i in {L1, L2, L3 }} p_G(y_i) log frac{p_G(y_i)}{p_P(y_i)}DKL​(pG​∥pP​)=yi​∈{L1,L2,L3}∑​pG​(yi​)logpP​(yi​)pG​(yi​)​ . For our example, . DKL(pG  ∥  pP)=log⁡10.8D_{KL}(p_G ; rVert ;p_P) = log frac{1}{0.8}DKL​(pG​∥pP​)=log0.81​ . It is evident that if $p_P(y = L2)$ decreses from $0.8$, $D_{KL}(p_G ; rVert ;p_P)$ will increase and vice versa. Note that KL divergence is not symmetric which means $D_{KL}(p_G ; rVert ;p_P) ne D_{KL}(p_P ; rVert ;p_G)$. . Cross-entory . Cross-entropy is another measure for distribution similarity. The formula is: . H(pG,pP)=∑yi∈{L1,L2,L3}−pG(yi)log⁡pP(yi)H(p_G, p_P) = sum_{y_i in {L1, L2, L3 }} - p_G(y_i) log p_P(y_i)H(pG​,pP​)=yi​∈{L1,L2,L3}∑​−pG​(yi​)logpP​(yi​) . For our example: . H(pG,pP)=−log⁡0.8=log⁡10.8H(p_G, p_P) = - log 0.8 = log frac{1}{0.8}H(pG​,pP​)=−log0.8=log0.81​ . KL divergence v/s cross-entropy . This shows that KL divergence and cross-entropy will return the same values for a simple classification problem. Then why do we use cross-entropy as a loss function and not KL divergence? . That’s because KL divergence will compute additional constant terms (zero here) that are not adding any value in minimization. .",
            "url": "https://patel-zeel.github.io/blog/ml/2022/01/20/kl-divergence.html",
            "relUrl": "/ml/2022/01/20/kl-divergence.html",
            "date": " • Jan 20, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Why .py files are better than .ipynb files for ML codebase",
            "content": "I have shifted from .ipynb files to .py files (and Jupyter to VS code) in the last couple of months. Here are some reasons why I feel .py files are better than .ipynb files: . Fewer Errors . .py files are easier to debug with a VS code like IDE, making it easier to find the errors. | Execution of .py starts fresh, unlike some left out variables silently getting carried over from the last execution/deleted cells in .ipynb files. | . Better Usage of a Shared Server . .py files release the resources (e.g., GPU memory) once executed. It could be inconvenient to repeatedly remind or be reminded by someone to release the resources manually from a Jupyter notebook. | . Increased Productivity . You can make use of fantastic auto-complete, syntax-highlighting extensions in VS code to save a lot of time while working with .py files. | . Boost Collaboration . .py do not take time to render on GitHub because they are just plain text files, unlike .ipynb files. | It is a lot easier to see the changes made by others in a .py file than a .ipynb file. | . Increased Modularity . Function and Class calls from other files are seamless with .py files. | . Feel free to comment your views/suggestions/additions in the comment box. .",
            "url": "https://patel-zeel.github.io/blog/markdown/2022/01/15/py_over_ipynb.html",
            "relUrl": "/markdown/2022/01/15/py_over_ipynb.html",
            "date": " • Jan 15, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Anonymization tips for double-blind submission",
            "content": "Use following command locally to search for author names, institute name and other terms you think may violate double-blind . git grep &lt;query&gt; . Above command matches the query everywhere and thus a safe way. Avoid GitHub search for this purpose, it fails to identify some terms many times and there is no regex there (yet)! . | Do not use full paths inside README file. If you move content in other repo, the links will either become unusable or may violate double-blind. So follow the example below. . Bad practice: [link](https://github.com/patel-zeel/reponame/blob/master/dataset) | Good practice: [link](dataset) | . | Point no. 2 does not work for GitHub pages links (username.github.io/stuff). Thus, keep in mind to manually update those (if you have a better idea, let everyone know in comments below) . | Download the repo zip locally and create an anonymized repository in your anonymized GitHub account. Open the GitHub web editor by pressing &quot;.&quot; (dot) at repo homepage. . | Now, you can select and drag all folders to the left pan of the web editor to upload them at once. Finally, commit with a meaningfull message and the changes will automatically be uploaded to the mail branch of your anonymized repo. . | Update the link in your manuscipt and submit !! . | Edit: . After acceptance, transfer the ownership to personal account and delete the ownership of anonymized account from the personal account. This will remove all the traces of repository from the anonymized account. However, repository will still show that the commits were made by anonymized account which is anyway not violation of the doule-blind explicitely. .",
            "url": "https://patel-zeel.github.io/blog/academic/2021/10/26/Anonymization-Tips.html",
            "relUrl": "/academic/2021/10/26/Anonymization-Tips.html",
            "date": " • Oct 26, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Input Warped GPs - A failed idea",
            "content": "Comments . We are warping inputs $ mathbf{x}$ into $ mathbf{w} cdot mathbf{x}$ | Learning second level GP over $ mathbf{w}$. | Appling penalty over $ mathbf{w}$ if varies too much unnecessary. | See problems at the end of the notebook. | We need to check mathematical concerns related to this transformation. | . import math import numpy as np import torch import gpytorch from matplotlib import pyplot as plt import regdata as rd from sklearn.cluster import KMeans . class ExactGPModel(gpytorch.models.ExactGP): def __init__(self, train_x, train_y, likelihood): super(ExactGPModel, self).__init__(train_x, train_y, likelihood) self.mean_module = gpytorch.means.ConstantMean() self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) def forward(self, x): mean_x = self.mean_module(x) covar_x = self.covar_module(x) return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) class ExactNSGPModel(gpytorch.models.ExactGP): def __init__(self, train_x, train_y, likelihood, num_latent): super(ExactNSGPModel, self).__init__(train_x, train_y, likelihood) # inds = np.random.choice(train_x.shape[0], size=num_latent, replace=False) # self.x_bar = train_x[inds] self.x_bar = torch.tensor(KMeans(n_clusters=num_latent).fit(train_x).cluster_centers_).to(train_x) self.w_bar = torch.nn.Parameter(torch.ones(num_latent,).to(self.x_bar)) self.bias = torch.nn.Parameter(torch.zeros(1,).to(self.x_bar)) self.latent_likelihood = gpytorch.likelihoods.GaussianLikelihood() # We can fix noise to be minimum but it is not ideal. Ideally, noise should automatically reduce to reasonable value. # self.latent_likelihood.raw_noise.requires_grad = False # self.latent_likelihood.raw_noise = torch.tensor(-10.) self.latent_model = ExactGPModel(self.x_bar, self.w_bar, self.latent_likelihood) self.mean_module = gpytorch.means.ConstantMean() self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) def forward(self, x): self.latent_model.eval() with gpytorch.settings.detach_test_caches(False): # needed to back propagate thru predictive posterior self.latent_model.set_train_data(self.x_bar, self.w_bar, strict=False) self.w = self.latent_likelihood(self.latent_model(x)) # predictive posterior x_warped = x*self.w.mean[:, None] + self.bias mean_x = self.mean_module(x_warped) covar_x = self.covar_module(x_warped) return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) . def training(model, likelihood): training_iter = 100 # Find optimal model hyperparameters model.train() likelihood.train() # Use the adam optimizer optimizer = torch.optim.Adam([ {&#39;params&#39;: model.parameters()}, # Includes GaussianLikelihood parameters ], lr=0.1) # &quot;Loss&quot; for GPs - the marginal log likelihood mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) for i in range(training_iter): # Zero gradients from previous iteration optimizer.zero_grad() # Output from model output = model(train_x) # Calc loss and backprop gradients try: loss = -mll(output, train_y) + torch.square(model.w.mean-1).mean() # print(model.latent_likelihood.noise) except AttributeError: loss = -mll(output, train_y) loss.backward() # print(&#39;Iter %d/%d - Loss: %.3f lengthscale: %.3f noise: %.3f&#39; % ( # i + 1, training_iter, loss.item(), # model.covar_module.base_kernel.lengthscale.item(), # model.likelihood.noise.item() # )) optimizer.step() def predict_plot(model, likelihood, title): # Get into evaluation (predictive posterior) mode model.eval() likelihood.eval() # Test points are regularly spaced along [0,1] # Make predictions by feeding model through likelihood with torch.no_grad(): observed_pred = likelihood(model(test_x)) with torch.no_grad(): # Initialize plot f, ax = plt.subplots(1, 1, figsize=(10, 6)) # Get upper and lower confidence bounds lower, upper = observed_pred.confidence_region() # Plot training data as black stars ax.plot(train_x.numpy(), train_y.numpy(), &#39;k*&#39;) # Plot predictive means as blue line ax.plot(test_x.numpy(), observed_pred.mean.numpy(), &#39;b&#39;) # Shade between the lower and upper confidence bounds ax.fill_between(test_x.numpy().ravel(), lower.numpy(), upper.numpy(), alpha=0.5) ax.legend([&#39;Observed Data&#39;, &#39;Mean&#39;, &#39;Confidence&#39;]) ax.set_title(title) return observed_pred . def GP(num_latent): # initialize likelihood and model likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(train_x, train_y, likelihood) training(model, likelihood) predict_plot(model, likelihood, &#39;GP&#39;) def NSGP(num_latent): # initialize likelihood and model likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactNSGPModel(train_x, train_y, likelihood, num_latent) training(model, likelihood) observed_pred = predict_plot(model, likelihood, &#39;NSGP&#39;) with torch.no_grad(): model.train() model.forward(test_x) plt.figure(figsize=(10,6)) plt.plot(test_x*model.w.mean[:, None], observed_pred.mean.numpy()) plt.title(&#39;Warped test inputs v/s test outputs&#39;) with torch.no_grad(): model.train() model.forward(test_x) plt.figure(figsize=(10,6)) plt.plot(test_x, model.w.mean, label=&#39;interpolated&#39;) plt.scatter(model.x_bar, model.w_bar, label=&#39;learned&#39;) plt.ylim(0,2) plt.title(&#39;Test input v/s weights&#39;) plt.legend() . Testing over various datasets . train_x, train_y, test_x = rd.DellaGattaGene(backend=&#39;torch&#39;).get_data() GP(0) NSGP(num_latent=7) . train_x, train_y, test_x = rd.Heinonen4(backend=&#39;torch&#39;).get_data() GP(0) NSGP(num_latent=10) . train_x, train_y, test_x = rd.Jump1D(backend=&#39;torch&#39;).get_data() GP(0) NSGP(num_latent=10) . train_x, train_y, test_x = rd.MotorcycleHelmet(backend=&#39;torch&#39;).get_data() GP(0) NSGP(num_latent=10) . train_x, train_y, test_x = rd.Olympic(backend=&#39;torch&#39;).get_data() GP(0) NSGP(num_latent=10) . train_x, train_y, test_x = rd.SineJump1D(backend=&#39;torch&#39;).get_data() GP(0) NSGP(num_latent=10) . train_x, train_y, test_x = rd.SineNoisy(backend=&#39;torch&#39;).get_data() GP(0) NSGP(num_latent=10) . Problems . Transformation from x to x_warped is not monotonic. | .",
            "url": "https://patel-zeel.github.io/blog/ml/2021/10/23/Warped-GP.html",
            "relUrl": "/ml/2021/10/23/Warped-GP.html",
            "date": " • Oct 23, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "SparseGPs in Stheno",
            "content": "Imports . . import regdata as rd import torch import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation from matplotlib import rc import wbml.out as out from wbml.plot import tweak from stheno import B, GP, EQ, PseudoObsVFE, PseudoObsFITC from varz.torch import Vars, minimise_l_bfgs_b, parametrised, Positive import lab.torch . Data preperation . x = B.linspace(0, 10, 100) x_obs = B.linspace(0, 7, 50_000) x_ind = B.linspace(0, 10, 20) # Construct a prior. f = GP(EQ().periodic(2 * B.pi)) # Sample a true, underlying function and observations. f_true = B.sin(x) y_obs = B.sin(x_obs) + B.sqrt(0.5) * B.randn(*x_obs.shape) . Plotting function . def plot(method): if method == &#39;VFE&#39;: # Plot result. plt.plot(x, f_true, label=&quot;True&quot;, style=&quot;test&quot;) plt.scatter( x_obs, y_obs, label=&quot;Observations&quot;, style=&quot;train&quot;, c=&quot;tab:green&quot;, alpha=0.35, ) plt.scatter( x_ind, obs.mu(f.measure)[:, 0], label=&quot;Inducing Points&quot;, style=&quot;train&quot;, s=20, ) plt.plot(x, mean, label=&quot;Prediction&quot;, style=&quot;pred&quot;) plt.fill_between(x, lower, upper, style=&quot;pred&quot;) tweak() plt.show() else: # Plot result. plt.plot(x, f_true, label=&quot;True&quot;, style=&quot;test&quot;) plt.scatter( x_obs, y_obs, label=&quot;Observations&quot;, style=&quot;train&quot;, c=&quot;tab:green&quot;, alpha=0.35, ) plt.scatter( x_ind, B.dense(f_post(x_ind).mean), label=&quot;Inducing Points&quot;, style=&quot;train&quot;, s=20, ) plt.plot(x, mean, label=&quot;Prediction&quot;, style=&quot;pred&quot;) plt.fill_between(x, lower, upper, style=&quot;pred&quot;) tweak() plt.show() . Sparse regression with Variational Free Energy (VFE) method . obs = PseudoObsVFE(f(x_ind), (f(x_obs, 0.5), y_obs)) # Compute the ELBO. out.kv(&quot;ELBO&quot;, obs.elbo(f.measure)) # Compute the approximate posterior. f_post = f | obs # Make predictions with the approximate posterior. mean, lower, upper = f_post(x, 0.5).marginal_credible_bounds() plot(&#39;VFE&#39;) . ELBO: -5.345e+04 . Sparse Regression with Fully Independent Training Conditional (FITC) mehod . obs = PseudoObsFITC(f(x_ind), (f(x_obs, 0.5), y_obs)) # Compute the ELBO. out.kv(&quot;ELBO&quot;, obs.elbo(f.measure)) # Compute the approximate posterior. f_post = f | obs # Make predictions with the approximate posterior. mean, lower, upper = f_post(x, 0.5).marginal_credible_bounds() plot(&#39;FITC&#39;) . ELBO: -5.345e+04 . Hyperparameter tuning (Noisy Sine data) . def model(vs): &quot;&quot;&quot;Constuct a model with learnable parameters.&quot;&quot;&quot; return vs[&#39;variance&#39;]*GP(EQ().stretch(vs[&#39;length_scale&#39;])) . torch.manual_seed(123) dataObj = rd.SineNoisy(scale_X=False, scale_y=False, return_test=True, backend=&#39;torch&#39;) x_obs, y_obs, x = dataObj.get_data() . plt.scatter(x_obs, y_obs, s=2); . VFE . vs = Vars(torch.float64) vs.positive(name=&quot;noise&quot;) vs.positive(name=&quot;length_scale&quot;); vs.positive(name=&quot;variance&quot;); vs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name=&#39;x_ind&#39;) vs.requires_grad(True) optimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1) fig, ax = plt.subplots(1,2,figsize=(15,5)) losses = [] def update(i): optimizer.zero_grad() gp = model(vs) obs = PseudoObsVFE(gp(vs[&#39;x_ind&#39;]), (gp(x_obs, vs[&#39;noise&#39;]), y_obs)) loss = -obs.elbo(gp.measure) losses.append(loss.item()) loss.backward() optimizer.step() gp_post = gp | obs mean, lower, upper = gp_post(x, vs[&#39;noise&#39;]).marginal_credible_bounds() ind_mean = B.dense(gp_post(vs[&#39;x_ind&#39;]).mean) ax[0].cla();ax[1].cla(); ax[0].scatter(x_obs, y_obs, s=2) with torch.no_grad(): ax[0].plot() ax[0].plot(x, B.dense(mean), label=&#39;Prediction&#39;) ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label=&#39;Uncertainty&#39;) ax[0].plot(x, dataObj.f(x), label=&#39;True&#39;) ax[0].scatter(vs[&#39;x_ind&#39;], ind_mean, label=&#39;Inducing points&#39;) ax[0].set_xlabel(&#39;X&#39;) ax[0].legend() ax[1].plot(losses, label=&#39;loss&#39;) ax[1].set_xlabel(&#39;Iterations&#39;) ax[1].legend() anim = FuncAnimation(fig, update, range(50)) rc(&#39;animation&#39;, html=&#39;jshtml&#39;) plt.close() anim . &lt;/input&gt; Once Loop Reflect FITC . vs = Vars(torch.float64) vs.positive(name=&quot;noise&quot;) vs.positive(name=&quot;length_scale&quot;); vs.positive(name=&quot;variance&quot;); vs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name=&#39;x_ind&#39;) vs.requires_grad(True) optimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1) fig, ax = plt.subplots(1,2,figsize=(15,5)) losses = [] def update(i): optimizer.zero_grad() gp = model(vs) obs = PseudoObsFITC(gp(vs[&#39;x_ind&#39;]), (gp(x_obs, vs[&#39;noise&#39;]), y_obs)) loss = -obs.elbo(gp.measure) losses.append(loss.item()) loss.backward() optimizer.step() gp_post = gp | obs mean, lower, upper = gp_post(x, vs[&#39;noise&#39;]).marginal_credible_bounds() ind_mean = B.dense(gp_post(vs[&#39;x_ind&#39;]).mean) ax[0].cla();ax[1].cla(); ax[0].scatter(x_obs, y_obs, s=2) with torch.no_grad(): ax[0].plot() ax[0].plot(x, B.dense(mean), label=&#39;Prediction&#39;) ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label=&#39;Uncertainty&#39;) ax[0].plot(x, dataObj.f(x), label=&#39;True&#39;) ax[0].scatter(vs[&#39;x_ind&#39;], ind_mean, label=&#39;Inducing points&#39;) ax[0].set_xlabel(&#39;X&#39;) ax[0].legend() ax[1].plot(losses, label=&#39;loss&#39;) ax[1].set_xlabel(&#39;Iterations&#39;) ax[1].legend() anim = FuncAnimation(fig, update, range(50)) rc(&#39;animation&#39;, html=&#39;jshtml&#39;) plt.close() anim . &lt;/input&gt; Once Loop Reflect",
            "url": "https://patel-zeel.github.io/blog/2021/10/12/SparseGPs.html",
            "relUrl": "/2021/10/12/SparseGPs.html",
            "date": " • Oct 12, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Docker Cheatsheet",
            "content": "Images . One can download (pull) a docker image from Docker Hub or respective websites: . # docker pull &lt;image-name&gt;:&lt;tag&gt; docker pull tensorflow/tensorflow:2.4.0-gpu-jupyter . Check the downloaded images with: . docker images . Sometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping: . docker run --rm -it tensorflow/tensorflow:2.4.0-gpu-jupyter . Containers . Create a new container from an image with following flags . -v: for telling docker to use a shared directory between host and container | -p &lt;host-port &gt; &lt;container-port &gt;: is to use port forwarding, an application running on &lt;container-port&gt; can be accessed only with &lt;host-port&gt; in host. | --name generates a name for the container for easy reference in other commands | --gpus all tells docker to use all GPUs available on host | --memory-swap Restricts RAM+Swap usage | -it makes sure container awaits after starting instead of instantly shutting down if no startup scripts are configured. | To create new container with default params: . docker create tensorflow/tensorflow:2.4.0-gpu-jupyter . To create new container with manual params: . docker create -it -v path_in_host: path_in_container -p9000:8888 --name aaai --cpus 2 --gpus all # To use specific gpus: --gpu &#39;&quot;device=0,2&quot;&#39; --memory 90g # Uses 90g memory --memory-swap 100g # --memory-swap is a modifier flag that only has meaning if --memory is also set. In this case 10g of swap will be used. tensorflow/tensorflow:2.4.0-gpu-jupyter . Update some of the above configurations after container creation: . # change RAM limit of a container named &quot;aaai&quot; docker update --memory-swap 50g aaai . Note: In general, changes made to container persist when container is stopped. . Check containers: . docker ps # shows running containers docker ps -a # shows all containers . Start a container (default script will be executed with this if any): . # docker start &lt;container-name&gt; docker start aaai . Stop a container: . # docker stop &lt;container-name&gt; docker stop aaai . Delete a container: . # docker rm &lt;container-name&gt; docker rm aaai . Go to a running container’s shell: . #docker exec -it &lt;container-name&gt; bash docker exec -it aaai bash # -it stands for interactive . Execute any command on a running container without opening a shell in container: . # docker exec -it &lt;container-name&gt; &lt;command&gt; docker exec -it aaai jupyter notebook list . Check container logs (including shell commands output): . # docker logs &lt;container-name&gt; docker logs aaai . System . Check all images, all containers and space occupied by them: . docker system df -v . Set up rootless docker . Refer to this guide: https://docs.docker.com/engine/security/rootless/ . Main steps: . Run dockerd-rootless-setuptool.sh install. | Setup PATH and DOCKER_HOME as suggested by command output. | systemctl --user restart docker. | Try docker images to check if things worked. | Try docker run --rm hello-world to check if things really worked. | .",
            "url": "https://patel-zeel.github.io/blog/markdown/2021/09/28/docker_cheatsheet.html",
            "relUrl": "/markdown/2021/09/28/docker_cheatsheet.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "How to apply constraint on parameters in various GP libraries",
            "content": "import numpy as np import torch import matplotlib.pyplot as plt from matplotlib import rc rc(&#39;font&#39;, **{&#39;size&#39;:18}) . GPy . from paramz.transformations import Logexp . gpy_trans = Logexp() . x = torch.arange(-1000,10000).to(torch.float) plt.plot(x, gpy_trans.f(x)); plt.xlabel(&#39;X&#39;) plt.ylabel(&#39;f(X)&#39;); . GPyTorch . from gpytorch.constraints import Positive . gpytorch_trans = Positive() . plt.plot(x, gpytorch_trans.transform(x)); plt.xlabel(&#39;X&#39;) plt.ylabel(&#39;f(X)&#39;); . GPFlow . from gpflow.utilities.bijectors import positive . gpflow_trans = positive() . plt.plot(x, gpflow_trans(x)); plt.xlabel(&#39;X&#39;) plt.ylabel(&#39;f(X)&#39;); . np.allclose(gpy_trans.f(x), gpytorch_trans.transform(x)) . True . np.allclose(gpy_trans.f(x), gpflow_trans(x)) . True .",
            "url": "https://patel-zeel.github.io/blog/2021/09/27/Constraints.html",
            "relUrl": "/2021/09/27/Constraints.html",
            "date": " • Sep 27, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Understanding Kernels in Gaussian Processes",
            "content": "!pip install -qq GPy import autograd.numpy as np import pandas as pd import GPy import matplotlib.pyplot as plt from autograd import grad from matplotlib.animation import FuncAnimation from matplotlib import rc import seaborn as sns . Basics of kernels in Gaussian processes . Gaussian process (GP) is a stochstic process where each observation is assumed to be a sample from a Gaussian (Normal) distribution. Probability density function (PDF) for a single observation $y_1$ is given as, $$ begin{aligned} y_1 sim mathcal{N} left( mu_1, sigma_1^2 right) end{aligned} $$ PDF of multiple such observations is a multivariate Gaussian distribution, $$ begin{aligned} Y sim mathcal{N} left(M, Sigma right) end{aligned} $$ In practice, $M$ and $ Sigma$ are modeled as functions of predictors $X$. . Now, multivariate PDF can be modified as, $$ begin{aligned} Y sim mathcal{N} left( mathcal{F}(X), mathcal{K}(X,X) right) end{aligned} $$ Where, $ mathcal{F}$ is a mean function and $ mathcal{K}$ is a kernel (covariance) function. Often $ mathcal{F}(X)$ is assumed to be zero ($ mathcal{F}(X)=0$) and only $ mathcal{K}(X, X)$ is employed to capture relationship between $X$ and $Y$. . The subsequent sections focus on various choices of $ mathcal{K}$ and their effect on $X$ and $Y$. . RBF (Radial basis function) Kernel, Stationarity and Isotropy . RBF is one of the most commonly used kernels in GPs due to it&#39;s infinetely differentiability (extreme flexibility). This property helps us to model a vast variety of functions $X to Y$. . RBF kernel is given as the following, $$ begin{aligned} mathcal{K}(x_1,x_2)= sigma^2exp left(- frac{(x-x&#39;)^2}{2l^2} right) end{aligned} $$ Where, $ sigma^2$ is variance and $l$ is known as lengthscale. . Stationarity . RBF is a stationary kernel and so it is invariant to translation in the input space. In other words, $ mathcal{K}(x,x&#39;)$ depends only on $x-x&#39;$. . Isotropy . RBF is also isotropic kernel, which means that $ mathcal{K}(x,x&#39;)$ depends only on $|x-x&#39;|$. Thus, we have $ mathcal{K}(x,x&#39;) = mathcal{K}(x&#39;,x)$. . Let&#39;s visualize few functions drawn from the RBF kernel . def K_rbf(X1, X2, sigma=1., l=1.): return (sigma**2)*(np.exp(-0.5*np.square(X1-X2.T)/l**2)) . Helper functions . def plot_functions(kernel_func, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1)): mean = np.zeros(X.shape[0]) cov = kernel_func(X, X, sigma, l) functions = np.random.multivariate_normal(mean, cov, size=5) fig = plt.figure(figsize=(14,8), constrained_layout=True) gs = fig.add_gridspec(2,4) ax0 = fig.add_subplot(gs[0, 1:-1]) ax0.set_ylim(*ax0_ylim) ax1 = fig.add_subplot(gs[1, 0:2]) ax1.set_ylim(*ax1_ylim) ax2 = fig.add_subplot(gs[1, 2:4]) for func in functions: ax0.plot(X, func,&#39;o-&#39;); ax0.set_xlabel(&#39;X&#39;);ax0.set_ylabel(&#39;Y&#39;);ax0.set_title(&#39;Functions drawn from &#39;+k_name+&#39; kernel&#39;); ax1.plot(X, cov[:,4]);ax1.set_title(&#39;K(0,X)&#39;);ax1.set_xlabel(&#39;X&#39;);ax1.set_ylabel(&#39;K(0,X)&#39;) sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True); ax2.set_xlabel(&#39;X&#39;);ax2.set_ylabel(&#39;X&#39;);ax2.set_title(&#39;Covariance matrix&#39;); def animate_functions(kernel_func, val_list, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1), k_name=&#39;&#39;,p_name=&#39;&#39;,symbol=&#39;&#39;): fig = plt.figure(figsize=(14,8)) gs = fig.add_gridspec(2,4) ax0 = fig.add_subplot(gs[0, 1:-1]);ax1 = fig.add_subplot(gs[1, 0:2]);ax2 = fig.add_subplot(gs[1, 2:4]); def update(p): ax0.cla();ax1.cla();ax2.cla(); ax0.set_ylim(*ax0_ylim);ax1.set_ylim(*ax1_ylim) if p_name == &#39;Lengthscale&#39;: cov = kernel_func(X, X, l=p) elif p_name == &#39;Variance&#39;: cov = kernel_func(X, X, sigma=np.sqrt(p)) elif p_name == &#39;Offset&#39;: cov = kernel_func(X, X, c=p) elif p_name == &#39;Period&#39;: cov = kernel_func(X, X, p=p) functions = np.random.multivariate_normal(mean, cov, size=5) for func in functions: ax0.plot(X, func,&#39;o-&#39;); ax0.set_xlabel(&#39;X&#39;);ax0.set_ylabel(&#39;Y&#39;);ax0.set_title(&#39;Functions drawn from &#39;+k_name+&#39; kernel n&#39;+p_name+&#39; (&#39;+symbol+&#39;) = &#39;+str(p)); ax1.plot(X, cov[:,4]);ax1.set_title(&#39;K(0,X)&#39;);ax1.set_title(&#39;K(0,X)&#39;);ax1.set_xlabel(&#39;X&#39;);ax1.set_ylabel(&#39;K(0,X)&#39;) sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True, cbar=False); ax2.set_xlabel(&#39;X&#39;);ax2.set_ylabel(&#39;X&#39;);ax2.set_title(&#39;Covariance matrix&#39;); anim = FuncAnimation(fig, update, frames=val_list, blit=False) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) return anim . Verifying if our kernel is consistent with GPy kernels. . X = np.linspace(101,1001,200).reshape(-1,1) sigma, l = 7, 11 assert np.allclose(K_rbf(X,X,sigma,l), GPy.kern.RBF(1, variance=sigma**2, lengthscale=l).K(X,X)) . np.random.seed(0) X = np.arange(-4,5).reshape(-1,1) sigma = 1. l = 3. k_name = &#39;RBF&#39; plot_functions(K_rbf, ax0_ylim=(-3.5,3)) . Let&#39;s see the effect of varying parameters $ sigma$ and $l$ of the RBF kernel function. . np.random.seed(0) sigma = 1. val_list = [0.5,1,2,3,4,5] animate_functions(K_rbf, val_list, k_name=&#39;RBF&#39;, p_name=&#39;Lengthscale&#39;, symbol=&#39;l&#39;) . &lt;/input&gt; Once Loop Reflect l = 1. val_list = [1,4,9,16,25] animate_functions(K_rbf, val_list, ax0_ylim=(-12,12), ax1_ylim=(-0.1, 26), k_name=&#39;RBF&#39;, p_name=&#39;Variance&#39;, symbol=&#39;sigma&#39;) . &lt;/input&gt; Once Loop Reflect With increase in value of $l$, functions drawn from the kernel become smoother. Covariance between a pair of points is increasing with increase in $l$. . Increasing $ sigma^2$ increase the overall uncertainty (width of the space where 95% of the functions live) across all the points. . Matern Kernel . Matern kernels are given by a general formula as following, $$ begin{aligned} mathcal{K}(x_1, x_2) = sigma^2 frac{1}{ Gamma( nu)2^{ nu-1}} Bigg( frac{ sqrt{2 nu}}{l} |x_1-x_2| Bigg)^ nu K_ nu Bigg( frac{ sqrt{2 nu}}{l} |x_1-x_2| Bigg) end{aligned} $$ Where, $ Gamma$ is gamma function and $K_ nu$ is modified Bessel function of second order. . The general formula is not very intuitive about the functionality of this kernel. In practice, Matern with $ nu= {0.5,1.5,2.5 }$ are used, where GP with each kernel is $( lceil nu rceil-1)$ times differentiable. . Matern functions corresponding to each $ nu$ values are defined as the following, $$ begin{aligned} Matern12 to mathcal{K_{ nu=0.5}}(x_1, x_2) &amp;= sigma^2exp left(- frac{|x_1-x_2|}{l} right) Matern32 to mathcal{K_{ nu=1.5}}(x_1, x_2) &amp;= sigma^2 left(1+ frac{ sqrt{3}|x_1-x_2|}{l} right)exp left(- frac{ sqrt{3}|x_1-x_2|}{l} right) Matern52 to mathcal{K_{ nu=2.5}}(x_1, x_2) &amp;= sigma^2 left(1+ frac{ sqrt{5}|x_1-x_2|}{l}+ frac{5(x_1-x_2)^2)}{3l^2} right)exp left(- frac{ sqrt{5}|x_1-x_2|}{l} right) end{aligned} $$ Matern kernels are stationary as well as isotropic. With $ nu to infty$ they converge to $RBF$ kernel. $Matern12$ is also known as $Exponential$ kernel in toolkits such as GPy. . Now, let&#39;s draw few functions from each of these versions and try to get intuition behind each of them. . def K_m12(X1, X2, sigma=1., l=1.): # v = 0.5 return (sigma**2)*(np.exp(-np.abs(X1-X2.T)/l)) def K_m32(X1, X2, sigma=1., l=1.): # v = 1.5 return (sigma**2)*(1+((3**0.5)*np.abs(X1-X2.T))/l)*(np.exp(-(3**0.5)*np.abs(X1-X2.T)/l)) def K_m52(X1, X2, sigma=1., l=1.): # v = 2.5 return (sigma**2)*(1+(((5**0.5)*np.abs(X1-X2.T))/l)+((5*(X1-X2.T)**2)/(3*l**2)))* (np.exp(-(5**0.5)*np.abs(X1-X2.T)/l)) . Verifying if our kernels are consistent with GPy kernels. . X = np.linspace(101,1001,50).reshape(-1,1) assert np.allclose(K_m32(X,X,sigma=7.,l=11.), GPy.kern.Matern32(1,lengthscale=11.,variance=7**2).K(X,X)) assert np.allclose(K_m52(X,X,sigma=7.,l=11.), GPy.kern.Matern52(1,lengthscale=11.,variance=7**2).K(X,X)) . X = np.arange(-4,5).reshape(-1,1) sigma = 1. l = 3. fig, ax = plt.subplots(3,2,figsize=(14,10)) names = [&#39;Matern12&#39;, &#39;Matern32&#39;, &#39;Matern52&#39;] for k_i, kernel in enumerate([K_m12, K_m32, K_m52]): mean = np.zeros(X.shape[0]) cov = kernel(X, X, sigma, l) functions = np.random.multivariate_normal(mean, cov, size=5) for func in functions: ax[k_i,0].plot(X, func); ax[k_i,0].set_xlabel(&#39;X&#39;);ax[k_i,0].set_ylabel(&#39;Y&#39;);ax[k_i,0].set_title(&#39;Functions drawn from &#39;+names[k_i]+&#39; kernel&#39;); sns.heatmap(cov.round(2), ax=ax[k_i,1], xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True); ax[k_i,1].set_xlabel(&#39;X&#39;);ax[k_i,1].set_ylabel(&#39;X&#39;);ax[k_i,1].set_title(&#39;Covariance matrix&#39;); plt.tight_layout(); . From the above plot, we can say that smoothness is increasing in functions as we increase $ nu$. Thus, smoothness of functions in terms of kernels is in the following order: Matern12&lt;Matern32&lt;Matern52. . Let us see effect of varying $ sigma$ and $l$ on Matern32 which is more popular among the three. . np.random.seed(0) sigma = 1. val_list = [0.5,1,2,3,4,5] animate_functions(K_m32, val_list, k_name=&#39;Matern32&#39;, p_name=&#39;Lengthscale&#39;, symbol=&#39;l&#39;) . &lt;/input&gt; Once Loop Reflect We can see that Matern32 kernel behaves similar to RBF with varying $l$. Though, Matern32 is less smoother than RBF. A quick comparison would clarify this. . X = np.linspace(-10,10,100).reshape(-1,1) plt.plot(X, K_rbf(X,X, l=3.)[:,50], label=&#39;RBF&#39;) plt.plot(X, K_m32(X,X, l=3.)[:,50], label=&#39;Matern32&#39;) plt.legend();plt.xlabel(&#39;X&#39;);plt.ylabel(&#39;Covariance (K(0,X))&#39;); plt.title(&#39;K(0,X)&#39;); . Periodic Kernel . Periodic Kernel is given as the following, $$ begin{aligned} mathcal{K}(x_1,x_2)= sigma^2 exp left(- frac{ sin^2( pi|x_1 - x_2|/p)}{2l^2} right) end{aligned} $$ Where $p$ is period. Let&#39;s visualize few functions drawn from this kernel. . def K_periodic(X1, X2, sigma=1., l=1., p=3.): return sigma**2 * np.exp(-0.5*np.square(np.sin(np.pi*(X1-X2.T)/p))/l**2) X = np.linspace(10,1001,50).reshape(-1,1) assert np.allclose(K_periodic(X,X,sigma=7.,l=11.,p=3.), GPy.kern.StdPeriodic(1,lengthscale=11.,variance=7**2,period=3.).K(X,X)) . np.random.seed(0) X = np.arange(-4,5).reshape(-1,1) sigma = 1 l = 1. p = 3. k_name = &#39;Periodic&#39; plot_functions(K_periodic) . We will investigate the effect of varying period $p$ now. . np.random.seed(0) val_list = [1., 2., 3., 4., 5.] animate_functions(K_periodic, val_list, ax1_ylim=(0.4,1.1), k_name=&#39;Periodic&#39;,p_name=&#39;Period&#39;) . &lt;/input&gt; Once Loop Reflect From the above animation we can see that, all points that are $p$ distance apart from each other have exactly same values because they have correlation of exactly 1 ($ sigma=1 to covariance=correlation$). . Now, we will investigate effect of lenging lengthscale $l$ while other parameters are constant. . np.random.seed(0) val_list = [1., 2., 3., 4., 5.] animate_functions(K_periodic, val_list, ax1_ylim=(0.6,1.1), k_name=&#39;Periodic&#39;,p_name=&#39;Lengthscale&#39;, symbol=&#39;l&#39;) . &lt;/input&gt; Once Loop Reflect We can see that correlation between a pair of locations $ {x_1,x_2|x_1-x_2&lt;p }$ increases as the lengthscale is increased. . Linear Kernel . Linear kernel (a.k.a. dot-product kernel) is given as the following, $$ begin{aligned} mathcal{K}(x_1,x_2)= (x_1-c)(x_2-c)+ sigma^2 end{aligned} $$ Let&#39;s visualize few functions drawn from the linear kernel . def K_lin(X1, X2, sigma=1., c=1.): return (X1-c)@(X2.T-c) + sigma**2 . np.random.seed(0) sigma = 1. c = 1. plot_functions(K_lin, ax0_ylim=(-10,5), ax1_ylim=(-3,7)) . Let&#39;s see the effect of varying parameters $ sigma$ and $c$ of the linear kernel function. . val_list = [-3,-2,-1,0,1,2,3] animate_functions(K_lin, val_list, ax0_ylim=(-15,12), ax1_ylim=(-3,23), p_name=&#39;Offset&#39;, symbol=&#39;c&#39;) . &lt;/input&gt; Once Loop Reflect np.random.seed(1) val_list = np.square(np.array([1,2,3,4,5,8])) animate_functions(K_lin, val_list, ax0_ylim=(-25,15), ax1_ylim=(-5,110), p_name=&#39;Variance&#39;, symbol=&#39;sigma&#39;) . &lt;/input&gt; Once Loop Reflect Varying $c$ parameter changes position of shallow region in covariance matrix. In other words, as $x to c$, points close to $x$ have variance $ to sigma^2$. Distant points have monotonically increasing variance. . Increasing $ sigma^2$ adds a constant in all variance and covariances. So, it allows more uncertainty across all points and weakens the monotonic trend of variance over distant points. . Non-stationary behaviour of Linear kernel . Unlike other stationary kernels, Linear kernel is not invariant of translations in the input space. The comparison below, visually supports this claim. . fig, ax = plt.subplots(2,2,figsize=(14,8), sharex=True) kerns = [K_rbf, K_m32, K_periodic, K_lin] k_names = [&#39;RBF&#39;, &#39;Matern32&#39;, &#39;Periodic&#39;, &#39;Linear&#39;] X = np.linspace(-10,10,21).reshape(-1,1) def update(x): count = 0 for i in range(2): for j in range(2): ax.ravel()[count].cla() tmp_kern = kerns[count] mean = np.zeros(X.shape[0]) cov = tmp_kern(X,X) ax.ravel()[count].plot(X, cov[:,x]); ax.ravel()[count].set_xlim(X[x-3],X[x+3]) ax.ravel()[count].set_xlabel(&#39;X&#39;); ax.ravel()[count].set_ylabel(&#39;K(&#39;+str(X[x].round(2))+&#39;,X)&#39;); ax.ravel()[count].set_title(&#39;Covariance K(&#39;+str(X[x].round(2))+&#39;,X) for &#39;+k_names[count]+&#39; kernel&#39;); count += 1 ax.ravel()[3].set_ylim(-5,80) plt.tight_layout() anim = FuncAnimation(fig, update, frames=[5,7,9,11,13,15], blit=False) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect &lt;Figure size 432x288 with 0 Axes&gt; . Multiplications of kernels . If a single kernel is having high bias in fitting a dataset, we can use mutiple of these kernels in multiplications and/or summations. First, let us see effect of multiplication of a few kernels. . Periodic * Linear . X = np.linspace(-10,10,100).reshape(-1,1) plt.plot(X, K_periodic(X,X,sigma=2.)[:,50], label=&#39;Periodic&#39;) plt.plot(X, K_lin(X,X,sigma=0.01,c=0)[:,50], label=&#39;Linear&#39;) plt.plot(X, K_periodic(X,X,sigma=2.)[:,50]*K_lin(X,X,sigma=0.01,c=0)[:,50], label=&#39;Periodic*Linear&#39;) plt.legend(bbox_to_anchor=(1,1));plt.xlabel(&#39;X&#39;);plt.ylabel(&#39;Covariance&#39;) plt.title(&#39;K(0,*)&#39;); . Linear * Linear . X = np.linspace(-1,1,100).reshape(-1,1) plt.plot(X, K_lin(X,X,c=-1)[:,50], label=&#39;Linear1&#39;) plt.plot(X, K_lin(X,X,c=1)[:,50], label=&#39;Linear2&#39;) plt.plot(X, K_lin(X,X,c=0.5)[:,50], label=&#39;Linear3&#39;) plt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50], label=&#39;Linear1*Linear3&#39;) plt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50]*K_lin(X,X,c=0.5)[:,50], label=&#39;Linear1*Linear2*Linear3&#39;) plt.legend(bbox_to_anchor=(1,1)); . Matern * Linear . X = np.linspace(-1,1,100).reshape(-1,1) k1 = K_lin(X,X,c=1)[:,50] k2 = K_m32(X,X)[:,50] plt.plot(X, k1, label=&#39;Linear&#39;) plt.plot(X, k2, label=&#39;Matern32&#39;) plt.plot(X, k1*k2, label=&#39;Matern32*Linear&#39;) plt.legend(bbox_to_anchor=(1,1)); . Appendix (Extra material) . At this stage, we do not know how the fuctions are drawn from linear kernel based covariance matrix end up being lines with various intercepts and slopes. . . Predicting at a single point after observing value at a single point . Let&#39;s see how would be a GP prediction after observing value at a single point. . Our kernel function is given by, . $K(x,x&#39;)=(x-c) cdot (x&#39;-c)+ sigma^2$ | . Now, we observe value $y$ at a location $x$ and we want to predict value $y^*$ at location $x^*$. $$ begin{aligned} (y^*|x_1,y_1,x^*) &amp;= K(x^*,x) cdot K^{-1}(x,x) cdot y &amp;= left( frac{(x-c)(x^*-c)+ sigma^2}{(x-c)(x-c)+ sigma^2} right) cdot y end{aligned} $$ $c$ and $ sigma^2$ do not vary in numerator and denominator so, the value of $y^* propto x^*$. . . Predicting at a single point after observing values at two points . Now, we&#39;ll take a case where two values ${y_1, y_2}$ are observed at ${x_1, x_2}$. Let us try to predict value $y^*$ at $x^*$. . $$ y^ = begin{bmatrix} K(x_1, x^) &amp; K(x_2,x^*) end{bmatrix} begin{bmatrix} K(x_1, x_1) &amp; K(x_1,x_2) K(x_2, x_1) &amp; K(x_2,x_2) end{bmatrix}^{-1} begin{bmatrix} y_1 y_2 end{bmatrix} . &amp; = begin{bmatrix} (x_1-c)(x^*-c)+ sigma^2 &amp; (x_2-c)(x^*-c)+ sigma^2 end{bmatrix} begin{bmatrix} (x_1-c)^2+ sigma^2 &amp; (x_1-c) (x_2-c)+ sigma^2 (x_2-c) (x_1-c)+ sigma^2 &amp; (x_2-c)^2 + sigma^2 end{bmatrix}^{-1} begin{bmatrix} y_1 y_2 end{bmatrix} . &amp; = begin{bmatrix} (x_1-c)(x^*-c)+ sigma^2 &amp; (x_2-c)(x^*-c)+ sigma^2 end{bmatrix} frac{1}{ sigma^2(x_1-x_2)^2} begin{bmatrix} (x_2-c)^2+ sigma^2 &amp; -[(x_1-c)(x_2-c)+ sigma^2] -[(x_2-c) (x_1-c)+ sigma^2] &amp; (x_1-c)^2 + sigma^2 end{bmatrix} begin{bmatrix} y_1 y_2 end{bmatrix} tag{1} . From Eq. (1) second term, we can say that if $ sigma^2=0$, matrix is not-invertible because determinant is zero. It means that, if $ sigma^2=0$, observing a single point is enough, we can infer values at infinite points after observing that single point. . Evaluating Eq. (1) further, it converges to the following equation, $$ begin{aligned} y^* = frac{(x_1y_2-x_2y_1)+x^*(y_1-y_2)}{(x_1-x_2)} end{aligned} $$ Interestingly, we can see that output does not depend on $c$ or $ sigma^2$ anymore. Let us verify experimentally if this is true for observing more than 2 data points. . Prepering useful functions . from scipy.optimize import minimize . def cov_func(x, x_prime, sigma, c): return (x-c)@(x_prime-c) + sigma**2 def neg_log_likelihood(params): n = X.shape[0] sigma, c, noise_std = params cov = cov_func(X, X.T, sigma, c) cov = cov + (noise_std**2)*np.eye(n) nll_ar = 0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) return nll_ar[0,0] def predict(params): sigma, c, noise_std = params k = cov_func(X, X.T, sigma, c) np.fill_diagonal(k, k.diagonal()+noise_std**2) k_inv = np.linalg.pinv(k) k_star = cov_func(X_test, X.T, sigma, c) mean = k_star@k_inv@Y cov = cov_func(X_test, X_test.T, sigma, c) - k_star@k_inv@k_star.T return mean, cov . Observing more than two points and changing hyperparameters manually . X = np.array([3,4,5,6,7,8]).reshape(-1,1) Y = np.array([6,9,8,11,10,13]).reshape(-1,1) X_test = np.linspace(1,8,20).reshape(-1,1) params_grid = [[1., 0.01, 10**-10], [100., 1., 10**-10], [100., 0.01, 10**-10], [1., 2., 1.]] # sigma, c, noise_std X_extra = np.hstack([np.ones((X.shape[0], 1)), X]) Theta = np.linalg.pinv(X_extra.T@X_extra)@X_extra.T@Y X_test_extra = np.hstack([np.ones((X_test.shape[0], 1)), X_test]) Y_test_ideal = X_test_extra@Theta fig, ax = plt.subplots(1,4,figsize=(16,5), sharey=True) means = [] for p_i, params in enumerate(params_grid): Y_test_mean, Y_test_cov = predict(params) means.append(Y_test_mean) ax[p_i].scatter(X, Y, label=&#39;train&#39;) ax[p_i].scatter(X_test, Y_test_mean, label=&#39;test&#39;) ax[p_i].legend();ax[p_i].set_xlabel(&#39;X&#39;);ax[p_i].set_ylabel(&#39;Y&#39;); ax[p_i].set_title(&#39;sigma=&#39;+str(params[0])+&#39;, c=&#39;+str(params[1])+&#39;, noise=&#39;+str(params[2])); . np.allclose(Y_test_ideal, means[0]), np.allclose(Y_test_ideal, means[1]), np.allclose(Y_test_ideal, means[2]), np.allclose(Y_test_ideal, means[3]) . (True, True, True, False) . model = GPy.models.GPRegression(X, Y, GPy.kern.Linear(input_dim=1)) # model[&#39;Gaussian_noise&#39;].fix(10**-10) # model.kern.variances.fix(10**-10) model.optimize() model.plot() plt.plot(X_test, Y_test_ideal, label=&#39;Normal Eq. fit&#39;) plt.plot(X_test,model.predict(X_test)[0], label=&#39;Prediction&#39;) plt.legend() model . Model: GP regression Objective: 13.51314321804978 Number of Parameters: 2 Number of Optimization Parameters: 2 Updates: True . GP_regression. valueconstraintspriors . &lt;td class=tg-left&gt; linear.variances &lt;/td&gt;&lt;td class=tg-right&gt; 2.806515343539501&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; Gaussian_noise.variance&lt;/td&gt;&lt;td class=tg-right&gt;2.0834221617534134&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . We can see that there is no change in fit with change in $c$ and $ sigma$. 4th fit is not matching with the ideal fit obtained by normal equation because of high noise. Now, let us estimate parameters by minimizing negative log marginal likelihood. . params = [1., 1., 1.] result = minimize(neg_log_likelihood, params, bounds=[(10**-5, 10**5), (10**-5, 10**5), (10**-5, 10**-5)]) params = result.x print(params, result.fun) Y_test_mean, Y_test_cov = predict(params) plt.scatter(X, Y, label=&#39;train&#39;) plt.scatter(X_test, Y_test_mean, label=&#39;test&#39;) plt.legend();plt.xlabel(&#39;X&#39;);plt.ylabel(&#39;Y&#39;); params = np.round(params, 4) plt.title(&#39;sigma=&#39;+str(params[0])+&#39;, c=&#39;+str(params[1])+&#39;, noise=&#39;+str(params[2])); np.allclose(Y_test_ideal, Y_test_mean) . [9.99998123e-01 9.99998123e-01 1.00000000e-05] 10207223403405.541 . False . def neg_log_likelihood(sigma, c, noise_std): n = X.shape[0] cov = cov_func(X, X.T, sigma, c) cov = cov + (noise_std**2)*np.eye(n) nll_ar = 0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) return nll_ar[0,0] . grad_func = grad(neg_log_likelihood, argnum=[0,1,2]) alpha = 0.01 loss = [] sigma, c, noise_std = 1., 1., 1. for _ in range(5000): grads = grad_func(sigma, c, noise_std) # print(grads) sigma = sigma - alpha*grads[0] c = c - alpha*grads[1] noise_std = noise_std - alpha*grads[2] loss.append(neg_log_likelihood(sigma, c, noise_std)) print(sigma, c, noise_std) plt.plot(loss); loss[-1] . 7.588989986845149 -2.830840439162303 32.2487569348891 . 31.05187173290998 . params = sigma, c, noise_std Y_test_mean, Y_test_cov = predict(params) plt.scatter(X, Y, label=&#39;train&#39;) plt.scatter(X_test, Y_test_mean, label=&#39;test&#39;) plt.legend();plt.xlabel(&#39;X&#39;);plt.ylabel(&#39;Y&#39;); params = np.round(params, 4) plt.title(&#39;sigma=&#39;+str(params[0])+&#39;, c=&#39;+str(params[1])+&#39;, noise=&#39;+str(params[2])); np.allclose(means[0], Y_test_mean, rtol=10**-1, atol=10**-1) . False .",
            "url": "https://patel-zeel.github.io/blog/ml/2021/03/22/GP_Kernels.html",
            "relUrl": "/ml/2021/03/22/GP_Kernels.html",
            "date": " • Mar 22, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Programatically download OpenAQ data",
            "content": "# !pip install boto3 botocore import pandas as pd import numpy as np import matplotlib.pyplot as plt import sys import boto3 import botocore import os from IPython.display import clear_output . Setup . s3 = boto3.client(&#39;s3&#39;, config=botocore.config.Config(signature_version=botocore.UNSIGNED)) bucket_name = &#39;openaq-fetches&#39; prefix = &#39;realtime-gzipped/&#39; path = &#39;/content/drive/MyDrive/IJCAI-21/data/OpenAQ-Delhi/&#39; start_date = &#39;2020/01/01&#39; # start date (inclusive) end_date = &#39;2020/12/31&#39; # end date (inclusive) . Download . for date in pd.date_range(start=start_date, end=end_date): clear_output(wait=True) date = str(date).split(&#39; &#39;)[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS print(&#39;Downloading:&#39;, date) data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date) for file_obj in data_dict[&#39;Contents&#39;]: f_name = file_obj[&#39;Key&#39;] tmp_path = &#39;/&#39;.join((path+f_name).split(&#39;/&#39;)[:-1]) if not os.path.exists(tmp_path): os.makedirs(tmp_path) s3.download_file(bucket_name, f_name, path+f_name) . Downloading: 2020-05-04 . Validate . for date in pd.date_range(start=start_date, end=end_date): date = str(date).split(&#39; &#39;)[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date) for file_obj in data_dict[&#39;Contents&#39;]: assert os.path.exists(path+file_obj[&#39;Key&#39;]), file_obj[&#39;Key&#39;] print(&#39;Validated&#39;) .",
            "url": "https://patel-zeel.github.io/blog/data/openaq/2020/09/21/Programatically_download_OpenAQ_data.html",
            "relUrl": "/data/openaq/2020/09/21/Programatically_download_OpenAQ_data.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Active Learning with Bayesian Linear Regression",
            "content": "A quick wrap-up for Bayesian Linear Regression (BLR) . We have a feature matrix $X$ and a target vector $Y$. We want to obtain $ theta$ vector in such a way that the error $ epsilon$ for the following equation is minimum. . $$ Y = X^T theta + epsilon $$Prior PDF for $ theta$ is, . $$ p( theta) sim mathcal{N}(M_0, S_0) $$Where $S_0$ is prior covariance matrix, and $M_0$ is prior mean. . Posterier PDF can be given as, . $$ begin{aligned} p( theta|X,Y) &amp; sim mathcal{N}( theta | M_n, S_n) S_n &amp;= (S_0^{-1} + sigma_{mle}^{-2}X^TX) M_n &amp;= S_n(S_0^{-1}M_0+ sigma_{mle}^{-2}X^TY) end{aligned} $$Maximum likelihood estimation of $ sigma$ can be calculated as, . $$ begin{aligned} theta_{mle} &amp;= (X^TX)^{-1}X^TY sigma_{mle} &amp;= ||Y - X^T theta_{mle}|| end{aligned} $$Finally, predicted mean $ hat{Y}_{mean}$ and predicted covariance matrix $ hat{Y}_{cov}$ can be given as, . $$ begin{aligned} hat{Y} &amp; sim mathcal{N}( hat{Y}_{mean}, hat{Y}_{cov}) hat{Y}_{mean} &amp;= XM_n hat{Y}_{cov} &amp;= X^TS_nX end{aligned} $$Now, let&#39;s put everything together and write a class for Bayesian Linear Regression. . Creating scikit-learn like class with fit predict methods for BLR . import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn import datasets from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler from sklearn.model_selection import train_test_split from matplotlib.animation import FuncAnimation from matplotlib import rc import warnings warnings.filterwarnings(&#39;ignore&#39;) seed = 0 # random seed for train_test_split . class BLR(): def __init__(self,S0, M0): # M0 -&gt; prior mean, S0 -&gt; prior covariance matrix self.S0 = S0 self.M0 = M0 def fit(self,x,y, return_self = False): self.x = x self.y = y # Maximum likelihood estimation for sigma parameter theta_mle = np.linalg.pinv(x.T@x)@(x.T@y) sigma_2_mle = np.linalg.norm(y - x@theta_mle)**2 sigma_mle = np.sqrt(sigma_2_mle) # Calculating predicted mean and covariance matrix for theta self.SN = np.linalg.pinv(np.linalg.pinv(self.S0) + (sigma_mle**-2)*x.T@x) self.MN = self.SN@(np.linalg.pinv(self.S0)@self.M0 + (sigma_mle**-2)*(x.T@y).squeeze()) # Calculating predicted mean and covariance matrix for data self.pred_var = x@self.SN@x.T self.y_hat_map = x@self.MN if return_self: return (self.y_hat_map, self.pred_var) def predict(self, x): self.pred_var = x@self.SN@x.T self.y_hat_map = x@self.MN return (self.y_hat_map, self.pred_var) def plot(self, s=1): # s -&gt; size of dots for scatter plot individual_var = self.pred_var.diagonal() plt.figure() plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.plot(self.x[:,1], self.y_hat_map, color=&#39;black&#39;, label=&#39;model&#39;) plt.fill_between(self.x[:,1], self.y_hat_map-individual_var, self.y_hat_map+individual_var, alpha=0.4, color=&#39;black&#39;, label=&#39;uncertainty&#39;) plt.scatter(self.x[:,1], self.y, label=&#39;actual data&#39;,s=s) plt.title(&#39;MAE is &#39;+str(np.mean(np.abs(self.y - self.y_hat_map)))) plt.legend() . Creating &amp; visualizing dataset . To start with, let&#39;s create a random dataset with degree 3 polynomial function with some added noise. . $$ Y = (5X^3 - 4X^2 + 3X - 2) + mathcal{N}(0,1) $$ np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) noise = np.random.randn(1000, ) Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . We&#39;ll try to fit a degree 5 polynomial function to our data. . X = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1)) N_features = X.shape[1] . plt.scatter(X[:,1], Y, s=0.5, label = &#39;data points&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.legend() plt.show() . Learning a BLR model on the entire data . We&#39;ll take $M_0$ (prior mean) as zero vector initially, assuming that we do not have any prior knowledge about $M_0$. We&#39;re taking $S_0$ (prior covariance) as the identity matrix, assuming that all coefficients are completely independent of each other. . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) model = BLR(S0, M0) . model.fit(X, Y) . Visualising the fit . model.plot(s=0.5) . This doesn&#39;t look like a good fit, right? Let&#39;s set the prior closer to the real values and visualize the fit again. . Visualising the fit after changing the prior . np.random.seed(seed) S0 = np.eye(N_features) M0 = np.array([-2, 3, -4, 5, 0, 0]) + np.random.randn(N_features, ) model = BLR(S0, M0) . model.fit(X, Y) model.plot(s=0.5) . Hmm, better. Now let&#39;s see how it fits after reducing the noise and setting the prior mean to zero vector again. . Visualising the fit after reducing the noise . np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) noise = np.random.randn(1000, ) * 0.5 Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) model = BLR(S0, M0) . model.fit(X, Y) model.plot(s=0.5) . When the noise was high, the model tended to align with the prior. After keeping the prior closer to the original coefficients, the model was improved as expected. From the last plot, we can say that as noise reduces from the data, the impact of the prior reduces, and the model tries to fit the data more precisely. Therefore, we can say that when data is too noisy or insufficient, a wisely chosen prior can produce a precise fit. . Intuition to Active Learning (Uncertainty Sampling) with an example . Let&#39;s take the case where we want to train a machine learning model to classify if a person is infected with COVID-19 or not, but the testing facilities for the same are not available so widely. We may have very few amounts of data for detected positive and detected negative patients. Now, we want our model to be highly confident or least uncertain about its results; otherwise, it may create havoc for wrongly classified patients, but, our bottleneck is labeled data. Thanks to active learning techniques, we can overcome this problem smartly. How? . We train our model with existing data and test it on all the suspected patients&#39; data. Let&#39;s say we have an uncertainty measure or confidence level about each tested data point (distance from the decision boundary in case of SVM, variance in case of Gaussian processes, or Bayesian Linear Regression). We can choose a patient for which our model is least certain, and send him to COVID-19 testing facilities (assuming that we can send only one patient at a time). Now, we can include his data to the train set and test the model on everyone else. By following the same procedure repeatedly, we can increase the size of our train data and confidence of the model without sending everyone randomly for testing. . This method is called Uncertainty Sampling in Active Learning. Now let&#39;s formally define Active Learning. From Wikipedia, . Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. . Now, we&#39;ll go through the active learning procedure step by step. . Train set, test set, and pool. What is what? . The train set includes labeled data points. The pool includes potential data points to query for a label, and the test set includes labeled data points to check the performance of our model. Here, we cannot actually do a query to anyone, so we assume that we do not have labels for the pool while training, and after each iteration, we include a data point from the pool set to the train set for which our model has the highest uncertainty. . So, the algorithm can be represented as the following, . Train the model with the train set. | Test the performance on the test set (This is expected to improve). | Test the model with the pool. | Query for the most uncertain datapoint from the pool. | Add that datapoint into the train set. | Repeat step 1 to step 5 for $K$ iterations ($K$ ranges from $0$ to the pool size). | Creating initial train set, test set, and pool . Let&#39;s take half of the dataset as the test set, and from another half, we will start with some points as the train set and remaining as the pool. Let&#39;s start with 2 data points as the train set. . np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) X = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1)) noise = np.random.randn(1000, ) * 0.5 Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . train_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed) train_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=2, random_state=seed) . Visualizing train, test and pool. . plt.scatter(test_X[:,1], test_Y, label=&#39;test set&#39;,color=&#39;r&#39;, s=2) plt.scatter(train_X[:,1], train_Y, label=&#39;train set&#39;,marker=&#39;s&#39;,color=&#39;k&#39;, s=50) plt.scatter(pool_X[:,1], pool_Y, label=&#39;pool&#39;,color=&#39;b&#39;, s=2) plt.xlabel(&#39;X&#39;) plt.ylabel(&#39;Y&#39;) plt.legend() plt.show() . Let&#39;s initialize a few dictionaries to keep track of each iteration. . train_X_iter = {} # to store train points at each iteration train_Y_iter = {} # to store corresponding labels to the train set at each iteration models = {} # to store the models at each iteration estimations = {} # to store the estimations on the test set at each iteration test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration . Training &amp; testing initial learner on train set (Iteration 0) . Now we will train the model for the initial train set, which is iteration 0. . train_X_iter[0] = train_X train_Y_iter[0] = train_Y . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) models[0] = BLR(S0, M0) . models[0].fit(train_X_iter[0], train_Y_iter[0]) . Creating a plot method to visualize train, test and pool with estimations and uncertainty. . def plot(ax, model, init_title=&#39;&#39;): # Plotting the pool ax.scatter(pool_X[:,1], pool_Y, label=&#39;pool&#39;,s=1,color=&#39;r&#39;,alpha=0.4) # Plotting the test data ax.scatter(test_X[:,1], test_Y, label=&#39;test data&#39;,s=1, color=&#39;b&#39;, alpha=0.4) # Combining the test &amp; the pool test_pool_X, test_pool_Y = np.append(test_X,pool_X, axis=0), np.append(test_Y,pool_Y) # Sorting test_pool for plotting sorted_inds = np.argsort(test_pool_X[:,1]) test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds] # Plotting test_pool with uncertainty model.predict(test_pool_X) individual_var = model.pred_var.diagonal() ax.plot(test_pool_X[:,1], model.y_hat_map, color=&#39;black&#39;, label=&#39;model&#39;) ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var , alpha=0.2, color=&#39;black&#39;, label=&#39;uncertainty&#39;) # Plotting the train data ax.scatter(model.x[:,1], model.y,s=40, color=&#39;k&#39;, marker=&#39;s&#39;, label=&#39;train data&#39;) ax.scatter(model.x[-1,1], model.y[-1],s=80, color=&#39;r&#39;, marker=&#39;o&#39;, label=&#39;last added point&#39;) # Plotting MAE on the test set model.predict(test_X) ax.set_title(init_title+&#39; MAE is &#39;+str(np.mean(np.abs(test_Y - model.y_hat_map)))) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.legend() . Plotting the estimations and uncertainty. . fig, ax = plt.subplots() plot(ax, models[0]) . Let&#39;s check the maximum uncertainty about any point for the model. . models[0].pred_var.diagonal().max() . 4.8261426545316604e-29 . Oops!! There is almost no uncertainty in the model. Why? let&#39;s try again with more train points. . train_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed) train_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=7, random_state=seed) . train_X_iter[0] = train_X train_Y_iter[0] = train_Y . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) models[0] = BLR(S0, M0) . models[0].fit(train_X_iter[0], train_Y_iter[0]) . fig, ax = plt.subplots() plot(ax, models[0]) . Now uncertainty is visible, and currently, it&#39;s high at the left-most points. We are trying to fit a degree 5 polynomial here. So our linear regression coefficients are 6, including the bias. If we choose train points equal to or lesser than 6, our model perfectly fits the train points and has no uncertainty. Choosing train points more than 6 induces uncertainty in the model. . Let&#39;s evaluate the performance on the test set. . estimations[0], _ = models[0].predict(test_X) test_mae_error[0] = np.mean(np.abs(test_Y - estimations[0])) . Mean Absolute Error (MAE) on the test set is . test_mae_error[0] . 0.5783654195019617 . Moving the most uncertain point from the pool to the train set . In the previous plot, we saw that the model was least certain about the left-most point. We&#39;ll move that point from the pool to the train set and see the effect. . esimations_pool, _ = models[0].predict(pool_X) . Finding out a point having the most uncertainty. . in_var = models[0].pred_var.diagonal().argmax() to_add_x = pool_X[in_var,:] to_add_y = pool_Y[in_var] . Adding the point from the pool to the train set. . train_X_iter[1] = np.vstack([train_X_iter[0], to_add_x]) train_Y_iter[1] = np.append(train_Y_iter[0], to_add_y) . Deleting the point from the pool. . pool_X = np.delete(pool_X, in_var, axis=0) pool_Y = np.delete(pool_Y, in_var) . Training again and visualising the results (Iteration 1) . This time, we will pass previously learnt prior to the next iteration. . S0 = np.eye(N_features) models[1] = BLR(S0, models[0].MN) . models[1].fit(train_X_iter[1], train_Y_iter[1]) . estimations[1], _ = models[1].predict(test_X) test_mae_error[1] = np.mean(np.abs(test_Y - estimations[1])) . MAE on the test set is . test_mae_error[1] . 0.5779411133071186 . Visualizing the results. . fig, ax = plt.subplots() plot(ax, models[1]) . Before &amp; after adding most uncertain point . fig, ax = plt.subplots(1,2, figsize=(13.5,4.5)) plot(ax[0], models[0],&#39;Before&#39;) plot(ax[1], models[1],&#39;After&#39;) . We can see that including most uncertain point into the train set has produced a better fit and MAE for test set has been reduced. Also, uncertainty has reduced at the left part of the data but it has increased a bit on the right part of the data. . Now let&#39;s do this for few more iterations in a loop and visualise the results. . Active learning procedure . num_iterations = 20 points_added_x= np.zeros((num_iterations+1, N_features)) points_added_y=[] print(&quot;Iteration, Cost n&quot;) print(&quot;-&quot;*40) for iteration in range(2, num_iterations+1): # Making predictions on the pool set based on model learnt in the respective train set estimations_pool, var = models[iteration-1].predict(pool_X) # Finding the point from the pool with highest uncertainty in_var = var.diagonal().argmax() to_add_x = pool_X[in_var,:] to_add_y = pool_Y[in_var] points_added_x[iteration-1,:] = to_add_x points_added_y.append(to_add_y) # Adding the point to the train set from the pool train_X_iter[iteration] = np.vstack([train_X_iter[iteration-1], to_add_x]) train_Y_iter[iteration] = np.append(train_Y_iter[iteration-1], to_add_y) # Deleting the point from the pool pool_X = np.delete(pool_X, in_var, axis=0) pool_Y = np.delete(pool_Y, in_var) # Training on the new set models[iteration] = BLR(S0, models[iteration-1].MN) models[iteration].fit(train_X_iter[iteration], train_Y_iter[iteration]) estimations[iteration], _ = models[iteration].predict(test_X) test_mae_error[iteration]= pd.Series(estimations[iteration] - test_Y.squeeze()).abs().mean() print(iteration, (test_mae_error[iteration])) . Iteration, Cost - 2 0.49023173501654815 3 0.4923391714942153 4 0.49040074812746753 5 0.49610198614600165 6 0.5015282102751122 7 0.5051264429971314 8 0.5099913097301352 9 0.504455016053513 10 0.5029219102020734 11 0.5009762782262487 12 0.5004883097883343 13 0.5005169638980388 14 0.5002731089932334 15 0.49927485683909884 16 0.49698416490822594 17 0.49355398855432897 18 0.49191185613804617 19 0.491164833699368 20 0.4908067530719673 . pd.Series(test_mae_error).plot(style=&#39;ko-&#39;) plt.xlim((-0.5, num_iterations+0.5)) plt.ylabel(&quot;MAE on test set&quot;) plt.xlabel(&quot;# Points Queried&quot;) plt.show() . The plot above shows that MAE on the test set fluctuates a bit initially then reduces gradually as we keep including more points from the pool to the train set. Let&#39;s visualise fits for all the iterations. We&#39;ll discuss this behaviour after that. . Visualizing active learning procedure . print(&#39;Initial model&#39;) print(&#39;Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}&#39;.format(*models[0].MN[::-1])) print(&#39; nFinal model&#39;) print(&#39;Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}&#39;.format(*models[num_iterations].MN[::-1])) . Initial model Y = 1.89 X^5 + 1.54 X^4 + 0.84 X^3 + -6.48 X^2 + 4.74 X + -1.63 Final model Y = 2.50 X^5 + 3.11 X^4 + 0.83 X^3 + -7.08 X^2 + 4.47 X + -1.58 . def update(iteration): ax.cla() plot(ax, models[iteration]) fig.tight_layout() . fig, ax = plt.subplots() anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations+1, 1), interval=250) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) . anim . &lt;/input&gt; Once Loop Reflect We can see that the point having highest uncertainty was chosen in first iteration and it produced the near optimal fit. After that, error reduced gradually. . Now, let&#39;s put everything together and create a class for active learning procedure . Creating a class for active learning procedure . class ActiveL(): def __init__(self, X, y, S0=None, M0=None, test_size=0.5, degree = 5, iterations = 20, seed=1): self.X_init = X self.y = y self.S0 = S0 self.M0 = M0 self.train_X_iter = {} # to store train points at each iteration self.train_Y_iter = {} # to store corresponding labels to the train set at each iteration self.models = {} # to store the models at each iteration self.estimations = {} # to store the estimations on the test set at each iteration self.test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration self.test_size = test_size self.degree = degree self.iterations = iterations self.seed = seed self.train_size = degree + 2 def data_preperation(self): # Adding polynomial features self.X = PolynomialFeatures(degree=self.degree).fit_transform(self.X_init) N_features = self.X.shape[1] # Splitting into train, test and pool train_pool_X, self.test_X, train_pool_Y, self.test_Y = train_test_split(self.X, self.y, test_size=self.test_size, random_state=self.seed) self.train_X, self.pool_X, self.train_Y, self.pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=self.train_size, random_state=self.seed) # Setting BLR prior incase of not given if self.M0 == None: self.M0 = np.zeros((N_features, )) if self.S0 == None: self.S0 = np.eye(N_features) def main(self): # Training for iteration 0 self.train_X_iter[0] = self.train_X self.train_Y_iter[0] = self.train_Y self.models[0] = BLR(self.S0, self.M0) self.models[0].fit(self.train_X, self.train_Y) # Running loop for all iterations for iteration in range(1, self.iterations+1): # Making predictions on the pool set based on model learnt in the respective train set estimations_pool, var = self.models[iteration-1].predict(self.pool_X) # Finding the point from the pool with highest uncertainty in_var = var.diagonal().argmax() to_add_x = self.pool_X[in_var,:] to_add_y = self.pool_Y[in_var] # Adding the point to the train set from the pool self.train_X_iter[iteration] = np.vstack([self.train_X_iter[iteration-1], to_add_x]) self.train_Y_iter[iteration] = np.append(self.train_Y_iter[iteration-1], to_add_y) # Deleting the point from the pool self.pool_X = np.delete(self.pool_X, in_var, axis=0) self.pool_Y = np.delete(self.pool_Y, in_var) # Training on the new set self.models[iteration] = BLR(self.S0, self.models[iteration-1].MN) self.models[iteration].fit(self.train_X_iter[iteration], self.train_Y_iter[iteration]) self.estimations[iteration], _ = self.models[iteration].predict(self.test_X) self.test_mae_error[iteration]= pd.Series(self.estimations[iteration] - self.test_Y.squeeze()).abs().mean() def _plot_iter_MAE(self, ax, iteration): ax.plot(list(self.test_mae_error.values())[:iteration+1], &#39;ko-&#39;) ax.set_title(&#39;MAE on test set over iterations&#39;) ax.set_xlim((-0.5, self.iterations+0.5)) ax.set_ylabel(&quot;MAE on test set&quot;) ax.set_xlabel(&quot;# Points Queried&quot;) def _plot(self, ax, model): # Plotting the pool ax.scatter(self.pool_X[:,1], self.pool_Y, label=&#39;pool&#39;,s=1,color=&#39;r&#39;,alpha=0.4) # Plotting the test data ax.scatter(self.test_X[:,1], self.test_Y, label=&#39;test data&#39;,s=1, color=&#39;b&#39;, alpha=0.4) # Combining test_pool test_pool_X, test_pool_Y = np.append(self.test_X, self.pool_X, axis=0), np.append(self.test_Y, self.pool_Y) # Sorting test_pool sorted_inds = np.argsort(test_pool_X[:,1]) test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds] # Plotting test_pool with uncertainty preds, var = model.predict(test_pool_X) individual_var = var.diagonal() ax.plot(test_pool_X[:,1], model.y_hat_map, color=&#39;black&#39;, label=&#39;model&#39;) ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var , alpha=0.2, color=&#39;black&#39;, label=&#39;uncertainty&#39;) # plotting the train data ax.scatter(model.x[:,1], model.y,s=10, color=&#39;k&#39;, marker=&#39;s&#39;, label=&#39;train data&#39;) ax.scatter(model.x[-1,1], model.y[-1],s=80, color=&#39;r&#39;, marker=&#39;o&#39;, label=&#39;last added point&#39;) # plotting MAE preds, var = model.predict(self.test_X) ax.set_title(&#39;MAE is &#39;+str(np.mean(np.abs(self.test_Y - preds)))) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.legend() def visualise_AL(self): fig, ax = plt.subplots(1,2,figsize=(13,5)) def update(iteration): ax[0].cla() ax[1].cla() self._plot(ax[0], self.models[iteration]) self._plot_iter_MAE(ax[1], iteration) fig.tight_layout() print(&#39;Initial model&#39;) print(&#39;Y = &#39;+&#39; + &#39;.join([&#39;{0:0.2f}&#39;.format(self.models[0].MN[i])+&#39; X^&#39;*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)])) print(&#39; nFinal model&#39;) print(&#39;Y = &#39;+&#39; + &#39;.join([&#39;{0:0.2f}&#39;.format(self.models[self.iterations].MN[i])+&#39; X^&#39;*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)])) anim = FuncAnimation(fig, update, frames=np.arange(0, self.iterations+1, 1), interval=250) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) return anim . Visualizing a different polynomial fit on the same dataset . Let&#39;s try to fit a degree 7 polynomial to the same data now. . np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) noise = np.random.randn(1000, ) * 0.5 Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . model = ActiveL(X_init.reshape(-1,1), Y, degree=7, iterations=20, seed=seed) . model.data_preperation() model.main() model.visualise_AL() . Initial model Y = -1.92 + 3.79 X^1 + -1.81 X^2 + -0.43 X^3 + -0.51 X^4 + -0.27 X^5 + -0.18 X^6 + -0.11 X^7 Final model Y = -1.79 + 4.86 X^1 + -5.38 X^2 + 0.50 X^3 + -0.17 X^4 + 1.19 X^5 + 1.83 X^6 + 1.31 X^7 . &lt;/input&gt; Once Loop Reflect We can clearly see that model was fitting the train points well and uncertainty was high at the left-most position. After first iteration, the left-most point was added to the train set and MAE reduced significantly. Similar phenomeneon happened at iteration 2 with the right-most point. After that error kept reducing at slower rate gradually because fit was near optimal after just 2 iterations. . Active learning for diabetes dataset from the Scikit-learn module . Let&#39;s run our model for diabetes data from sklearn module. The data have various features like age, sex, weight etc. of diabetic people and target is increment in disease after one year. We&#39;ll choose only &#39;weight&#39; feature, which seems to have more correlation with the target. . We&#39;ll try to fit degree 1 polynomial to this data, as our data seems to have a linear fit. First, let&#39;s check the performance of Scikit-learn linear regression model. . X, Y = datasets.load_diabetes(return_X_y=True) X = X[:, 2].reshape(-1,1) # Choosing only feature 2 which seems more relevent to linear regression # Normalizing X = (X - X.min())/(X.max() - X.min()) Y = (Y - Y.min())/(Y.max() - Y.min()) . Visualizing the dataset. . plt.scatter(X, Y) plt.xlabel(&#39;Weight of the patients&#39;) plt.ylabel(&#39;Increase in the disease after a year&#39;) plt.show() . Let&#39;s fit the Scikit-learn linear regression model with 50% train-test split. . from sklearn.linear_model import LinearRegression train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state = seed) . clf = LinearRegression() . clf.fit(train_X, train_Y) pred_Y = clf.predict(test_X) . Visualizing the fit &amp; MAE. . plt.scatter(X, Y, label=&#39;data&#39;, s=5) plt.plot(test_X, pred_Y, label=&#39;model&#39;, color=&#39;r&#39;) plt.xlabel(&#39;Weight of the patients&#39;) plt.ylabel(&#39;Increase in the disease after a year&#39;) plt.title(&#39;MAE is &#39;+str(np.mean(np.abs(pred_Y - test_Y)))) plt.legend() plt.show() . Now we&#39;ll fit the same data with our BLR model . model = ActiveL(X.reshape(-1,1), Y, degree=1, iterations=20, seed=seed) . model.data_preperation() model.main() model.visualise_AL() . Initial model Y = 0.41 + 0.16 X^1 Final model Y = 0.13 + 0.86 X^1 . &lt;/input&gt; Once Loop Reflect Initially, the fit is leaning towards zero slope, which is the influence of bias due to a low number of training points. It&#39;s interesting to see that our initial train points tend to make a vertical fit, but the model doesn&#39;t get carried away by that and stabilizes the self with prior. . print(&#39;MAE for Scikit-learn Linear Regression is&#39;,np.mean(np.abs(pred_Y - test_Y))) print(&#39;MAE for Bayesian Linear Regression is&#39;, model.test_mae_error[20]) . MAE for Scikit-learn Linear Regression is 0.15424985705353944 MAE for Bayesian Linear Regression is 0.15738001811804758 . At the end, results of sklearn linear regression and our active learning based BLR model are comparable even though we&#39;ve used only 20 points to train our model over 221 points used by sklearn. This is because active learning enables us to choose those datapoints for training, which are going to contribute the most towards a precise fit. .",
            "url": "https://patel-zeel.github.io/blog/ml/2020/03/28/Active_Learning_with_Bayesian_Linear_Regression.html",
            "relUrl": "/ml/2020/03/28/Active_Learning_with_Bayesian_Linear_Regression.html",
            "date": " • Mar 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://patel-zeel.github.io/blog/",
          "relUrl": "/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://patel-zeel.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}