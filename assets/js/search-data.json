{
  
    
        "post0": {
            "title": "Docker Cheatsheet",
            "content": "Images . One can download (pull) a docker image from Docker Hub or respective websites: . # docker pull &lt;image-name&gt;:&lt;tag&gt; docker pull tensorflow/tensorflow:2.4.0-gpu-jupyter . Check the downloaded images with: . docker images . Sometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping: . docker run --rm tensorflow/tensorflow:2.4.0-gpu-jupyter . Containers . Create a new container from an image with following flags . -v: for telling docker to use a shared directory between host and container | -p &lt;host-port &gt; &lt;container-port &gt;: is to use port forwarding, an application running on &lt;container-port&gt; can be accessed only with &lt;host-port&gt; in host. | --name generates a name for the container for easy reference in other commands | --gpus all tells docker to use all GPUs available on host | -m Restricts RAM usage | To create new container with default params: . docker create tensorflow/tensorflow:2.4.0-gpu-jupyter . To create new container with manual params: . docker create -v path_in_host: path_in_container -p9000:8888 --name aaai --cpus 2 --gpus all -m 100g tensorflow/tensorflow:2.4.0-gpu-jupyter . Update some of the above configurations after container creation: . # change RAM limit of a container named &quot;aaai&quot; docker update -m 50g aaai . Note: In general, changes made to container persist when container is stopped. . Check containers: . docker ps # shows running containers docker ps -a # shows all containers . Start a container (default script will be executed with this if any): . # docker start &lt;container-name&gt; docker start aaai . Stop a container: . # docker stop &lt;container-name&gt; docker stop aaai . Delete a container: . # docker rm &lt;container-name&gt; docker rm aaai . Go to a running container’s shell: . #docker exec -it &lt;container-name&gt; bash docker exec -it aaai bash # -it stands for interactive . Execute any command on a running container without opening a shell in container: . # docker exec -it &lt;container-name&gt; &lt;command&gt; docker exec -it aaai jupyter notebook list . Check container logs (including shell commands output): . # docker logs &lt;container-name&gt; docker logs aaai . System . Check all images, all containers and space occupied by them: . docker system df -v .",
            "url": "https://patel-zeel.github.io/blog/markdown/2021/09/28/docker_cheatsheet.html",
            "relUrl": "/markdown/2021/09/28/docker_cheatsheet.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How to apply constraint on parameters in various GP libraries",
            "content": "import numpy as np import torch import matplotlib.pyplot as plt from matplotlib import rc rc(&#39;font&#39;, **{&#39;size&#39;:18}) . GPy . from paramz.transformations import Logexp . gpy_trans = Logexp() . x = torch.arange(-1000,10000).to(torch.float) plt.plot(x, gpy_trans.f(x)); plt.xlabel(&#39;X&#39;) plt.ylabel(&#39;f(X)&#39;); . GPyTorch . from gpytorch.constraints import Positive . gpytorch_trans = Positive() . plt.plot(x, gpytorch_trans.transform(x)); plt.xlabel(&#39;X&#39;) plt.ylabel(&#39;f(X)&#39;); . GPFlow . from gpflow.utilities.bijectors import positive . gpflow_trans = positive() . plt.plot(x, gpflow_trans(x)); plt.xlabel(&#39;X&#39;) plt.ylabel(&#39;f(X)&#39;); . np.allclose(gpy_trans.f(x), gpytorch_trans.transform(x)) . True . np.allclose(gpy_trans.f(x), gpflow_trans(x)) . True .",
            "url": "https://patel-zeel.github.io/blog/2021/09/27/Constraints.html",
            "relUrl": "/2021/09/27/Constraints.html",
            "date": " • Sep 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Bayesian ML",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib import rc from matplotlib.animation import FuncAnimation from mpl_toolkits import mplot3d from jax import numpy as jnp, grad . What is the intuition behind the Bayesian update? itempool . admin https://www.youtube.com/watch?v=HZGCoVF3YvM .png) . .png) . .png) . Bayes theorem . $$ p(B|A) = frac{p(A|B)}{p(A)}p(B) to p( theta|D) = frac{p(D| theta)}{p(D)}p( theta) $$ $p(D| theta)$ - likelihood | $p( theta)$ - prior | $p(D)$ - evidence | $p( theta|D)$ - posterior | . Let us understand Likelihood, prior and evidence individually then we will move into the posterior . But what is the Likelihood? $ to$ How likely the event is, given a belief (parameters)? . $$ p(D| theta) $$Let&#39;s get real with a simple coin flip experiment. . We want to determine the probability (likelihood) of $D= {H,T,H,H,H,H,T,H,H,H }$ after 10 coin flips believing that we have a fair coin ($p(H)= theta=0.5$). . $$ p(D| theta) = prod limits_{i=1}^{10}p(D_i| theta) = (0.5)^8(0.5)^2 = 2^{-10} $$More generally, out of N experiments, if we see $h$ heads, likelihood $p(D| theta)$ is, . $$ p(D| theta) = ( theta)^h(1- theta)^{N-h} $$This is also known as the &quot;Bernoulli likelihood&quot;. . $D$ event is less likely given a fair coin | Your intuition isn&#39;t telling you that the coin is biased towards $H$? What&#39;s your guess for $ theta$? | Let&#39;s visualize likelihood of $D$ for various values of $ theta$. | . def Bernoulli(theta,N,h): return (theta**h)*((1-theta)**(N-h)) def BernoulliModified(theta,N,h): # exp after log return np.exp(h*np.log(theta) + (N-h)*np.log(1-theta)) def LogBernoulli(theta,N,h): # exp after log return h*np.log(theta) + (N-h)*np.log(1-theta) N,h = 10,8 theta = np.linspace(0.01,0.99,100) BL = [Bernoulli(t,N,h) for t in theta] BLM = BernoulliModified(theta,N,h) LogBL = LogBernoulli(theta,N,h) rc(&#39;font&#39;,size=20) rc(&#39;text&#39;,usetex=True) fig, ax = plt.subplots(figsize=(10,4)) ax.plot(theta, BL); plt.xlabel(&#39;$ theta$&#39;);plt.ylabel(&#39;Likelihood $p(D| theta)$&#39;); plt.title(&#39;Likelihood of observing $D= {H,T,H,H,H,H,T,H,H,H }$&#39;); . Visually, we can see that, our event is most likely at $ theta=0.8$. How can we concretely say that? . Maximum likelihood estimation . $ theta$ would be optimal at $ frac{d}{d theta}L(D| theta)=0$ | $ theta$ would be maximum at $ frac{d^2}{d theta^2}L(D| theta)&lt;0$. | . begin{aligned} p(D| theta) &amp;= ( theta)^h(1- theta)^{N-h} frac{d}{d theta}p(D| theta) &amp;= h theta^{h-1}(1- theta)^{N-h} - (N-h) theta^h(1- theta)^{N-h-1} = 0 text{follows that,} h(1- theta)-(N-h) theta &amp;= 0 h - h theta - N theta + h theta &amp;= 0 theta_{MLE} &amp;= frac{h}{H} end{aligned} Verifying our example, we had $h=8$ and $N=10$, so $ theta_{MLE}=8/10=0.8$. . We see from our plot that this is the maxima, but one can also verify it by double differentiation. . Likelihood v/s modified likelihood v/s log likelihood . Avoiding numerical errors | . begin{aligned} p(D| theta) &amp;= ( theta)^h(1- theta)^{N-h} p(D| theta) &amp;= exp left[h log( theta)+(N-h) log(1- theta) right] log p(D|theta) &amp;= h log( theta)+(N-h) log(1- theta) end{aligned} def Bernoulli(theta,N,h): return (theta**h)*((1-theta)**(N-h)) def BernoulliModified(theta,N,h): # exp after log return np.exp(h*np.log(theta) + (N-h)*np.log(1-theta)) def LogBernoulli(theta,N,h): # exp after log return h*np.log(theta) + (N-h)*np.log(1-theta) N,h = 10,8 theta = np.linspace(0.01,0.99,100) BL = [Bernoulli(t,N,h) for t in theta] BLM = BernoulliModified(theta,N,h) LogBL = LogBernoulli(theta,N,h) rc(&#39;font&#39;,size=20) rc(&#39;text&#39;,usetex=True) fig, ax = plt.subplots(1,3,figsize=(15,4)) ax[0].plot(theta, BL); ax[1].plot(theta, BLM); ax[2].plot(theta, LogBL); for axs in ax: axs.set_xlabel(&#39;$ theta$&#39;) axs.vlines(0.8,*axs.get_ylim(),linestyle=&#39;--&#39;, label=&#39;MLE&#39;) ax[0].text(0,0.0005,&#39;Almost zero&#39;) ax[1].text(0,0.0005,&#39;Almost zero&#39;) ax[0].legend() ax[0].set_title(&#39;Likelihood&#39;); ax[1].set_title(&#39;Modified likelihood&#39;); ax[2].set_title(&#39;Log likelihood&#39;); . MLE with log likelihood . $ theta$ would be optimal at $ frac{d}{d theta} log L(D| theta)=0$ | $ theta$ would be maximum at $ frac{d^2}{d theta^2} log L(D| theta)&lt;0$. | . begin{aligned} p(D| theta) &amp;= ( theta)^h(1- theta)^{N-h} log p(D| theta) &amp;= h log( theta)+(N-h) log(1- theta) frac{d}{d theta} log p(D| theta) &amp;= frac{h}{ theta} - frac{N-h}{1- theta} = 0 text{follows that,} h(1- theta)-(N-h) theta &amp;= 0 h - h theta - N theta + h theta &amp;= 0 theta_{MLE} &amp;= frac{h}{H} end{aligned}Notice that double differentiation is trivial in this setting, . begin{aligned} frac{d}{d theta} log p(D| theta) &amp;= frac{h}{ theta} - frac{N-h}{1- theta} frac{d^2}{d theta^2} log p(D| theta) &amp;= - frac{h}{ theta^2}- frac{N-h}{(1- theta)^2} &lt; 0 end{aligned}Thus, now onwards, we will directly use log likelihood . Gaussian distribution . A contineous random variable is called Gaussian distributed if it follows the below pdf, $$ p(x| mu, sigma) = frac{1}{ sqrt{2 pi sigma^2}}exp left[{- frac{(x- mu)^2}{2 sigma^2}} right] $$ . rc(&#39;font&#39;,size=20) rc(&#39;text&#39;,usetex=True) np.random.seed(0) x = np.load(&#39;../data/bml_norm_x.npy&#39;) plt.figure(figsize=(15,4)) plt.eventplot(x); plt.yticks([]);plt.xlabel(&#39;x&#39;); plt.title(&#39;What should be $ mu$? What should be $ sigma$?&#39;); . Let us visualize the pdf of Gaussian distribution by varying $ mu$ and $ sigma$. . def GaussianPDF(mu, sigma, x): return (1/np.sqrt(2*np.pi*sigma**2))*np.exp(-np.square(x-mu)/2/sigma**2) rc(&#39;font&#39;,size=16) rc(&#39;text&#39;,usetex=True) fig, ax = plt.subplots(figsize=(10,4)) mu, sigma = 0, 10 def update(mu): np.random.seed(0) ax.cla() x = np.sort(np.random.normal(mu, sigma, 100)) pdfx = np.linspace(x.min(), x.max(), 100) G_pdf = GaussianPDF(mu, sigma, pdfx) ax.plot(pdfx, G_pdf,label=&#39;pmf&#39;) ax.scatter(x, np.ones(x.shape[0])*-0.001, marker=&#39;|&#39;,c=&#39;k&#39;) ax.vlines([mu+sigma, mu-sigma], *ax.get_ylim(), label=&#39;$ sigma$&#39;, linestyle=&#39;--&#39;) ax.vlines([mu+2*sigma, mu-2*sigma], *ax.get_ylim(), label=&#39;$2 sigma$&#39;, linestyle=&#39;--&#39;,color=&#39;r&#39;) ax.legend() ax.set_xlabel(&#39;x&#39;);ax.set_ylabel(&#39;pdf&#39;); ax.set_title(f&#39;mu = {mu}, sigma={sigma}&#39;); ax.set_xlim(-35,90) plt.tight_layout() plt.close() anim = FuncAnimation(fig, update, frames=np.arange(1,60,5)) rc(&#39;animation&#39;,html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect &lt;Figure size 432x288 with 0 Axes&gt; . def GaussianPDF(mu, sigma, x): return (1/np.sqrt(2*np.pi*sigma**2))*np.exp(-np.square(x-mu)/2/sigma**2) rc(&#39;font&#39;,size=16) rc(&#39;text&#39;,usetex=True) fig, ax = plt.subplots(figsize=(10,4)) mu, sigma = 0, 10 def update(sigma): np.random.seed(0) ax.cla() x = np.sort(np.random.normal(mu, sigma, 100)) pdfx = np.linspace(x.min(), x.max(), 100) G_pdf = GaussianPDF(mu, sigma, pdfx) ax.plot(pdfx, G_pdf,label=&#39;pmf&#39;) ax.scatter(x, np.ones(x.shape[0])*-0.001, marker=&#39;|&#39;,c=&#39;k&#39;) ax.vlines([mu+sigma, mu-sigma], *ax.get_ylim(), label=&#39;$ sigma$&#39;, linestyle=&#39;--&#39;) ax.vlines([mu+2*sigma, mu-2*sigma], *ax.get_ylim(), label=&#39;$2 sigma$&#39;, linestyle=&#39;--&#39;,color=&#39;r&#39;) ax.legend() ax.set_xlabel(&#39;x&#39;);ax.set_ylabel(&#39;pdf&#39;); ax.set_title(f&#39;mu = {mu}, sigma={sigma}&#39;); ax.set_xlim(-35,35) ax.set_ylim(-0.02,0.12) plt.tight_layout() plt.close() anim = FuncAnimation(fig, update, frames=np.arange(4,11)) rc(&#39;animation&#39;,html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect &lt;Figure size 432x288 with 0 Axes&gt; . MLE estimate for the Gaussian distribution parameters . We draw N samples independently from a Gaussian distribution. $D= {x_1, x_2, ..., x_N }$ . Let us estimate $ mu_{MLE}$ first, begin{aligned} p(D| mu) &amp;= prod limits_{i=1}^{N} frac{1}{ sqrt{2 pi sigma^2}}exp left[{- frac{(x_i- mu)^2}{2 sigma^2}} right] log p(D| mu) &amp;= - frac{N}{2} log(2 pi sigma^2)- frac{1}{2} sum limits_{i=1}^{N} frac{(x_i- mu)^2}{ sigma^2} frac{d}{d mu} log p(D| mu) &amp;= sum limits_{i=1}^{N} frac{(x_i- mu)}{ sigma^2} = 0 text{Follows that,} mu_{MLE} &amp;= frac{1}{N} sum limits_{i=1}^{N}x_i end{aligned} . Now, we will estimate $ sigma_{MLE}$, begin{aligned} p(D| sigma) &amp;= prod limits_{i=1}^{N} frac{1}{ sqrt{2 pi sigma^2}}exp left[{- frac{(x_i- mu)^2}{2 sigma^2}} right] log p(D| sigma) &amp;= - frac{N}{2} log(2 pi sigma^2)- frac{1}{2} sum limits_{i=1}^{N} frac{(x_i- mu)^2}{ sigma^2} frac{d}{d sigma} log p(D| sigma) &amp;= - frac{N}{2} frac{4 pi sigma}{2 pi sigma^2} + sum limits_{i=1}^{N} frac{(x_i- mu)^2}{ sigma^3} = 0 text{Follows that,} frac{N}{ sigma} &amp;= sum limits_{i=1}^{N} frac{(x_i- mu)^2}{ sigma^3} sigma^2_{MLE} &amp;= frac{1}{N} sum limits_{i=1}^{N}(x_i- mu)^2 end{aligned} . def LogGaussian(mu, sigma, x): return -0.5*np.log(2*np.pi*sigma**2)*x.shape[0] - 0.5*np.sum(np.square((x.squeeze()-mu)/sigma)) mu,sigma=10,3 np.random.seed(0) x = np.load(&#39;../data/bml_norm_x.npy&#39;) muR = np.linspace(1,18,100) Lmu = [LogGaussian(mu, sigma, x) for mu in muR] Lsigma = [LogGaussian(mu, sigma, x) for sigma in muR] fig, ax = plt.subplots(3,1,figsize=(12,8), sharex=True) ax[0].plot(muR, Lmu);ax[0].set_ylabel(&#39;log likelihood nover $ mu$&#39;); ax[1].plot(muR, Lsigma, color=&#39;r&#39;) ax[1].set_ylabel(&#39;log likelihood nover $ sigma$&#39;); ax[0].set_title(&#39;Log likelihood&#39;);ax[0].set_xlabel(&#39;$ mu$&#39;);ax[1].set_xlabel(&#39;$ sigma$&#39;); ax[0].vlines(np.mean(x), *ax[0].get_ylim(), linestyle=&#39;--&#39;,label=&#39;$ mu_{MLE}$&#39;) ax[1].vlines(np.std(x), *ax[0].get_ylim(), linestyle=&#39;--&#39;,label=&#39;$ sigma_{MLE}$&#39;,color=&#39;r&#39;) ax[2].eventplot(x);ax[2].set_xlabel(&#39;x&#39;);ax[2].set_ylabel(&#39;Number of points&#39;); ax[2].vlines(np.mean(x), *ax[2].get_ylim(), linestyle=&#39;--&#39;,label=&#39;$ mu_{MLE}$&#39;) ax[2].vlines([np.mean(x)-np.std(x),np.mean(x)+np.std(x)], *ax[2].get_ylim(), linestyle=&#39;--&#39;,label=&#39;$ mu_{MLE} pm sigma_{MLE}$&#39;,color=&#39;r&#39;) ax[2].set_title(&#39;Samples&#39;); for axs in ax: axs.legend(bbox_to_anchor=(1.2,1)); plt.tight_layout(); . What is the difference between pdf/pmf and likelihood? . Poisson distribution . A discrete random variable is called Poisson distributed if it follows the below pmf, $$ p(x| lambda) = frac{ lambda^xe^{- lambda}}{x!} $$ . Wonder how this formula is derived? checkout https://www.youtube.com/watch?v=7cg-rxofqj8 . msg = np.loadtxt(&#39;https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/data/txtdata.csv&#39;) plt.figure(figsize=(10,4)) plt.bar(range(len(msg)),msg); plt.title(&#39;text-messages received every hour&#39;); plt.xlabel(&#39;Time-stamp&#39;);plt.ylabel(&#39;Number of msgs received&#39;); . Other applications . Telephone calls arriving in a system | Customers arriving at a counter | The number of photons emitted in a single laser pulse | And many more on Wikipedia . def Poisson(lmd, k): return lmd**k*np.exp(-lmd)/np.math.factorial(int(k)) k = np.arange(21) rc(&#39;font&#39;,size=14) fig, ax = plt.subplots() def update(lmd): ax.cla() P_pdf = [Poisson(lmd, ki) for ki in k] ax.plot(k, P_pdf,&#39;o-&#39;,label=&#39;pmf&#39;) ax.set_xlabel(&#39;x&#39;);ax.set_ylabel(&#39;pmf&#39;); ax.set_title(f&#39;lambda = {lmd}&#39;) ax.set_ylim(0,0.4); ax.legend([&#39;pmf&#39;]) plt.close() anim = FuncAnimation(fig, update, frames=np.arange(1,11)) rc(&#39;animation&#39;,html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect MLE estimate for the Poisson distribution parameters . We draw N samples independently from a Poisson distribution. $D= {x_1, x_2, ..., x_N }$ . begin{aligned} p(D| lambda) &amp;= prod limits_{i=1}^{N} frac{ lambda^{x_i}e^{- lambda}}{x_i!} log p(D| lambda) &amp;= sum limits_{i=1}^{N} log( lambda^{x_i})+ sum limits_{i=1}^{N} log(e^{- lambda})- sum limits_{i=1}^{N} log(x_i!) frac{d}{d lambda} log p(D| lambda) &amp;= frac{ sum limits_{i=1}^{N}x_i}{ lambda} - sum limits_{i=1}^{N}1 = 0 lambda_{MLE} &amp;= frac{1}{N} sum limits_{i=1}^{N}x_i end{aligned} np.random.seed(123) x = msg L = [np.sum([np.log(Poisson(lmdi, xi)) for xi in x]) for lmdi in range(1,30)] rc(&#39;font&#39;,size=20) fig, ax = plt.subplots(1,2,figsize=(12,4)) ax[0].plot(range(1,30), L);ax[0].set_xlabel(&#39;$ lambda$&#39;);ax[0].set_ylabel(&#39;log likelihood&#39;); ax[0].set_title(&#39;Log likelihood&#39;); ax[0].vlines(np.mean(x), *ax[0].get_ylim(), linestyle=&#39;--&#39;,label=&#39;MLE&#39;) ax[1].bar(range(len(msg)), msg);ax[1].set_xlabel(&#39;Time-stamps&#39;);ax[1].set_ylabel(&#39;Number of points&#39;); ax[1].set_title(&#39;Number of msg per hour&#39;); ax[0].legend(bbox_to_anchor=(0.1,0.3)); . Bayesian linear regression (1D without bias) . We know the formulation of the simple linear regression for $ mathbf{x}= {x_1,x_2,..,x_N }$ and $ mathbf{y}= {y_1,y_2,..,y_N }$, . $$ D = mathbf{y} = theta mathbf{x} + boldsymbol{ epsilon} $$In probabilistic world, we can define the Gaussian distributions over the noise parameter as following, begin{aligned} boldsymbol{ epsilon} &amp; sim mathcal{N}(0, I sigma_n^2) end{aligned} . np.random.seed(2) rc(&#39;font&#39;,size=20) N = 3 x = np.linspace(0,2,N).reshape(-1,1) t = 3 sigma_n = 2 y = t*x + np.random.multivariate_normal(np.zeros(N), np.eye(N)*sigma_n**2).reshape(-1,1) plt.scatter(x, y);plt.xlabel(&#39;x&#39;);plt.ylabel(&#39;y&#39;); . Notice that we did not define any distribution over $ theta_0, theta_1$ yet, because we do not need it yet. Likelihood in this case also follows that same formulations we did till now, $$ p( mathbf{y}| theta) sim mathcal{N}( theta mathbf{x}, I sigma_n^2) $$ . Now, we should be able to appreciate the effectiveness of MLE as we can find such $ theta$ so that our likelihood of observing given data $D= mathbf{y}$ is maximized. . begin{aligned} p(D| theta) &amp;= frac{1}{ sqrt{(2 pi)^N} sigma_n^N}exp left[- frac{1}{2}( mathbf{y}- theta mathbf{x})^T(I sigma_n^2)^{-1}( mathbf{y}- theta mathbf{x}) right] log p(D| theta) &amp;= - frac{N}{2} log(2 pi sigma_n^2) - sum limits_{i=1}^{N} frac{(y_i - theta x_i)^2}{2 sigma_n^2} argmin - log p(D| theta)&amp;= argmin sum limits_{i=1}^{N}(y_i - theta x_i)^2 end{aligned}This returns us the same cost function as linear regression, which can be solved using gradient descent as well as any other optimization method. Let us fist find MLE for $ theta$. . begin{aligned} frac{d}{d theta} log p(D| theta) &amp;= sum limits_{i=1}^{N} frac{(y_i - theta x_i)x_i}{ sigma_n^2} = 0 theta_{MLE} &amp;= frac{ sum limits_{i=1}^{N}y_ix_i}{ sum limits_{i=1}^{N}x_i^2} end{aligned} def loglin(t1): N = x.shape[0] return -(N/2)*jnp.log(2*jnp.pi*sigma_n**2) - jnp.sum(jnp.square(y-t1*x)) T = np.linspace(0,10,11) LL = [-loglin(t1) for t1 in T] fig, ax = plt.subplots() ax.plot(T, LL); ax.vlines(3,*ax.get_ylim(),linestyle=&#39;--&#39;,label=&#39;GT&#39;,color=&#39;k&#39;) ax.vlines(np.sum(x*y)/np.sum(np.square(x)),*ax.get_ylim(),linestyle=&#39;--&#39;,label=&#39;MLE&#39;) ax.set_xlabel(&quot;theta&quot;);ax.set_ylabel(&quot;neg log likelihood&quot;); plt.legend(bbox_to_anchor=[1,1]); . np.random.seed(1) rc(&#39;font&#39;,size=20) plt.scatter(x, y);plt.xlabel(&#39;x&#39;);plt.ylabel(&#39;y&#39;); t_mle = np.sum(x*y).squeeze()/np.sum(np.square(x)).squeeze() plt.plot(x, t*x, label=&#39;GT&#39;) plt.plot(x, t_mle*x, label=&#39;MLE&#39;) plt.legend(); . Gradient descent is illustrated below. . costs = [] thetas = [] theta = 10. lr = 0.02 grad_func = grad(loglin) for iteration in range(20): dt = -grad_func(theta) theta = theta - lr*dt costs.append(-loglin(theta)) thetas.append(theta) rc(&#39;font&#39;,size=14) fig,ax = plt.subplots(1,2,figsize=(10,4)) def update(i): ax[1].cla();ax[0].cla(); ax[0].plot(T, LL,color=&#39;b&#39;) ax[0].set_xlabel(&#39;theta&#39;);ax[0].set_ylabel(&#39;neg log likelihood&#39;); ax[0].scatter(thetas[:i+1], costs[:i+1], label=&#39;solution&#39;,c=&#39;r&#39;) ax[0].legend() ax[1].scatter(x,y) ax[1].plot(x, thetas[i]*x, label=&#39;estimated fit&#39;) ax[1].set_xlabel(&#39;x&#39;);ax[1].set_ylabel(&#39;y&#39;); ax[1].legend() ax[1].set_ylim(-5,16); plt.tight_layout() # plt.tight_layout() plt.close() anim = FuncAnimation(fig, update, range(20)) anim . &lt;/input&gt; Once Loop Reflect &lt;Figure size 432x288 with 0 Axes&gt; . You should trust MLE with a caution..!! . New evidence does not completely determine the new belief but it updates the prior belief. | According to MLE, . $D= {H,H,H,H } to theta=1$ . | $D= {H,T,T,H } to theta=0.5$ . | $D= {T,T,T,T } to theta=0$ . | . | Also, MLE provides point estimates only, what if we care about uncertainty in $ theta_{opt}$ too? . | Thus, we need to incorporate the prior belief using MAP estimation. | . Maximum A Posteriori (MAP) estimation . begin{aligned} p( theta_{post}|D) &amp;= frac{p(D| theta_{prior})}{p(D)}p( theta_{prior}) p( theta_{post}|D) &amp; propto p(D| theta_{prior})p( theta_{prior}) end{aligned} Prior on parameters . For linear regression problem, let us assume that our theta comes from a Gaussian distribution. . begin{align} p( theta) sim mathcal{N}(0, sigma^2) end{align} np.random.seed(0) fig,ax = plt.subplots(1,2,figsize=(10,4)) sigma = 1 samples = np.random.normal(0,sigma,size=10) thetap = np.linspace(-2,2,20) def update(i): for axs in ax: axs.cla() ax[0].plot(thetap, GaussianPDF(0, sigma, thetap)) ax[0].set_xlabel(&#39;theta&#39;);ax[0].set_ylabel(&#39;pdf&#39;); ax[0].scatter(samples[i], GaussianPDF(0, sigma, samples[i]), label=&#39;sample&#39;) ax[0].legend() ax[1].scatter(x,y) ax[1].plot(x, samples[i]*x, label=&#39;prior fit&#39;) ax[1].set_xlabel(&#39;x&#39;);ax[1].set_ylabel(&#39;y&#39;); ax[1].legend() ax[1].set_ylim(-4,16); plt.tight_layout() # plt.tight_layout() plt.close() anim = FuncAnimation(fig, update, range(10)) anim . &lt;/input&gt; Once Loop Reflect &lt;Figure size 432x288 with 0 Axes&gt; . Now, we will find MAP estimate incorporating the prior. . begin{aligned} p( theta|D) propto p(D| theta)p( theta) &amp;= frac{1}{ sqrt{(2 pi)^N} sigma_n^N}exp left[- frac{1}{2}( mathbf{y}- theta mathbf{x})^T(I sigma_n^2)^{-1}( mathbf{y}- theta mathbf{x}) right] frac{1}{ sqrt{2 pi sigma^2}}exp left[{- frac{( theta-0)^2}{2 sigma^2}} right] log [p(D| theta)p( theta)] &amp;= - frac{N}{2} log(2 pi sigma_n^2) - sum limits_{i=1}^{N} frac{(y_i - theta x_i)^2}{2 sigma_n^2} - frac{1}{2} log(2 pi sigma^2)- frac{ theta^2}{2 sigma^2} frac{d}{d theta} log [p(D| theta)p( theta)] &amp;= sum limits_{i=1}^{N} frac{(y_i - theta x_i)x_i}{ sigma_n^2} - frac{ theta}{ sigma^2} = 0 theta_{MAP} &amp;= frac{ sum limits_{i=1}^{N}y_ix_i}{ sum limits_{i=1}^{N}x_i^2 + frac{ sigma_n^2}{ sigma^2}} end{aligned} def loglin(t1): N = x.shape[0] return -(N/2)*jnp.log(2*jnp.pi*sigma_n**2) - jnp.sum(jnp.square(y-t1*x)) T = np.linspace(0,10,11) LL = [-loglin(t1) for t1 in T] fig, ax = plt.subplots() ax.plot(T, LL); ax.vlines(3,*ax.get_ylim(),linestyle=&#39;--&#39;,label=&#39;GT&#39;,color=&#39;k&#39;) ax.vlines(0,*ax.get_ylim(),linestyle=&#39;--&#39;,label=&#39;Prior mean&#39;,color=&#39;g&#39;) ax.vlines(np.sum(x*y)/np.sum(np.square(x)),*ax.get_ylim(),linestyle=&#39;--&#39;,label=&#39;MLE&#39;) ax.vlines(np.sum(x*y)/(np.sum(np.square(x))+(sigma_n**2/sigma**2)),*ax.get_ylim(),linestyle=&#39;--&#39;,label=&#39;MAP&#39;,color=&#39;r&#39;) ax.set_xlabel(&quot;theta&quot;);ax.set_ylabel(&quot;neg log likelihood&quot;); plt.legend(bbox_to_anchor=[1,1]); . np.random.seed(1) rc(&#39;font&#39;,size=20) plt.scatter(x, y);plt.xlabel(&#39;x&#39;);plt.ylabel(&#39;y&#39;); t_mle = np.sum(x*y)/np.sum(np.square(x)) plt.plot(x, t*x, label=&#39;GT&#39;) plt.plot(x, t_mle*x, label=&#39;MLE&#39;) t_map = np.sum(x*y)/(np.sum(np.square(x))+(sigma_n**2/sigma**2)) plt.plot(x, t_map*x, label=f&#39;MAP, sigma={sigma}&#39;) t_map = np.sum(x*y)/(np.sum(np.square(x))+(sigma_n**2/0.5**2)) plt.plot(x, t_map*x, label=f&#39;MAP, sigma={0.5}&#39;) plt.plot(x, 0*x, label=&#39;Mean of Prior&#39;) plt.legend(bbox_to_anchor=(1,1)); plt.title(f&#39;sigma _n = {sigma_n}&#39;); . np.random.seed(1) rc(&#39;font&#39;,size=20) plt.scatter(x, y);plt.xlabel(&#39;x&#39;);plt.ylabel(&#39;y&#39;); t_mle = np.sum(x*y)/np.sum(np.square(x)) plt.plot(x, t*x, label=&#39;GT&#39;) plt.plot(x, t_mle*x, label=&#39;MLE&#39;) t_map = np.sum(x*y)/(np.sum(np.square(x))+(sigma_n**2/sigma**2)) plt.plot(x, t_map*x, label=f&#39;MAP, sigma _n={sigma_n}&#39;) t_map = np.sum(x*y)/(np.sum(np.square(x))+(1**2/1**2)) plt.plot(x, t_map*x, label=f&#39;MAP, sigma _n={1}&#39;) plt.plot(x, 0*x, label=&#39;Mean of Prior&#39;) plt.legend(bbox_to_anchor=(1,1)); plt.title(f&#39;sigma = {sigma}&#39;); . Probabilistic linear regression (1D with bias term) . Now, we will programatically explore MLE and MAP for 1D linear regression after including the bias term. . begin{aligned} mathbf{ theta}_{MLE} &amp;= (X^TX)^{-1}X^T mathbf{y} mathbf{ theta}_{MAP} &amp;= (X^TX + I frac{ sigma_n^2}{ sigma^2})^{-1}X^T mathbf{y} end{aligned} N = 2 x = np.linspace(0,5,N).reshape(-1,1) np.random.seed(1) rc(&#39;font&#39;,size=20) t0, t1 = 3, 4 sigma_n = 10 y = t0 + t1*x + np.random.multivariate_normal(np.zeros(N), np.eye(N)*sigma_n**2).reshape(-1,1) plt.scatter(x, y);plt.xlabel(&#39;x&#39;);plt.ylabel(&#39;y&#39;); . def LogLin2D(t0, t1): N = x.shape[0] return (N/2)*jnp.log(2*jnp.pi*sigma_n**2) + jnp.sum(jnp.square(y-t0-t1*x)) from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(figsize=(12,4)) ax = fig.add_subplot(121) T0, T1 = np.meshgrid(np.linspace(-20,40,20), np.linspace(-20,40,20)) Z = np.array([LogLin2D(t00, t11) for t00, t11 in zip(T0.ravel(), T1.ravel())]).reshape(*T0.shape) mp = ax.contour(T0, T1, Z); plt.clabel(mp); ax.set_xlabel(&#39;t0&#39;);ax.set_ylabel(&#39;t1&#39;); ax.set_title(&#39;Neg. log likelihood&#39;); x_extra = np.hstack([np.ones((x.shape[0], 1)), x]) t0_mle, t1_mle = np.linalg.inv(x_extra.T@x_extra)@x_extra.T@y t0_map, t1_map = np.linalg.inv(x_extra.T@x_extra + np.eye(x.shape[1])*(sigma_n**2/sigma**2))@x_extra.T@y ax.scatter(t0, t1, label=&#39;GT&#39;); # ax.annotate(&#39;GT&#39;, (t0, t1)) ax.scatter(t0_mle, t1_mle, label=&#39;MLE&#39;); # ax.annotate(&#39;MLE&#39;, (t0_mle, t1_mle)) ax.scatter(t0_map, t1_map, label=&#39;MAP&#39;); # ax.annotate(&#39;MAP&#39;, (t0_map, t1_map)) ax.scatter(0, 0, label=&#39;Prior mean&#39;); # ax.text(-4,0, &#39;Prior mean&#39;) ax.legend(bbox_to_anchor=(-0.2,1)) ax = fig.add_subplot(122, projection=&#39;3d&#39;) ax.plot_surface(T0, T1, Z); ax.view_init(35, 90+65); ax.set_xlabel(&#39;t0&#39;);ax.set_ylabel(&#39;t1&#39;); . plt.scatter(x,y); plt.plot(x, t0_mle+t1_mle*x, label=&#39;MLE&#39;) plt.plot(x, t0_map+t1_map*x, label=&#39;MAP&#39;) plt.plot(x, t0+t1*x, label=&#39;GT&#39;) plt.plot(x, 0*x, label=&#39;Prior mean&#39;) plt.title(f&#39;sigma = {sigma}, sigma _n = {sigma_n}&#39;) plt.legend(bbox_to_anchor=(1,1)); . Another case . N = 10 x = np.linspace(0,5,N).reshape(-1,1) np.random.seed(2) rc(&#39;font&#39;,size=20) t0, t1 = 3, 4 sigma_n = 2 y = t0 + t1*x + np.random.multivariate_normal(np.zeros(N), np.eye(N)*sigma_n**2).reshape(-1,1) plt.scatter(x, y);plt.xlabel(&#39;x&#39;);plt.ylabel(&#39;y&#39;); . def LogLin2D(t0, t1): N = x.shape[0] return (N/2)*jnp.log(2*jnp.pi*sigma_n**2) + jnp.sum(jnp.square(y-t0-t1*x)) from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(figsize=(12,4)) ax = fig.add_subplot(121) T0, T1 = np.meshgrid(np.linspace(-5,8,11), np.linspace(-5,10,11)) Z = np.array([LogLin2D(t00, t11) for t00, t11 in zip(T0.ravel(), T1.ravel())]).reshape(*T0.shape) mp = ax.contour(T0, T1, Z); plt.clabel(mp); ax.set_xlabel(&#39;t0&#39;);ax.set_ylabel(&#39;t1&#39;); ax.set_title(&#39;Neg. log likelihood&#39;); x_extra = np.hstack([np.ones((x.shape[0], 1)), x]) t0_mle, t1_mle = np.linalg.inv(x_extra.T@x_extra)@x_extra.T@y t0_map, t1_map = np.linalg.inv(x_extra.T@x_extra + np.eye(x.shape[1])*(sigma_n**2/sigma**2))@x_extra.T@y ax.scatter(t0, t1, label=&#39;GT&#39;); # ax.annotate(&#39;GT&#39;, (t0, t1)) ax.scatter(t0_mle, t1_mle, label=&#39;MLE&#39;); # ax.annotate(&#39;MLE&#39;, (t0_mle, t1_mle)) ax.scatter(t0_map, t1_map, label=&#39;MAP&#39;); # ax.annotate(&#39;MAP&#39;, (t0_map, t1_map)) ax.scatter(0, 0, label=&#39;Prior mean&#39;); # ax.text(-4,0, &#39;Prior mean&#39;) ax.legend(bbox_to_anchor=(-0.2,1)) ax = fig.add_subplot(122, projection=&#39;3d&#39;) ax.plot_surface(T0, T1, Z); ax.view_init(35, 90+65); ax.set_xlabel(&#39;t0&#39;);ax.set_ylabel(&#39;t1&#39;); . plt.scatter(x,y); plt.plot(x, t0_mle+t1_mle*x, label=&#39;MLE&#39;) plt.plot(x, t0_map+t1_map*x, label=&#39;MAP&#39;) plt.plot(x, t0+t1*x, label=&#39;GT&#39;) plt.plot(x, 0*x, label=&#39;Prior mean&#39;) plt.title(f&#39;sigma = {sigma}, sigma _n = {sigma_n}&#39;) plt.legend(bbox_to_anchor=(1,1)); . Logistic regression (classification) . .",
            "url": "https://patel-zeel.github.io/blog/2021/03/23/bayesian-ml.html",
            "relUrl": "/2021/03/23/bayesian-ml.html",
            "date": " • Mar 23, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Understanding Kernels in Gaussian Process",
            "content": "!pip install -qq GPy import autograd.numpy as np import pandas as pd import GPy import matplotlib.pyplot as plt from autograd import grad from matplotlib.animation import FuncAnimation from matplotlib import rc import seaborn as sns . Basics of kernels in Gaussian processes . Gaussian process (GP) is a stochstic process where each observation is assumed to be a sample from a Gaussian (Normal) distribution. Probability density function (PDF) for a single observation $y_1$ is given as, $$ begin{aligned} y_1 sim mathcal{N} left( mu_1, sigma_1^2 right) end{aligned} $$ PDF of multiple such observations is a multivariate Gaussian distribution, $$ begin{aligned} Y sim mathcal{N} left(M, Sigma right) end{aligned} $$ In practice, $M$ and $ Sigma$ are modeled as functions of predictors $X$. . Now, multivariate PDF can be modified as, $$ begin{aligned} Y sim mathcal{N} left( mathcal{F}(X), mathcal{K}(X,X) right) end{aligned} $$ Where, $ mathcal{F}$ is a mean function and $ mathcal{K}$ is a kernel (covariance) function. Often $ mathcal{F}(X)$ is assumed to be zero ($ mathcal{F}(X)=0$) and only $ mathcal{K}(X, X)$ is employed to capture relationship between $X$ and $Y$. . The subsequent sections focus on various choices of $ mathcal{K}$ and their effect on $X$ and $Y$. . RBF (Radial basis function) Kernel, Stationarity and Isotropy . RBF is one of the most commonly used kernels in GPs due to it&#39;s infinetely differentiability (extreme flexibility). This property helps us to model a vast variety of functions $X to Y$. . RBF kernel is given as the following, $$ begin{aligned} mathcal{K}(x_1,x_2)= sigma^2exp left(- frac{(x-x&#39;)^2}{2l^2} right) end{aligned} $$ Where, $ sigma^2$ is variance and $l$ is known as lengthscale. . Stationarity . RBF is a stationary kernel and so it is invariant to translation in the input space. In other words, $ mathcal{K}(x,x&#39;)$ depends only on $x-x&#39;$. . Isotropy . RBF is also isotropic kernel, which means that $ mathcal{K}(x,x&#39;)$ depends only on $|x-x&#39;|$. Thus, we have $ mathcal{K}(x,x&#39;) = mathcal{K}(x&#39;,x)$. . Let&#39;s visualize few functions drawn from the RBF kernel . def K_rbf(X1, X2, sigma=1., l=1.): return (sigma**2)*(np.exp(-0.5*np.square(X1-X2.T)/l**2)) . Helper functions . def plot_functions(kernel_func, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1)): mean = np.zeros(X.shape[0]) cov = kernel_func(X, X, sigma, l) functions = np.random.multivariate_normal(mean, cov, size=5) fig = plt.figure(figsize=(14,8), constrained_layout=True) gs = fig.add_gridspec(2,4) ax0 = fig.add_subplot(gs[0, 1:-1]) ax0.set_ylim(*ax0_ylim) ax1 = fig.add_subplot(gs[1, 0:2]) ax1.set_ylim(*ax1_ylim) ax2 = fig.add_subplot(gs[1, 2:4]) for func in functions: ax0.plot(X, func,&#39;o-&#39;); ax0.set_xlabel(&#39;X&#39;);ax0.set_ylabel(&#39;Y&#39;);ax0.set_title(&#39;Functions drawn from &#39;+k_name+&#39; kernel&#39;); ax1.plot(X, cov[:,4]);ax1.set_title(&#39;K(0,X)&#39;);ax1.set_xlabel(&#39;X&#39;);ax1.set_ylabel(&#39;K(0,X)&#39;) sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True); ax2.set_xlabel(&#39;X&#39;);ax2.set_ylabel(&#39;X&#39;);ax2.set_title(&#39;Covariance matrix&#39;); def animate_functions(kernel_func, val_list, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1), k_name=&#39;&#39;,p_name=&#39;&#39;,symbol=&#39;&#39;): fig = plt.figure(figsize=(14,8)) gs = fig.add_gridspec(2,4) ax0 = fig.add_subplot(gs[0, 1:-1]);ax1 = fig.add_subplot(gs[1, 0:2]);ax2 = fig.add_subplot(gs[1, 2:4]); def update(p): ax0.cla();ax1.cla();ax2.cla(); ax0.set_ylim(*ax0_ylim);ax1.set_ylim(*ax1_ylim) if p_name == &#39;Lengthscale&#39;: cov = kernel_func(X, X, l=p) elif p_name == &#39;Variance&#39;: cov = kernel_func(X, X, sigma=np.sqrt(p)) elif p_name == &#39;Offset&#39;: cov = kernel_func(X, X, c=p) elif p_name == &#39;Period&#39;: cov = kernel_func(X, X, p=p) functions = np.random.multivariate_normal(mean, cov, size=5) for func in functions: ax0.plot(X, func,&#39;o-&#39;); ax0.set_xlabel(&#39;X&#39;);ax0.set_ylabel(&#39;Y&#39;);ax0.set_title(&#39;Functions drawn from &#39;+k_name+&#39; kernel n&#39;+p_name+&#39; (&#39;+symbol+&#39;) = &#39;+str(p)); ax1.plot(X, cov[:,4]);ax1.set_title(&#39;K(0,X)&#39;);ax1.set_title(&#39;K(0,X)&#39;);ax1.set_xlabel(&#39;X&#39;);ax1.set_ylabel(&#39;K(0,X)&#39;) sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True, cbar=False); ax2.set_xlabel(&#39;X&#39;);ax2.set_ylabel(&#39;X&#39;);ax2.set_title(&#39;Covariance matrix&#39;); anim = FuncAnimation(fig, update, frames=val_list, blit=False) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) return anim . Verifying if our kernel is consistent with GPy kernels. . X = np.linspace(101,1001,200).reshape(-1,1) sigma, l = 7, 11 assert np.allclose(K_rbf(X,X,sigma,l), GPy.kern.RBF(1, variance=sigma**2, lengthscale=l).K(X,X)) . np.random.seed(0) X = np.arange(-4,5).reshape(-1,1) sigma = 1. l = 3. k_name = &#39;RBF&#39; plot_functions(K_rbf, ax0_ylim=(-3.5,3)) . Let&#39;s see the effect of varying parameters $ sigma$ and $l$ of the RBF kernel function. . np.random.seed(0) sigma = 1. val_list = [0.5,1,2,3,4,5] animate_functions(K_rbf, val_list, k_name=&#39;RBF&#39;, p_name=&#39;Lengthscale&#39;, symbol=&#39;l&#39;) . &lt;/input&gt; Once Loop Reflect l = 1. val_list = [1,4,9,16,25] animate_functions(K_rbf, val_list, ax0_ylim=(-12,12), ax1_ylim=(-0.1, 26), k_name=&#39;RBF&#39;, p_name=&#39;Variance&#39;, symbol=&#39;sigma&#39;) . &lt;/input&gt; Once Loop Reflect With increase in value of $l$, functions drawn from the kernel become smoother. Covariance between a pair of points is increasing with increase in $l$. . Increasing $ sigma^2$ increase the overall uncertainty (width of the space where 95% of the functions live) across all the points. . Matern Kernel . Matern kernels are given by a general formula as following, $$ begin{aligned} mathcal{K}(x_1, x_2) = sigma^2 frac{1}{ Gamma( nu)2^{ nu-1}} Bigg( frac{ sqrt{2 nu}}{l} |x_1-x_2| Bigg)^ nu K_ nu Bigg( frac{ sqrt{2 nu}}{l} |x_1-x_2| Bigg) end{aligned} $$ Where, $ Gamma$ is gamma function and $K_ nu$ is modified Bessel function of second order. . The general formula is not very intuitive about the functionality of this kernel. In practice, Matern with $ nu= {0.5,1.5,2.5 }$ are used, where GP with each kernel is $( lceil nu rceil-1)$ times differentiable. . Matern functions corresponding to each $ nu$ values are defined as the following, $$ begin{aligned} Matern12 to mathcal{K_{ nu=0.5}}(x_1, x_2) &amp;= sigma^2exp left(- frac{|x_1-x_2|}{l} right) Matern32 to mathcal{K_{ nu=1.5}}(x_1, x_2) &amp;= sigma^2 left(1+ frac{ sqrt{3}|x_1-x_2|}{l} right)exp left(- frac{ sqrt{3}|x_1-x_2|}{l} right) Matern52 to mathcal{K_{ nu=2.5}}(x_1, x_2) &amp;= sigma^2 left(1+ frac{ sqrt{5}|x_1-x_2|}{l}+ frac{5(x_1-x_2)^2)}{3l^2} right)exp left(- frac{ sqrt{5}|x_1-x_2|}{l} right) end{aligned} $$ Matern kernels are stationary as well as isotropic. With $ nu to infty$ they converge to $RBF$ kernel. $Matern12$ is also known as $Exponential$ kernel in toolkits such as GPy. . Now, let&#39;s draw few functions from each of these versions and try to get intuition behind each of them. . def K_m12(X1, X2, sigma=1., l=1.): # v = 0.5 return (sigma**2)*(np.exp(-np.abs(X1-X2.T)/l)) def K_m32(X1, X2, sigma=1., l=1.): # v = 1.5 return (sigma**2)*(1+((3**0.5)*np.abs(X1-X2.T))/l)*(np.exp(-(3**0.5)*np.abs(X1-X2.T)/l)) def K_m52(X1, X2, sigma=1., l=1.): # v = 2.5 return (sigma**2)*(1+(((5**0.5)*np.abs(X1-X2.T))/l)+((5*(X1-X2.T)**2)/(3*l**2)))* (np.exp(-(5**0.5)*np.abs(X1-X2.T)/l)) . Verifying if our kernels are consistent with GPy kernels. . X = np.linspace(101,1001,50).reshape(-1,1) assert np.allclose(K_m32(X,X,sigma=7.,l=11.), GPy.kern.Matern32(1,lengthscale=11.,variance=7**2).K(X,X)) assert np.allclose(K_m52(X,X,sigma=7.,l=11.), GPy.kern.Matern52(1,lengthscale=11.,variance=7**2).K(X,X)) . X = np.arange(-4,5).reshape(-1,1) sigma = 1. l = 3. fig, ax = plt.subplots(3,2,figsize=(14,10)) names = [&#39;Matern12&#39;, &#39;Matern32&#39;, &#39;Matern52&#39;] for k_i, kernel in enumerate([K_m12, K_m32, K_m52]): mean = np.zeros(X.shape[0]) cov = kernel(X, X, sigma, l) functions = np.random.multivariate_normal(mean, cov, size=5) for func in functions: ax[k_i,0].plot(X, func); ax[k_i,0].set_xlabel(&#39;X&#39;);ax[k_i,0].set_ylabel(&#39;Y&#39;);ax[k_i,0].set_title(&#39;Functions drawn from &#39;+names[k_i]+&#39; kernel&#39;); sns.heatmap(cov.round(2), ax=ax[k_i,1], xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True); ax[k_i,1].set_xlabel(&#39;X&#39;);ax[k_i,1].set_ylabel(&#39;X&#39;);ax[k_i,1].set_title(&#39;Covariance matrix&#39;); plt.tight_layout(); . From the above plot, we can say that smoothness is increasing in functions as we increase $ nu$. Thus, smoothness of functions in terms of kernels is in the following order: Matern12&lt;Matern32&lt;Matern52. . Let us see effect of varying $ sigma$ and $l$ on Matern32 which is more popular among the three. . np.random.seed(0) sigma = 1. val_list = [0.5,1,2,3,4,5] animate_functions(K_m32, val_list, k_name=&#39;Matern32&#39;, p_name=&#39;Lengthscale&#39;, symbol=&#39;l&#39;) . &lt;/input&gt; Once Loop Reflect We can see that Matern32 kernel behaves similar to RBF with varying $l$. Though, Matern32 is less smoother than RBF. A quick comparison would clarify this. . X = np.linspace(-10,10,100).reshape(-1,1) plt.plot(X, K_rbf(X,X, l=3.)[:,50], label=&#39;RBF&#39;) plt.plot(X, K_m32(X,X, l=3.)[:,50], label=&#39;Matern32&#39;) plt.legend();plt.xlabel(&#39;X&#39;);plt.ylabel(&#39;Covariance (K(0,X))&#39;); plt.title(&#39;K(0,X)&#39;); . Periodic Kernel . Periodic Kernel is given as the following, $$ begin{aligned} mathcal{K}(x_1,x_2)= sigma^2 exp left(- frac{ sin^2( pi|x_1 - x_2|/p)}{2l^2} right) end{aligned} $$ Where $p$ is period. Let&#39;s visualize few functions drawn from this kernel. . def K_periodic(X1, X2, sigma=1., l=1., p=3.): return sigma**2 * np.exp(-0.5*np.square(np.sin(np.pi*(X1-X2.T)/p))/l**2) X = np.linspace(10,1001,50).reshape(-1,1) assert np.allclose(K_periodic(X,X,sigma=7.,l=11.,p=3.), GPy.kern.StdPeriodic(1,lengthscale=11.,variance=7**2,period=3.).K(X,X)) . np.random.seed(0) X = np.arange(-4,5).reshape(-1,1) sigma = 1 l = 1. p = 3. k_name = &#39;Periodic&#39; plot_functions(K_periodic) . We will investigate the effect of varying period $p$ now. . np.random.seed(0) val_list = [1., 2., 3., 4., 5.] animate_functions(K_periodic, val_list, ax1_ylim=(0.4,1.1), k_name=&#39;Periodic&#39;,p_name=&#39;Period&#39;) . &lt;/input&gt; Once Loop Reflect From the above animation we can see that, all points that are $p$ distance apart from each other have exactly same values because they have correlation of exactly 1 ($ sigma=1 to covariance=correlation$). . Now, we will investigate effect of lenging lengthscale $l$ while other parameters are constant. . np.random.seed(0) val_list = [1., 2., 3., 4., 5.] animate_functions(K_periodic, val_list, ax1_ylim=(0.6,1.1), k_name=&#39;Periodic&#39;,p_name=&#39;Lengthscale&#39;, symbol=&#39;l&#39;) . &lt;/input&gt; Once Loop Reflect We can see that correlation between a pair of locations $ {x_1,x_2|x_1-x_2&lt;p }$ increases as the lengthscale is increased. . Linear Kernel . Linear kernel (a.k.a. dot-product kernel) is given as the following, $$ begin{aligned} mathcal{K}(x_1,x_2)= (x_1-c)(x_2-c)+ sigma^2 end{aligned} $$ Let&#39;s visualize few functions drawn from the linear kernel . def K_lin(X1, X2, sigma=1., c=1.): return (X1-c)@(X2.T-c) + sigma**2 . np.random.seed(0) sigma = 1. c = 1. plot_functions(K_lin, ax0_ylim=(-10,5), ax1_ylim=(-3,7)) . Let&#39;s see the effect of varying parameters $ sigma$ and $c$ of the linear kernel function. . val_list = [-3,-2,-1,0,1,2,3] animate_functions(K_lin, val_list, ax0_ylim=(-15,12), ax1_ylim=(-3,23), p_name=&#39;Offset&#39;, symbol=&#39;c&#39;) . &lt;/input&gt; Once Loop Reflect np.random.seed(1) val_list = np.square(np.array([1,2,3,4,5,8])) animate_functions(K_lin, val_list, ax0_ylim=(-25,15), ax1_ylim=(-5,110), p_name=&#39;Variance&#39;, symbol=&#39;sigma&#39;) . &lt;/input&gt; Once Loop Reflect Varying $c$ parameter changes position of shallow region in covariance matrix. In other words, as $x to c$, points close to $x$ have variance $ to sigma^2$. Distant points have monotonically increasing variance. . Increasing $ sigma^2$ adds a constant in all variance and covariances. So, it allows more uncertainty across all points and weakens the monotonic trend of variance over distant points. . Non-stationary behaviour of Linear kernel . Unlike other stationary kernels, Linear kernel is not invariant of translations in the input space. The comparison below, visually supports this claim. . fig, ax = plt.subplots(2,2,figsize=(14,8), sharex=True) kerns = [K_rbf, K_m32, K_periodic, K_lin] k_names = [&#39;RBF&#39;, &#39;Matern32&#39;, &#39;Periodic&#39;, &#39;Linear&#39;] X = np.linspace(-10,10,21).reshape(-1,1) def update(x): count = 0 for i in range(2): for j in range(2): ax.ravel()[count].cla() tmp_kern = kerns[count] mean = np.zeros(X.shape[0]) cov = tmp_kern(X,X) ax.ravel()[count].plot(X, cov[:,x]); ax.ravel()[count].set_xlim(X[x-3],X[x+3]) ax.ravel()[count].set_xlabel(&#39;X&#39;); ax.ravel()[count].set_ylabel(&#39;K(&#39;+str(X[x].round(2))+&#39;,X)&#39;); ax.ravel()[count].set_title(&#39;Covariance K(&#39;+str(X[x].round(2))+&#39;,X) for &#39;+k_names[count]+&#39; kernel&#39;); count += 1 ax.ravel()[3].set_ylim(-5,80) plt.tight_layout() anim = FuncAnimation(fig, update, frames=[5,7,9,11,13,15], blit=False) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect &lt;Figure size 432x288 with 0 Axes&gt; . Multiplications of kernels . If a single kernel is having high bias in fitting a dataset, we can use mutiple of these kernels in multiplications and/or summations. First, let us see effect of multiplication of a few kernels. . Periodic * Linear . X = np.linspace(-10,10,100).reshape(-1,1) plt.plot(X, K_periodic(X,X,sigma=2.)[:,50], label=&#39;Periodic&#39;) plt.plot(X, K_lin(X,X,sigma=0.01,c=0)[:,50], label=&#39;Linear&#39;) plt.plot(X, K_periodic(X,X,sigma=2.)[:,50]*K_lin(X,X,sigma=0.01,c=0)[:,50], label=&#39;Periodic*Linear&#39;) plt.legend(bbox_to_anchor=(1,1));plt.xlabel(&#39;X&#39;);plt.ylabel(&#39;Covariance&#39;) plt.title(&#39;K(0,*)&#39;); . Linear * Linear . X = np.linspace(-1,1,100).reshape(-1,1) plt.plot(X, K_lin(X,X,c=-1)[:,50], label=&#39;Linear1&#39;) plt.plot(X, K_lin(X,X,c=1)[:,50], label=&#39;Linear2&#39;) plt.plot(X, K_lin(X,X,c=0.5)[:,50], label=&#39;Linear3&#39;) plt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50], label=&#39;Linear1*Linear3&#39;) plt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50]*K_lin(X,X,c=0.5)[:,50], label=&#39;Linear1*Linear2*Linear3&#39;) plt.legend(bbox_to_anchor=(1,1)); . Matern * Linear . X = np.linspace(-1,1,100).reshape(-1,1) k1 = K_lin(X,X,c=1)[:,50] k2 = K_m32(X,X)[:,50] plt.plot(X, k1, label=&#39;Linear&#39;) plt.plot(X, k2, label=&#39;Matern32&#39;) plt.plot(X, k1*k2, label=&#39;Matern32*Linear&#39;) plt.legend(bbox_to_anchor=(1,1)); . Appendix (Extra material) . At this stage, we do not know how the fuctions are drawn from linear kernel based covariance matrix end up being lines with various intercepts and slopes. . . Predicting at a single point after observing value at a single point . Let&#39;s see how would be a GP prediction after observing value at a single point. . Our kernel function is given by, . $K(x,x&#39;)=(x-c) cdot (x&#39;-c)+ sigma^2$ | . Now, we observe value $y$ at a location $x$ and we want to predict value $y^*$ at location $x^*$. $$ begin{aligned} (y^*|x_1,y_1,x^*) &amp;= K(x^*,x) cdot K^{-1}(x,x) cdot y &amp;= left( frac{(x-c)(x^*-c)+ sigma^2}{(x-c)(x-c)+ sigma^2} right) cdot y end{aligned} $$ $c$ and $ sigma^2$ do not vary in numerator and denominator so, the value of $y^* propto x^*$. . . Predicting at a single point after observing values at two points . Now, we&#39;ll take a case where two values ${y_1, y_2}$ are observed at ${x_1, x_2}$. Let us try to predict value $y^*$ at $x^*$. . $$ y^ = begin{bmatrix} K(x_1, x^) &amp; K(x_2,x^*) end{bmatrix} begin{bmatrix} K(x_1, x_1) &amp; K(x_1,x_2) K(x_2, x_1) &amp; K(x_2,x_2) end{bmatrix}^{-1} begin{bmatrix} y_1 y_2 end{bmatrix} . &amp; = begin{bmatrix} (x_1-c)(x^*-c)+ sigma^2 &amp; (x_2-c)(x^*-c)+ sigma^2 end{bmatrix} begin{bmatrix} (x_1-c)^2+ sigma^2 &amp; (x_1-c) (x_2-c)+ sigma^2 (x_2-c) (x_1-c)+ sigma^2 &amp; (x_2-c)^2 + sigma^2 end{bmatrix}^{-1} begin{bmatrix} y_1 y_2 end{bmatrix} . &amp; = begin{bmatrix} (x_1-c)(x^*-c)+ sigma^2 &amp; (x_2-c)(x^*-c)+ sigma^2 end{bmatrix} frac{1}{ sigma^2(x_1-x_2)^2} begin{bmatrix} (x_2-c)^2+ sigma^2 &amp; -[(x_1-c)(x_2-c)+ sigma^2] -[(x_2-c) (x_1-c)+ sigma^2] &amp; (x_1-c)^2 + sigma^2 end{bmatrix} begin{bmatrix} y_1 y_2 end{bmatrix} tag{1} . From Eq. (1) second term, we can say that if $ sigma^2=0$, matrix is not-invertible because determinant is zero. It means that, if $ sigma^2=0$, observing a single point is enough, we can infer values at infinite points after observing that single point. . Evaluating Eq. (1) further, it converges to the following equation, $$ begin{aligned} y^* = frac{(x_1y_2-x_2y_1)+x^*(y_1-y_2)}{(x_1-x_2)} end{aligned} $$ Interestingly, we can see that output does not depend on $c$ or $ sigma^2$ anymore. Let us verify experimentally if this is true for observing more than 2 data points. . Prepering useful functions . from scipy.optimize import minimize . def cov_func(x, x_prime, sigma, c): return (x-c)@(x_prime-c) + sigma**2 def neg_log_likelihood(params): n = X.shape[0] sigma, c, noise_std = params cov = cov_func(X, X.T, sigma, c) cov = cov + (noise_std**2)*np.eye(n) nll_ar = 0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) return nll_ar[0,0] def predict(params): sigma, c, noise_std = params k = cov_func(X, X.T, sigma, c) np.fill_diagonal(k, k.diagonal()+noise_std**2) k_inv = np.linalg.pinv(k) k_star = cov_func(X_test, X.T, sigma, c) mean = k_star@k_inv@Y cov = cov_func(X_test, X_test.T, sigma, c) - k_star@k_inv@k_star.T return mean, cov . Observing more than two points and changing hyperparameters manually . X = np.array([3,4,5,6,7,8]).reshape(-1,1) Y = np.array([6,9,8,11,10,13]).reshape(-1,1) X_test = np.linspace(1,8,20).reshape(-1,1) params_grid = [[1., 0.01, 10**-10], [100., 1., 10**-10], [100., 0.01, 10**-10], [1., 2., 1.]] # sigma, c, noise_std X_extra = np.hstack([np.ones((X.shape[0], 1)), X]) Theta = np.linalg.pinv(X_extra.T@X_extra)@X_extra.T@Y X_test_extra = np.hstack([np.ones((X_test.shape[0], 1)), X_test]) Y_test_ideal = X_test_extra@Theta fig, ax = plt.subplots(1,4,figsize=(16,5), sharey=True) means = [] for p_i, params in enumerate(params_grid): Y_test_mean, Y_test_cov = predict(params) means.append(Y_test_mean) ax[p_i].scatter(X, Y, label=&#39;train&#39;) ax[p_i].scatter(X_test, Y_test_mean, label=&#39;test&#39;) ax[p_i].legend();ax[p_i].set_xlabel(&#39;X&#39;);ax[p_i].set_ylabel(&#39;Y&#39;); ax[p_i].set_title(&#39;sigma=&#39;+str(params[0])+&#39;, c=&#39;+str(params[1])+&#39;, noise=&#39;+str(params[2])); . np.allclose(Y_test_ideal, means[0]), np.allclose(Y_test_ideal, means[1]), np.allclose(Y_test_ideal, means[2]), np.allclose(Y_test_ideal, means[3]) . (True, True, True, False) . model = GPy.models.GPRegression(X, Y, GPy.kern.Linear(input_dim=1)) # model[&#39;Gaussian_noise&#39;].fix(10**-10) # model.kern.variances.fix(10**-10) model.optimize() model.plot() plt.plot(X_test, Y_test_ideal, label=&#39;Normal Eq. fit&#39;) plt.plot(X_test,model.predict(X_test)[0], label=&#39;Prediction&#39;) plt.legend() model . Model: GP regression Objective: 13.51314321804978 Number of Parameters: 2 Number of Optimization Parameters: 2 Updates: True . GP_regression. valueconstraintspriors . &lt;td class=tg-left&gt; linear.variances &lt;/td&gt;&lt;td class=tg-right&gt; 2.806515343539501&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; Gaussian_noise.variance&lt;/td&gt;&lt;td class=tg-right&gt;2.0834221617534134&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . We can see that there is no change in fit with change in $c$ and $ sigma$. 4th fit is not matching with the ideal fit obtained by normal equation because of high noise. Now, let us estimate parameters by minimizing negative log marginal likelihood. . params = [1., 1., 1.] result = minimize(neg_log_likelihood, params, bounds=[(10**-5, 10**5), (10**-5, 10**5), (10**-5, 10**-5)]) params = result.x print(params, result.fun) Y_test_mean, Y_test_cov = predict(params) plt.scatter(X, Y, label=&#39;train&#39;) plt.scatter(X_test, Y_test_mean, label=&#39;test&#39;) plt.legend();plt.xlabel(&#39;X&#39;);plt.ylabel(&#39;Y&#39;); params = np.round(params, 4) plt.title(&#39;sigma=&#39;+str(params[0])+&#39;, c=&#39;+str(params[1])+&#39;, noise=&#39;+str(params[2])); np.allclose(Y_test_ideal, Y_test_mean) . [9.99998123e-01 9.99998123e-01 1.00000000e-05] 10207223403405.541 . False . def neg_log_likelihood(sigma, c, noise_std): n = X.shape[0] cov = cov_func(X, X.T, sigma, c) cov = cov + (noise_std**2)*np.eye(n) nll_ar = 0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) return nll_ar[0,0] . grad_func = grad(neg_log_likelihood, argnum=[0,1,2]) alpha = 0.01 loss = [] sigma, c, noise_std = 1., 1., 1. for _ in range(5000): grads = grad_func(sigma, c, noise_std) # print(grads) sigma = sigma - alpha*grads[0] c = c - alpha*grads[1] noise_std = noise_std - alpha*grads[2] loss.append(neg_log_likelihood(sigma, c, noise_std)) print(sigma, c, noise_std) plt.plot(loss); loss[-1] . 7.588989986845149 -2.830840439162303 32.2487569348891 . 31.05187173290998 . params = sigma, c, noise_std Y_test_mean, Y_test_cov = predict(params) plt.scatter(X, Y, label=&#39;train&#39;) plt.scatter(X_test, Y_test_mean, label=&#39;test&#39;) plt.legend();plt.xlabel(&#39;X&#39;);plt.ylabel(&#39;Y&#39;); params = np.round(params, 4) plt.title(&#39;sigma=&#39;+str(params[0])+&#39;, c=&#39;+str(params[1])+&#39;, noise=&#39;+str(params[2])); np.allclose(means[0], Y_test_mean, rtol=10**-1, atol=10**-1) . False .",
            "url": "https://patel-zeel.github.io/blog/ml/2021/03/22/GP_Kernels.html",
            "relUrl": "/ml/2021/03/22/GP_Kernels.html",
            "date": " • Mar 22, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Programatically download OpenAQ data",
            "content": "# !pip install boto3 botocore import pandas as pd import numpy as np import matplotlib.pyplot as plt import sys import boto3 import botocore import os from IPython.display import clear_output . Setup . s3 = boto3.client(&#39;s3&#39;, config=botocore.config.Config(signature_version=botocore.UNSIGNED)) bucket_name = &#39;openaq-fetches&#39; prefix = &#39;realtime-gzipped/&#39; path = &#39;/content/drive/MyDrive/IJCAI-21/data/OpenAQ-Delhi/&#39; start_date = &#39;2020/01/01&#39; # start date (inclusive) end_date = &#39;2020/12/31&#39; # end date (inclusive) . Download . for date in pd.date_range(start=start_date, end=end_date): clear_output(wait=True) date = str(date).split(&#39; &#39;)[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS print(&#39;Downloading:&#39;, date) data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date) for file_obj in data_dict[&#39;Contents&#39;]: f_name = file_obj[&#39;Key&#39;] tmp_path = &#39;/&#39;.join((path+f_name).split(&#39;/&#39;)[:-1]) if not os.path.exists(tmp_path): os.makedirs(tmp_path) s3.download_file(bucket_name, f_name, path+f_name) . Downloading: 2020-05-04 . Validate . for date in pd.date_range(start=start_date, end=end_date): date = str(date).split(&#39; &#39;)[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date) for file_obj in data_dict[&#39;Contents&#39;]: assert os.path.exists(path+file_obj[&#39;Key&#39;]), file_obj[&#39;Key&#39;] print(&#39;Validated&#39;) .",
            "url": "https://patel-zeel.github.io/blog/data/openaq/2020/09/21/Programatically_download_OpenAQ_data.html",
            "relUrl": "/data/openaq/2020/09/21/Programatically_download_OpenAQ_data.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Active Learning with Bayesian Linear Regression",
            "content": "A quick wrap-up for Bayesian Linear Regression (BLR) . We have a feature matrix $X$ and a target vector $Y$. We want to obtain $ theta$ vector in such a way that the error $ epsilon$ for the following equation is minimum. . $$ Y = X^T theta + epsilon $$Prior PDF for $ theta$ is, . $$ p( theta) sim mathcal{N}(M_0, S_0) $$Where $S_0$ is prior covariance matrix, and $M_0$ is prior mean. . Posterier PDF can be given as, . $$ begin{aligned} p( theta|X,Y) &amp; sim mathcal{N}( theta | M_n, S_n) S_n &amp;= (S_0^{-1} + sigma_{mle}^{-2}X^TX) M_n &amp;= S_n(S_0^{-1}M_0+ sigma_{mle}^{-2}X^TY) end{aligned} $$Maximum likelihood estimation of $ sigma$ can be calculated as, . $$ begin{aligned} theta_{mle} &amp;= (X^TX)^{-1}X^TY sigma_{mle} &amp;= ||Y - X^T theta_{mle}|| end{aligned} $$Finally, predicted mean $ hat{Y}_{mean}$ and predicted covariance matrix $ hat{Y}_{cov}$ can be given as, . $$ begin{aligned} hat{Y} &amp; sim mathcal{N}( hat{Y}_{mean}, hat{Y}_{cov}) hat{Y}_{mean} &amp;= XM_n hat{Y}_{cov} &amp;= X^TS_nX end{aligned} $$Now, let&#39;s put everything together and write a class for Bayesian Linear Regression. . Creating scikit-learn like class with fit predict methods for BLR . import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn import datasets from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler from sklearn.model_selection import train_test_split from matplotlib.animation import FuncAnimation from matplotlib import rc import warnings warnings.filterwarnings(&#39;ignore&#39;) seed = 0 # random seed for train_test_split . class BLR(): def __init__(self,S0, M0): # M0 -&gt; prior mean, S0 -&gt; prior covariance matrix self.S0 = S0 self.M0 = M0 def fit(self,x,y, return_self = False): self.x = x self.y = y # Maximum likelihood estimation for sigma parameter theta_mle = np.linalg.pinv(x.T@x)@(x.T@y) sigma_2_mle = np.linalg.norm(y - x@theta_mle)**2 sigma_mle = np.sqrt(sigma_2_mle) # Calculating predicted mean and covariance matrix for theta self.SN = np.linalg.pinv(np.linalg.pinv(self.S0) + (sigma_mle**-2)*x.T@x) self.MN = self.SN@(np.linalg.pinv(self.S0)@self.M0 + (sigma_mle**-2)*(x.T@y).squeeze()) # Calculating predicted mean and covariance matrix for data self.pred_var = x@self.SN@x.T self.y_hat_map = x@self.MN if return_self: return (self.y_hat_map, self.pred_var) def predict(self, x): self.pred_var = x@self.SN@x.T self.y_hat_map = x@self.MN return (self.y_hat_map, self.pred_var) def plot(self, s=1): # s -&gt; size of dots for scatter plot individual_var = self.pred_var.diagonal() plt.figure() plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.plot(self.x[:,1], self.y_hat_map, color=&#39;black&#39;, label=&#39;model&#39;) plt.fill_between(self.x[:,1], self.y_hat_map-individual_var, self.y_hat_map+individual_var, alpha=0.4, color=&#39;black&#39;, label=&#39;uncertainty&#39;) plt.scatter(self.x[:,1], self.y, label=&#39;actual data&#39;,s=s) plt.title(&#39;MAE is &#39;+str(np.mean(np.abs(self.y - self.y_hat_map)))) plt.legend() . Creating &amp; visualizing dataset . To start with, let&#39;s create a random dataset with degree 3 polynomial function with some added noise. . $$ Y = (5X^3 - 4X^2 + 3X - 2) + mathcal{N}(0,1) $$ np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) noise = np.random.randn(1000, ) Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . We&#39;ll try to fit a degree 5 polynomial function to our data. . X = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1)) N_features = X.shape[1] . plt.scatter(X[:,1], Y, s=0.5, label = &#39;data points&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.legend() plt.show() . Learning a BLR model on the entire data . We&#39;ll take $M_0$ (prior mean) as zero vector initially, assuming that we do not have any prior knowledge about $M_0$. We&#39;re taking $S_0$ (prior covariance) as the identity matrix, assuming that all coefficients are completely independent of each other. . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) model = BLR(S0, M0) . model.fit(X, Y) . Visualising the fit . model.plot(s=0.5) . This doesn&#39;t look like a good fit, right? Let&#39;s set the prior closer to the real values and visualize the fit again. . Visualising the fit after changing the prior . np.random.seed(seed) S0 = np.eye(N_features) M0 = np.array([-2, 3, -4, 5, 0, 0]) + np.random.randn(N_features, ) model = BLR(S0, M0) . model.fit(X, Y) model.plot(s=0.5) . Hmm, better. Now let&#39;s see how it fits after reducing the noise and setting the prior mean to zero vector again. . Visualising the fit after reducing the noise . np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) noise = np.random.randn(1000, ) * 0.5 Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) model = BLR(S0, M0) . model.fit(X, Y) model.plot(s=0.5) . When the noise was high, the model tended to align with the prior. After keeping the prior closer to the original coefficients, the model was improved as expected. From the last plot, we can say that as noise reduces from the data, the impact of the prior reduces, and the model tries to fit the data more precisely. Therefore, we can say that when data is too noisy or insufficient, a wisely chosen prior can produce a precise fit. . Intuition to Active Learning (Uncertainty Sampling) with an example . Let&#39;s take the case where we want to train a machine learning model to classify if a person is infected with COVID-19 or not, but the testing facilities for the same are not available so widely. We may have very few amounts of data for detected positive and detected negative patients. Now, we want our model to be highly confident or least uncertain about its results; otherwise, it may create havoc for wrongly classified patients, but, our bottleneck is labeled data. Thanks to active learning techniques, we can overcome this problem smartly. How? . We train our model with existing data and test it on all the suspected patients&#39; data. Let&#39;s say we have an uncertainty measure or confidence level about each tested data point (distance from the decision boundary in case of SVM, variance in case of Gaussian processes, or Bayesian Linear Regression). We can choose a patient for which our model is least certain, and send him to COVID-19 testing facilities (assuming that we can send only one patient at a time). Now, we can include his data to the train set and test the model on everyone else. By following the same procedure repeatedly, we can increase the size of our train data and confidence of the model without sending everyone randomly for testing. . This method is called Uncertainty Sampling in Active Learning. Now let&#39;s formally define Active Learning. From Wikipedia, . Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. . Now, we&#39;ll go through the active learning procedure step by step. . Train set, test set, and pool. What is what? . The train set includes labeled data points. The pool includes potential data points to query for a label, and the test set includes labeled data points to check the performance of our model. Here, we cannot actually do a query to anyone, so we assume that we do not have labels for the pool while training, and after each iteration, we include a data point from the pool set to the train set for which our model has the highest uncertainty. . So, the algorithm can be represented as the following, . Train the model with the train set. | Test the performance on the test set (This is expected to improve). | Test the model with the pool. | Query for the most uncertain datapoint from the pool. | Add that datapoint into the train set. | Repeat step 1 to step 5 for $K$ iterations ($K$ ranges from $0$ to the pool size). | Creating initial train set, test set, and pool . Let&#39;s take half of the dataset as the test set, and from another half, we will start with some points as the train set and remaining as the pool. Let&#39;s start with 2 data points as the train set. . np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) X = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1)) noise = np.random.randn(1000, ) * 0.5 Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . train_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed) train_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=2, random_state=seed) . Visualizing train, test and pool. . plt.scatter(test_X[:,1], test_Y, label=&#39;test set&#39;,color=&#39;r&#39;, s=2) plt.scatter(train_X[:,1], train_Y, label=&#39;train set&#39;,marker=&#39;s&#39;,color=&#39;k&#39;, s=50) plt.scatter(pool_X[:,1], pool_Y, label=&#39;pool&#39;,color=&#39;b&#39;, s=2) plt.xlabel(&#39;X&#39;) plt.ylabel(&#39;Y&#39;) plt.legend() plt.show() . Let&#39;s initialize a few dictionaries to keep track of each iteration. . train_X_iter = {} # to store train points at each iteration train_Y_iter = {} # to store corresponding labels to the train set at each iteration models = {} # to store the models at each iteration estimations = {} # to store the estimations on the test set at each iteration test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration . Training &amp; testing initial learner on train set (Iteration 0) . Now we will train the model for the initial train set, which is iteration 0. . train_X_iter[0] = train_X train_Y_iter[0] = train_Y . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) models[0] = BLR(S0, M0) . models[0].fit(train_X_iter[0], train_Y_iter[0]) . Creating a plot method to visualize train, test and pool with estimations and uncertainty. . def plot(ax, model, init_title=&#39;&#39;): # Plotting the pool ax.scatter(pool_X[:,1], pool_Y, label=&#39;pool&#39;,s=1,color=&#39;r&#39;,alpha=0.4) # Plotting the test data ax.scatter(test_X[:,1], test_Y, label=&#39;test data&#39;,s=1, color=&#39;b&#39;, alpha=0.4) # Combining the test &amp; the pool test_pool_X, test_pool_Y = np.append(test_X,pool_X, axis=0), np.append(test_Y,pool_Y) # Sorting test_pool for plotting sorted_inds = np.argsort(test_pool_X[:,1]) test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds] # Plotting test_pool with uncertainty model.predict(test_pool_X) individual_var = model.pred_var.diagonal() ax.plot(test_pool_X[:,1], model.y_hat_map, color=&#39;black&#39;, label=&#39;model&#39;) ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var , alpha=0.2, color=&#39;black&#39;, label=&#39;uncertainty&#39;) # Plotting the train data ax.scatter(model.x[:,1], model.y,s=40, color=&#39;k&#39;, marker=&#39;s&#39;, label=&#39;train data&#39;) ax.scatter(model.x[-1,1], model.y[-1],s=80, color=&#39;r&#39;, marker=&#39;o&#39;, label=&#39;last added point&#39;) # Plotting MAE on the test set model.predict(test_X) ax.set_title(init_title+&#39; MAE is &#39;+str(np.mean(np.abs(test_Y - model.y_hat_map)))) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.legend() . Plotting the estimations and uncertainty. . fig, ax = plt.subplots() plot(ax, models[0]) . Let&#39;s check the maximum uncertainty about any point for the model. . models[0].pred_var.diagonal().max() . 4.8261426545316604e-29 . Oops!! There is almost no uncertainty in the model. Why? let&#39;s try again with more train points. . train_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed) train_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=7, random_state=seed) . train_X_iter[0] = train_X train_Y_iter[0] = train_Y . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) models[0] = BLR(S0, M0) . models[0].fit(train_X_iter[0], train_Y_iter[0]) . fig, ax = plt.subplots() plot(ax, models[0]) . Now uncertainty is visible, and currently, it&#39;s high at the left-most points. We are trying to fit a degree 5 polynomial here. So our linear regression coefficients are 6, including the bias. If we choose train points equal to or lesser than 6, our model perfectly fits the train points and has no uncertainty. Choosing train points more than 6 induces uncertainty in the model. . Let&#39;s evaluate the performance on the test set. . estimations[0], _ = models[0].predict(test_X) test_mae_error[0] = np.mean(np.abs(test_Y - estimations[0])) . Mean Absolute Error (MAE) on the test set is . test_mae_error[0] . 0.5783654195019617 . Moving the most uncertain point from the pool to the train set . In the previous plot, we saw that the model was least certain about the left-most point. We&#39;ll move that point from the pool to the train set and see the effect. . esimations_pool, _ = models[0].predict(pool_X) . Finding out a point having the most uncertainty. . in_var = models[0].pred_var.diagonal().argmax() to_add_x = pool_X[in_var,:] to_add_y = pool_Y[in_var] . Adding the point from the pool to the train set. . train_X_iter[1] = np.vstack([train_X_iter[0], to_add_x]) train_Y_iter[1] = np.append(train_Y_iter[0], to_add_y) . Deleting the point from the pool. . pool_X = np.delete(pool_X, in_var, axis=0) pool_Y = np.delete(pool_Y, in_var) . Training again and visualising the results (Iteration 1) . This time, we will pass previously learnt prior to the next iteration. . S0 = np.eye(N_features) models[1] = BLR(S0, models[0].MN) . models[1].fit(train_X_iter[1], train_Y_iter[1]) . estimations[1], _ = models[1].predict(test_X) test_mae_error[1] = np.mean(np.abs(test_Y - estimations[1])) . MAE on the test set is . test_mae_error[1] . 0.5779411133071186 . Visualizing the results. . fig, ax = plt.subplots() plot(ax, models[1]) . Before &amp; after adding most uncertain point . fig, ax = plt.subplots(1,2, figsize=(13.5,4.5)) plot(ax[0], models[0],&#39;Before&#39;) plot(ax[1], models[1],&#39;After&#39;) . We can see that including most uncertain point into the train set has produced a better fit and MAE for test set has been reduced. Also, uncertainty has reduced at the left part of the data but it has increased a bit on the right part of the data. . Now let&#39;s do this for few more iterations in a loop and visualise the results. . Active learning procedure . num_iterations = 20 points_added_x= np.zeros((num_iterations+1, N_features)) points_added_y=[] print(&quot;Iteration, Cost n&quot;) print(&quot;-&quot;*40) for iteration in range(2, num_iterations+1): # Making predictions on the pool set based on model learnt in the respective train set estimations_pool, var = models[iteration-1].predict(pool_X) # Finding the point from the pool with highest uncertainty in_var = var.diagonal().argmax() to_add_x = pool_X[in_var,:] to_add_y = pool_Y[in_var] points_added_x[iteration-1,:] = to_add_x points_added_y.append(to_add_y) # Adding the point to the train set from the pool train_X_iter[iteration] = np.vstack([train_X_iter[iteration-1], to_add_x]) train_Y_iter[iteration] = np.append(train_Y_iter[iteration-1], to_add_y) # Deleting the point from the pool pool_X = np.delete(pool_X, in_var, axis=0) pool_Y = np.delete(pool_Y, in_var) # Training on the new set models[iteration] = BLR(S0, models[iteration-1].MN) models[iteration].fit(train_X_iter[iteration], train_Y_iter[iteration]) estimations[iteration], _ = models[iteration].predict(test_X) test_mae_error[iteration]= pd.Series(estimations[iteration] - test_Y.squeeze()).abs().mean() print(iteration, (test_mae_error[iteration])) . Iteration, Cost - 2 0.49023173501654815 3 0.4923391714942153 4 0.49040074812746753 5 0.49610198614600165 6 0.5015282102751122 7 0.5051264429971314 8 0.5099913097301352 9 0.504455016053513 10 0.5029219102020734 11 0.5009762782262487 12 0.5004883097883343 13 0.5005169638980388 14 0.5002731089932334 15 0.49927485683909884 16 0.49698416490822594 17 0.49355398855432897 18 0.49191185613804617 19 0.491164833699368 20 0.4908067530719673 . pd.Series(test_mae_error).plot(style=&#39;ko-&#39;) plt.xlim((-0.5, num_iterations+0.5)) plt.ylabel(&quot;MAE on test set&quot;) plt.xlabel(&quot;# Points Queried&quot;) plt.show() . The plot above shows that MAE on the test set fluctuates a bit initially then reduces gradually as we keep including more points from the pool to the train set. Let&#39;s visualise fits for all the iterations. We&#39;ll discuss this behaviour after that. . Visualizing active learning procedure . print(&#39;Initial model&#39;) print(&#39;Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}&#39;.format(*models[0].MN[::-1])) print(&#39; nFinal model&#39;) print(&#39;Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}&#39;.format(*models[num_iterations].MN[::-1])) . Initial model Y = 1.89 X^5 + 1.54 X^4 + 0.84 X^3 + -6.48 X^2 + 4.74 X + -1.63 Final model Y = 2.50 X^5 + 3.11 X^4 + 0.83 X^3 + -7.08 X^2 + 4.47 X + -1.58 . def update(iteration): ax.cla() plot(ax, models[iteration]) fig.tight_layout() . fig, ax = plt.subplots() anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations+1, 1), interval=250) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) . anim . &lt;/input&gt; Once Loop Reflect We can see that the point having highest uncertainty was chosen in first iteration and it produced the near optimal fit. After that, error reduced gradually. . Now, let&#39;s put everything together and create a class for active learning procedure . Creating a class for active learning procedure . class ActiveL(): def __init__(self, X, y, S0=None, M0=None, test_size=0.5, degree = 5, iterations = 20, seed=1): self.X_init = X self.y = y self.S0 = S0 self.M0 = M0 self.train_X_iter = {} # to store train points at each iteration self.train_Y_iter = {} # to store corresponding labels to the train set at each iteration self.models = {} # to store the models at each iteration self.estimations = {} # to store the estimations on the test set at each iteration self.test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration self.test_size = test_size self.degree = degree self.iterations = iterations self.seed = seed self.train_size = degree + 2 def data_preperation(self): # Adding polynomial features self.X = PolynomialFeatures(degree=self.degree).fit_transform(self.X_init) N_features = self.X.shape[1] # Splitting into train, test and pool train_pool_X, self.test_X, train_pool_Y, self.test_Y = train_test_split(self.X, self.y, test_size=self.test_size, random_state=self.seed) self.train_X, self.pool_X, self.train_Y, self.pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=self.train_size, random_state=self.seed) # Setting BLR prior incase of not given if self.M0 == None: self.M0 = np.zeros((N_features, )) if self.S0 == None: self.S0 = np.eye(N_features) def main(self): # Training for iteration 0 self.train_X_iter[0] = self.train_X self.train_Y_iter[0] = self.train_Y self.models[0] = BLR(self.S0, self.M0) self.models[0].fit(self.train_X, self.train_Y) # Running loop for all iterations for iteration in range(1, self.iterations+1): # Making predictions on the pool set based on model learnt in the respective train set estimations_pool, var = self.models[iteration-1].predict(self.pool_X) # Finding the point from the pool with highest uncertainty in_var = var.diagonal().argmax() to_add_x = self.pool_X[in_var,:] to_add_y = self.pool_Y[in_var] # Adding the point to the train set from the pool self.train_X_iter[iteration] = np.vstack([self.train_X_iter[iteration-1], to_add_x]) self.train_Y_iter[iteration] = np.append(self.train_Y_iter[iteration-1], to_add_y) # Deleting the point from the pool self.pool_X = np.delete(self.pool_X, in_var, axis=0) self.pool_Y = np.delete(self.pool_Y, in_var) # Training on the new set self.models[iteration] = BLR(self.S0, self.models[iteration-1].MN) self.models[iteration].fit(self.train_X_iter[iteration], self.train_Y_iter[iteration]) self.estimations[iteration], _ = self.models[iteration].predict(self.test_X) self.test_mae_error[iteration]= pd.Series(self.estimations[iteration] - self.test_Y.squeeze()).abs().mean() def _plot_iter_MAE(self, ax, iteration): ax.plot(list(self.test_mae_error.values())[:iteration+1], &#39;ko-&#39;) ax.set_title(&#39;MAE on test set over iterations&#39;) ax.set_xlim((-0.5, self.iterations+0.5)) ax.set_ylabel(&quot;MAE on test set&quot;) ax.set_xlabel(&quot;# Points Queried&quot;) def _plot(self, ax, model): # Plotting the pool ax.scatter(self.pool_X[:,1], self.pool_Y, label=&#39;pool&#39;,s=1,color=&#39;r&#39;,alpha=0.4) # Plotting the test data ax.scatter(self.test_X[:,1], self.test_Y, label=&#39;test data&#39;,s=1, color=&#39;b&#39;, alpha=0.4) # Combining test_pool test_pool_X, test_pool_Y = np.append(self.test_X, self.pool_X, axis=0), np.append(self.test_Y, self.pool_Y) # Sorting test_pool sorted_inds = np.argsort(test_pool_X[:,1]) test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds] # Plotting test_pool with uncertainty preds, var = model.predict(test_pool_X) individual_var = var.diagonal() ax.plot(test_pool_X[:,1], model.y_hat_map, color=&#39;black&#39;, label=&#39;model&#39;) ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var , alpha=0.2, color=&#39;black&#39;, label=&#39;uncertainty&#39;) # plotting the train data ax.scatter(model.x[:,1], model.y,s=10, color=&#39;k&#39;, marker=&#39;s&#39;, label=&#39;train data&#39;) ax.scatter(model.x[-1,1], model.y[-1],s=80, color=&#39;r&#39;, marker=&#39;o&#39;, label=&#39;last added point&#39;) # plotting MAE preds, var = model.predict(self.test_X) ax.set_title(&#39;MAE is &#39;+str(np.mean(np.abs(self.test_Y - preds)))) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.legend() def visualise_AL(self): fig, ax = plt.subplots(1,2,figsize=(13,5)) def update(iteration): ax[0].cla() ax[1].cla() self._plot(ax[0], self.models[iteration]) self._plot_iter_MAE(ax[1], iteration) fig.tight_layout() print(&#39;Initial model&#39;) print(&#39;Y = &#39;+&#39; + &#39;.join([&#39;{0:0.2f}&#39;.format(self.models[0].MN[i])+&#39; X^&#39;*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)])) print(&#39; nFinal model&#39;) print(&#39;Y = &#39;+&#39; + &#39;.join([&#39;{0:0.2f}&#39;.format(self.models[self.iterations].MN[i])+&#39; X^&#39;*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)])) anim = FuncAnimation(fig, update, frames=np.arange(0, self.iterations+1, 1), interval=250) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) return anim . Visualizing a different polynomial fit on the same dataset . Let&#39;s try to fit a degree 7 polynomial to the same data now. . np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) noise = np.random.randn(1000, ) * 0.5 Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . model = ActiveL(X_init.reshape(-1,1), Y, degree=7, iterations=20, seed=seed) . model.data_preperation() model.main() model.visualise_AL() . Initial model Y = -1.92 + 3.79 X^1 + -1.81 X^2 + -0.43 X^3 + -0.51 X^4 + -0.27 X^5 + -0.18 X^6 + -0.11 X^7 Final model Y = -1.79 + 4.86 X^1 + -5.38 X^2 + 0.50 X^3 + -0.17 X^4 + 1.19 X^5 + 1.83 X^6 + 1.31 X^7 . &lt;/input&gt; Once Loop Reflect We can clearly see that model was fitting the train points well and uncertainty was high at the left-most position. After first iteration, the left-most point was added to the train set and MAE reduced significantly. Similar phenomeneon happened at iteration 2 with the right-most point. After that error kept reducing at slower rate gradually because fit was near optimal after just 2 iterations. . Active learning for diabetes dataset from the Scikit-learn module . Let&#39;s run our model for diabetes data from sklearn module. The data have various features like age, sex, weight etc. of diabetic people and target is increment in disease after one year. We&#39;ll choose only &#39;weight&#39; feature, which seems to have more correlation with the target. . We&#39;ll try to fit degree 1 polynomial to this data, as our data seems to have a linear fit. First, let&#39;s check the performance of Scikit-learn linear regression model. . X, Y = datasets.load_diabetes(return_X_y=True) X = X[:, 2].reshape(-1,1) # Choosing only feature 2 which seems more relevent to linear regression # Normalizing X = (X - X.min())/(X.max() - X.min()) Y = (Y - Y.min())/(Y.max() - Y.min()) . Visualizing the dataset. . plt.scatter(X, Y) plt.xlabel(&#39;Weight of the patients&#39;) plt.ylabel(&#39;Increase in the disease after a year&#39;) plt.show() . Let&#39;s fit the Scikit-learn linear regression model with 50% train-test split. . from sklearn.linear_model import LinearRegression train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state = seed) . clf = LinearRegression() . clf.fit(train_X, train_Y) pred_Y = clf.predict(test_X) . Visualizing the fit &amp; MAE. . plt.scatter(X, Y, label=&#39;data&#39;, s=5) plt.plot(test_X, pred_Y, label=&#39;model&#39;, color=&#39;r&#39;) plt.xlabel(&#39;Weight of the patients&#39;) plt.ylabel(&#39;Increase in the disease after a year&#39;) plt.title(&#39;MAE is &#39;+str(np.mean(np.abs(pred_Y - test_Y)))) plt.legend() plt.show() . Now we&#39;ll fit the same data with our BLR model . model = ActiveL(X.reshape(-1,1), Y, degree=1, iterations=20, seed=seed) . model.data_preperation() model.main() model.visualise_AL() . Initial model Y = 0.41 + 0.16 X^1 Final model Y = 0.13 + 0.86 X^1 . &lt;/input&gt; Once Loop Reflect Initially, the fit is leaning towards zero slope, which is the influence of bias due to a low number of training points. It&#39;s interesting to see that our initial train points tend to make a vertical fit, but the model doesn&#39;t get carried away by that and stabilizes the self with prior. . print(&#39;MAE for Scikit-learn Linear Regression is&#39;,np.mean(np.abs(pred_Y - test_Y))) print(&#39;MAE for Bayesian Linear Regression is&#39;, model.test_mae_error[20]) . MAE for Scikit-learn Linear Regression is 0.15424985705353944 MAE for Bayesian Linear Regression is 0.15738001811804758 . At the end, results of sklearn linear regression and our active learning based BLR model are comparable even though we&#39;ve used only 20 points to train our model over 221 points used by sklearn. This is because active learning enables us to choose those datapoints for training, which are going to contribute the most towards a precise fit. .",
            "url": "https://patel-zeel.github.io/blog/ml/2020/03/28/Active_Learning_with_Bayesian_Linear_Regression.html",
            "relUrl": "/ml/2020/03/28/Active_Learning_with_Bayesian_Linear_Regression.html",
            "date": " • Mar 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My Webpage .",
          "url": "https://patel-zeel.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://patel-zeel.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}